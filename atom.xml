<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://DragonFive.github.io</id>
    <title>dragon</title>
    <updated>2025-03-14T01:39:02.971Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://DragonFive.github.io"/>
    <link rel="self" href="https://DragonFive.github.io/atom.xml"/>
    <subtitle>邮箱(base64)：MTY5MDMwMjk2M0BxcS5jb20=
</subtitle>
    <logo>https://DragonFive.github.io/images/avatar.png</logo>
    <icon>https://DragonFive.github.io/favicon.ico</icon>
    <rights>All rights reserved 2025, dragon</rights>
    <entry>
        <title type="html"><![CDATA[deepseek R1 的部署性能]]></title>
        <id>https://DragonFive.github.io/post/deepseek-r1-benchmark/</id>
        <link href="https://DragonFive.github.io/post/deepseek-r1-benchmark/">
        </link>
        <updated>2025-02-17T11:30:46.000Z</updated>
        <content type="html"><![CDATA[<p>往期相关文章：</p>
<p><a href="https://dragonfive.github.io/post/sglang-ep-de-shi-xian/">sglang ep实现</a></p>
<p><a href="https://dragonfive.github.io/post/sglang-dp-shi-xian/">sglang dp的实现</a></p>
<p><a href="https://dragonfive.github.io/post/deepseek-v2-mla-de-li-jie/">deepseek V2 MLA 的理解</a></p>
<p><a href="https://dragonfive.github.io/post/vllm-prefill-decode-kernel/">vllm prefill 和 decode 的kernel代码解读</a></p>
<p><a href="https://dragonfive.github.io/post/sglang-model-run">sglang的模型执行</a></p>
<p><a href="https://dragonfive.github.io/post/sglang-de-mla-dai-ma-gen-zong/">sglang 的 MLA 代码跟踪</a></p>
<p>‍</p>
<p>deepseek R1 不同推理引擎部署的吞吐简单记录：</p>
<p>‍使用benchmark脚本：https://github.com/sgl-project/sglang/blob/main/python/sglang/bench_serving.py</p>
<p>1、在两台 H800 上分别使用vllm和 sglang部署deepseek，跑 benchmark 看产出数据的性能：</p>
<p>每秒向server发4个请求，一共发960条请求，每条请求长2000个token，要求输出2000个token。</p>
<p>（这时sglang还不支持dp）</p>
<table>
<thead>
<tr>
<th>配置</th>
<th>qps</th>
<th>input token/s</th>
<th>Output token/s</th>
<th>E2E均值/中位数(ms)</th>
<th>TTFT均值/中位数/p99分位(ms)</th>
<th>TPOT均值/中位数/p99分位(ms)</th>
<th>ITL均值/中位数/p99分位(ms)</th>
</tr>
</thead>
<tbody>
<tr>
<td>vllm_0.7.2_两机</td>
<td>1.09</td>
<td>1103.64</td>
<td>1075.78</td>
<td>367757.72/379839.38</td>
<td>184617.36/195902.60/455122.99</td>
<td>195.03/190.85/357.03</td>
<td>187.69/156.33/586.61</td>
</tr>
<tr>
<td>sglang两机</td>
<td>0.67</td>
<td>675.63</td>
<td>658.57</td>
<td>582352.85/589923.39</td>
<td>297396.49/319688.85/715794.45</td>
<td>311.18/288.19/828.12</td>
<td>290.12/248.61/835.41</td>
</tr>
</tbody>
</table>
<p>‍</p>
<p>2、在一台H20上分别使用vllm和 sglang部署deepseek，跑 benchmark 看产出数据的性能：</p>
<p>总输入token：49851，总输出2574919，总请求数：500，平均输入token：99.7，平均输出token：5149.8，瞬间把500条请求打过去</p>
<table>
<thead>
<tr>
<th>后端</th>
<th>配置(tp-pp)</th>
<th>qps</th>
<th>input token/s</th>
<th>Output token/s</th>
<th>E2E均值/中位数(ms)</th>
<th>TTFT均值/中位数/(ms)</th>
<th>TPOT均值/中位数(ms)</th>
<th>ITL均值/中位数(ms)</th>
</tr>
</thead>
<tbody>
<tr>
<td>sglang</td>
<td>tp=8</td>
<td>0.17</td>
<td>16.98</td>
<td>876.83</td>
<td>1710813.62/1768756.35</td>
<td>564953.54/419441.76</td>
<td>231.00/179.63</td>
<td>222.66/160.13</td>
</tr>
<tr>
<td>tp8+dp</td>
<td></td>
<td>0.30</td>
<td>29.84</td>
<td>1541.31</td>
<td>1114222.53/1144923.91</td>
<td>10717.03/10949.87</td>
<td>212.49/215.73</td>
<td>214.32/219.45</td>
</tr>
<tr>
<td>vllm0.7.2</td>
<td>tp=8</td>
<td>0.10</td>
<td>10.13</td>
<td>523.37</td>
<td>2489494.11/2407268.89</td>
<td>1105337.01/3756.43</td>
<td>283.14/251.26</td>
<td>269.63/160.38</td>
</tr>
</tbody>
</table>
<p>‍</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[sglang ep 的实现]]></title>
        <id>https://DragonFive.github.io/post/sglang-ep-de-shi-xian/</id>
        <link href="https://DragonFive.github.io/post/sglang-ep-de-shi-xian/">
        </link>
        <updated>2024-12-21T07:59:15.000Z</updated>
        <content type="html"><![CDATA[<p>‍<br>
往期相关文章：</p>
<p><a href="https://dragonfive.github.io/post/sglang-dp-shi-xian/">sglang dp的实现</a></p>
<p><a href="https://dragonfive.github.io/post/deepseek-v2-mla-de-li-jie/">deepseek V2 MLA 的理解</a></p>
<p><a href="https://dragonfive.github.io/post/vllm-prefill-decode-kernel/">vllm prefill 和 decode 的kernel代码解读</a></p>
<p><a href="https://dragonfive.github.io/post/sglang-model-run">sglang的模型执行</a></p>
<p><a href="https://dragonfive.github.io/post/sglang-de-mla-dai-ma-gen-zong/">sglang 的 MLA 代码跟踪</a></p>
<p>‍</p>
<p>sglang 在 12月6日终于合入了ep功能，pr 连接：<a href="https://github.com/sgl-project/sglang/pull/2371">MoE Expert Parallel</a>，可以拿来学习一下。</p>
<p>开启了一个 <code>enable_ep_moe</code>​ 的开关，在模型中通过判断是否开启来确定使用什么moe实现</p>
<pre><code class="language-python"> MoEImpl = EPMoE if global_server_args_dict[&quot;enable_ep_moe&quot;] else FusedMoE
</code></pre>
<p>‍</p>
<p>具体 ep 实现是在 python/sglang/srt/layers/moe/ep_moe/layer.py，看这代码不知道为啥还有点激动。</p>
<p>EPMoE的核心是将专家分布在不同的 tensor parallel rank上，然后在 token 推理时每个rank 会过滤出属于自己负责的 expert 的 token，处理完之后各rank通过 allreduce 拿到全量的部分。</p>
<p>‍</p>
<h2 id="1-通信部分">1、通信部分</h2>
<p>ep 前，如果 attention 做了 dp，就会做 allgather 让每个rank把数据都拿到;</p>
<p>代码位于 python/sglang/srt/models/deepseek_v2.py</p>
<pre><code class="language-python">        # Fully Connected
        if self.enable_dp_attention:
            hidden_states, start_idx, end_idx = all_gather(
                hidden_states, forward_batch, self.tp_rank, self.tp_size, self.tp_group
            )
            hidden_states = self.mlp(hidden_states)
            hidden_states = hidden_states[start_idx:end_idx]
        else:
            hidden_states = self.mlp(hidden_states)
</code></pre>
<p>‍</p>
<p>ep 每个rank会保留属于自己负责的expert 的数据；</p>
<p>ep 后每个rank会通过 allreduce 来拿到所有数据的处理结果</p>
<p>代码位于：python/sglang/srt/models/deepseek_v2.py</p>
<pre><code class="language-python">def forward(self, hidden_states: torch.Tensor) -&gt; torch.Tensor:
    # ...
    final_hidden_states = self.experts(hidden_states=hidden_states, router_logits=router_logits)
    if self.tp_size &gt; 1:
        # MoE输出需要在不同GPU间做all_reduce
        final_hidden_states = tensor_model_parallel_all_reduce(final_hidden_states)
    return final_hidden_states
</code></pre>
<p>‍</p>
<p>核心的处理在 python/sglang/srt/layers/moe/ep_moe/layer.py</p>
<p>‍</p>
<h2 id="2-expert-分配和数据划分">2、expert 分配和数据划分</h2>
<p>EPMoE将experts平均分配到不同的GPU上</p>
<p>代码位于 python/sglang/srt/layers/moe/ep_moe/layer.py</p>
<pre><code class="language-python">self.num_experts = num_experts
assert self.num_experts % self.tp_size == 0  # 确保experts可以平均分配
self.num_experts_per_partition = self.num_experts // self.tp_size
self.start_expert_id = self.tp_rank * self.num_experts_per_partition 
self.end_expert_id = self.start_expert_id + self.num_experts_per_partition - 1
</code></pre>
<p>首先通过router_logits计算每个token应该被哪些experts处理:</p>
<pre><code class="language-python">topk_weights, topk_ids = select_experts(
    hidden_states=hidden_states,
    ...
)
</code></pre>
<p>对expert ids排序,构建数据结构:</p>
<pre><code class="language-python">reorder_topk_ids, src2dst, seg_indptr = run_moe_ep_preproess(topk_ids, self.num_experts)
</code></pre>
<p>生成了三个重要的数据结构:</p>
<ul>
<li>reorder_topk_ids: 排序后的expert ids</li>
<li>src2dst: 原始位置到重排序后位置的映射</li>
<li>seg_indptr: 每个expert处理的token范围的索引</li>
</ul>
<p>‍</p>
<p>这里跟稀疏架构的请求ps 或alltoall 通信前的操作有点类似，不过稀疏架构更多是为了去重。</p>
<p>重排输入数据:</p>
<pre><code class="language-python">pre_reorder_triton_kernel(
    hidden_states,
    gateup_input, # 重排后的输入
    src2dst,
    topk_ids,
    ...
)
</code></pre>
<p>这个 kernel 完成了关键的数据重排序工作：</p>
<ul>
<li>将输入 token 按照它们请求的专家 ID 重新排序</li>
<li><strong>只保留当前 rank 负责的专家对应的请求</strong>，过滤掉其他 rank 的数据</li>
</ul>
<p>‍</p>
<p>‍</p>
<h2 id="3-expert-计算与结果恢复">3、expert 计算与结果恢复</h2>
<pre><code class="language-python">gateup_output = self.grouped_gemm_runner(
    a=gateup_input,
    b=self.w13_weight,
    c=gateup_output,
    batch_size=self.num_experts_per_partition,
</code></pre>
<p>使用分组矩阵乘法，每个专家单独处理自己负责的 token：</p>
<ul>
<li>seg_indptr_cur_rank 指示每个专家负责的 token 范围</li>
<li>每个专家只处理分配给自己的 token</li>
</ul>
<p>‍</p>
<pre><code class="language-python">post_reorder_triton_kernel(hidden_states.size(0),),
    BLOCK_SIZE=512,
)
</code></pre>
<p>这个 kernel 完成了最终的输出重排序：</p>
<ul>
<li>将专家计算的结果重新排回原始 token 顺序</li>
<li>应用专家权重进行加权求和</li>
<li><strong>只处理当前 rank 负责的专家输出</strong>，其他 rank 的专家输出会在各自的 rank 上处理</li>
</ul>
<p>‍</p>
<p>‍</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[sglang dp的实现]]></title>
        <id>https://DragonFive.github.io/post/sglang-dp-de-shi-xian/</id>
        <link href="https://DragonFive.github.io/post/sglang-dp-de-shi-xian/">
        </link>
        <updated>2024-12-06T03:56:07.000Z</updated>
        <content type="html"><![CDATA[<p>往期相关文章：</p>
<p><a href="https://dragonfive.github.io/post/sglang-model-run">sglang的模型执行</a></p>
<p><a href="https://dragonfive.github.io/post/sglang-de-mla-dai-ma-gen-zong/">sglang 的 MLA 代码跟踪</a></p>
<p><a href="https://dragonfive.github.io/post/deepseek-v2-mla-de-li-jie/">deepseek V2 MLA 的理解</a></p>
<p><a href="https://dragonfive.github.io/post/vllm-prefill-decode-kernel/">vllm prefill 和 decode 的kernel代码解读</a></p>
<p>‍</p>
<p>在11月16日，sglang 合并了对MLA的dp支持，工作还是很棒的，pr链接为：https://github.com/sgl-project/sglang/pull/1970，在12月4日，sglang发布了V0.4.0版本，并发了一篇博客，其中对dp的工作原理进行了详细的介绍。</p>
<p>https://lmsys.org/blog/2024-12-04-sglang-v0-4/#data-parallelism-attention-for-deepseek-models</p>
<p>‍</p>
<p>​<img src="assets/image-20250306113913-dm2wbfq.png" alt="image" loading="lazy">​</p>
<p>会在 mla 之前去把数据进行拆分，然后在完成mla 之后在moe之前进行allgather，moe 使用tp并行，文中也提到了ep的思路，但pr还没合并。整体效果还是非常不错的。</p>
<p>​<img src="assets/image-20250306114125-qpl4ecy.png" alt="image" loading="lazy">​</p>
<p>为什么要用dp呢，其实是 mla 如果不用dp，那么似乎就会退化成 MQA，做tp会在每个rank上都有一个kv，那这样其实是浪费了一些显存，那与其这样，莫不如就让它在每个rank上有个replica，然后把数据切分开，这样不就ok了吗。</p>
<p>接下来，我门就看看这个dp的代码是怎样的吧，pr链接为：https://github.com/sgl-project/sglang/pull/1970。</p>
<p>‍</p>
<h2 id="1-数据拆分与结果合并">1、数据拆分与结果合并</h2>
<p>‍</p>
<p>结果合并的代码比较好找，在python/sglang/srt/models/deepseek_v2.py 文件中</p>
<pre><code class="language-python">    if self.enable_dp_attention:
        hidden_states, start_idx, end_idx = all_gather(
            hidden_states, forward_batch, self.tp_rank, self.tp_size, self.tp_group
        )
        hidden_states = self.mlp(hidden_states)
        hidden_states = hidden_states[start_idx:end_idx]
</code></pre>
<p>‍</p>
<p>这里调用 all_gather 来将各个 dp rank 做 mla 之后的结果 allgather过来来做后边的 moe ，同时会记住本rank负责的请求的 start_idx 和 end_idx。</p>
<p>在完成 moe 的 mlp 层之后，会通过 <code>hidden_states[start_idx:end_idx]</code>​ 来拿到本rank的 请求，执行下一层的 mla。</p>
<p>all_gather 的实现如下，</p>
<pre><code class="language-python">def all_gather(
    input_tensor: torch.Tensor, forward_batch: ForwardBatch, rank, world_size, group
):
    if world_size == 1:
        return input_tensor

    all_lens = forward_batch.global_num_tokens
    max_len = max(forward_batch.global_num_tokens)

    padded_tensor = torch.nn.functional.pad(
        input_tensor, (0, 0, 0, max_len - input_tensor.shape[0])
    )

    torch.distributed.all_gather_into_tensor(
        forward_batch.gathered_buffer, padded_tensor, group=group
    )

    gathered_tensors = torch.concat(
        [
            forward_batch.gathered_buffer[i * max_len : i * max_len + all_lens[i]]
            for i in range(world_size)
        ]
    )

    start_index = 0 if rank == 0 else sum(all_lens[:rank])
    end_index = start_index + all_lens[rank]

    return gathered_tensors, start_index, end_index
</code></pre>
<p>这里有一些注意点：</p>
<ul>
<li>global_num_tokens 是一个 list，收集了每个 rank 的 token 数量。然后会根据最大token数对数据进行 pad，</li>
<li>all_gather要求所有进程的tensor大小必须相同, 所以需要将每个进程的tensor都pad到最大长度。</li>
</ul>
<p>input_tensor 的 shape 为 <code>[num_tokens, hidden_dim]</code>​</p>
<pre><code class="language-python">pad = (0, 0,        # 最后一维（hidden_dim）不做padding
       0, max_len - input_tensor.shape[0])  # 第一维（seq_len）只在末尾padding
</code></pre>
<ul>
<li>
<p>padding后的shape会变成：<code>[max_len, hidden_dim]</code>​</p>
</li>
<li>
<p>gather 之后要先通过<code>[i * max_len : i * max_len + all_lens[i]]</code>​ 切片来切出有效的数据部分，扔掉padding的部分。取出有效部分之后再做 concat。</p>
</li>
<li>
<p>返回的时候会把 本rank token 开始和结束的行号也返回，方便昨晚moe MLP 之后还原来做后边层的MLA。</p>
</li>
</ul>
<p>‍</p>
<h2 id="2-数据是怎么切分的">2、数据是怎么切分的</h2>
<p>‍</p>
<p>在 python/sglang/srt/managers/data_parallel_controller.py 文件中，每次拿到数据都会通过 round_robin 的方式轮循到下一个worker，然后通过zmq 发给它</p>
<pre><code class="language-python">    def round_robin_scheduler(self, req):
        self.workers[self.round_robin_counter].send_pyobj(req)
        self.round_robin_counter = (self.round_robin_counter + 1) % len(self.workers)
</code></pre>
<p>‍</p>
<pre><code class="language-python">    def event_loop(self):
        while True:
            while True:
                try:
                    recv_req = self.recv_from_tokenizer.recv_pyobj(zmq.NOBLOCK)
                except zmq.ZMQError:
                    break
                if isinstance(
                    recv_req,
                    (
                        TokenizedGenerateReqInput,
                        TokenizedEmbeddingReqInput,
                    ),
                ):
                    self.dispatching(recv_req)
                else:
                    # Send other control messages to all workers
                    for worker in self.workers:
                        worker.send_pyobj(recv_req)
</code></pre>
<p>‍</p>
<p>在 python/sglang/srt/managers/scheduler.py 文件中，会拿到分发来的数据，然后进行调度</p>
<pre><code class="language-python">		if self.tp_rank == 0 or self.server_args.enable_dp_attention:
            self.recv_from_tokenizer = get_zmq_socket(
                context, zmq.PULL, port_args.scheduler_input_ipc_name
            )
</code></pre>
<p>需要注意的是 在准备下一个batch的时候，会调用prepare_dp_attn_batch</p>
<pre><code class="language-python">        if self.server_args.enable_dp_attention:
            ret = self.prepare_dp_attn_batch(ret)

        return ret
</code></pre>
<p>‍</p>
<p>而第一部分说的 allgather 中的 global_num_tokens 就是从这里来的。</p>
<pre><code class="language-python">    def prepare_dp_attn_batch(self, local_batch: ScheduleBatch):
        # Check if other DP workers have running batches
        if local_batch is None:
            num_tokens = 0
        elif local_batch.forward_mode.is_decode():
            num_tokens = local_batch.batch_size()
        else:
            num_tokens = local_batch.extend_num_tokens

        local_num_tokens = torch.tensor(
            num_tokens, dtype=torch.int64, device=self.device
        )
        global_num_tokens = torch.empty(
            self.tp_size, dtype=torch.int64, device=self.device
        )
        torch.distributed.all_gather_into_tensor(
            global_num_tokens,
            local_num_tokens,
            group=self.tp_worker.get_tp_device_group(),
        )
		....
</code></pre>
<p>‍</p>
<p>从宏观来看，在整个模型进行一次forward 之前，会获得每个rank上拿到的 token数，然后来进程N层的mla 和 moe 时会反复复用这个 token 数，所以会把对token数对 allgather 放在这里，跟实际数据的 allgather 不在一块。</p>
<p>‍</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[sglang 的 MLA 代码跟踪]]></title>
        <id>https://DragonFive.github.io/post/sglang-de-mla-dai-ma-gen-zong/</id>
        <link href="https://DragonFive.github.io/post/sglang-de-mla-dai-ma-gen-zong/">
        </link>
        <updated>2024-09-01T13:44:58.000Z</updated>
        <content type="html"><![CDATA[<p>往期相关文章：</p>
<p><a href="https://dragonfive.github.io/post/sglang-model-run">sglang的模型执行</a></p>
<p><a href="https://dragonfive.github.io/post/deepseek-v2-mla-de-li-jie/">deepseek V2 MLA 的理解</a></p>
<p><a href="https://dragonfive.github.io/post/vllm-prefill-decode-kernel/">vllm prefill 和 decode 的kernel代码解读</a></p>
<p>‍</p>
<p>‍</p>
<p>在前面文章中分析了一下 MLA 的 transformers 实现：<a href="https://dragonfive.github.io/post/deepseek-v2-mla-de-li-jie/">deepseek V2 MLA 的理解</a></p>
<p>在8月5日，sglang 合入了MLA的实现， <a href="https://github.com/sgl-project/sglang/pull/905">upport MLA for DeepSeek-V2 with Triton - step 1#905</a>，虽然已经了解了矩阵吸收的概念，但是还是难以想象具体该怎么实现，这个 sglang 的实现也看上去似懂非懂的，索性看了<a href="https://www.zhihu.com/people/james0zan">章明星老师</a>的知乎文章：<a href="https://zhuanlan.zhihu.com/p/700214123">DeepSeek-V2 高性能推理 (1)：通过矩阵吸收十倍提速 MLA 算子</a>，了解了其中的关键（话说章老师的标题总是这么震惊吗 orz）。</p>
<p>‍</p>
<h2 id="公式部分">公式部分</h2>
<p>最重要的就两点：</p>
<p>‍</p>
<p>1、key 的矩阵的吸收</p>
<p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><msubsup><mi>c</mi><mi>q</mi><mi>T</mi></msubsup><mo>⋅</mo><msubsup><mi>W</mi><mrow><mi>U</mi><mi>Q</mi></mrow><mi>T</mi></msubsup><mo>)</mo><mo>⋅</mo><mo>(</mo><msub><mi>W</mi><mrow><mi>U</mi><mi>K</mi></mrow></msub><mo>⋅</mo><msub><mi>c</mi><mrow><mi>k</mi><mi>v</mi></mrow></msub><mo>)</mo><mo>=</mo><mo>(</mo><msubsup><mi>c</mi><mi>q</mi><mi>T</mi></msubsup><mo>⋅</mo><msubsup><mi>W</mi><mrow><mi>U</mi><mi>Q</mi></mrow><mi>T</mi></msubsup><mo>⋅</mo><msub><mi>W</mi><mrow><mi>U</mi><mi>K</mi></mrow></msub><mo>)</mo><mo>⋅</mo><msub><mi>c</mi><mrow><mi>k</mi><mi>v</mi></mrow></msub></mrow><annotation encoding="application/x-tex">(c_q^T\cdot W_{UQ}^T)\cdot (W_{UK}\cdot c_{kv}) = (c_q^T \cdot W_{UQ}^T \cdot W_{UK})\cdot c_{kv}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2244389999999998em;vertical-align:-0.383108em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">q</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.383108em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.25277em;vertical-align:-0.411439em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-2.424669em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.10903em;">U</span><span class="mord mathdefault mtight">Q</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.411439em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.10903em;">U</span><span class="mord mathdefault mtight" style="margin-right:0.07153em;">K</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span><span class="mord mathdefault mtight" style="margin-right:0.03588em;">v</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.2244389999999998em;vertical-align:-0.383108em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">q</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.383108em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.25277em;vertical-align:-0.411439em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-2.424669em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.10903em;">U</span><span class="mord mathdefault mtight">Q</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.411439em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.10903em;">U</span><span class="mord mathdefault mtight" style="margin-right:0.07153em;">K</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span><span class="mord mathdefault mtight" style="margin-right:0.03588em;">v</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></p>
<p>这样算 query 矩阵乘的时候直接把解析 key的矩阵一起算了，这样就可以一直使用 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>c</mi><mrow><mi>k</mi><mi>v</mi></mrow></msub></mrow><annotation encoding="application/x-tex">c_{kv}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span><span class="mord mathdefault mtight" style="margin-right:0.03588em;">v</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>。</p>
<p>‍</p>
<p>2、value 的矩阵的吸收</p>
<p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><mi>s</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi><mo>⋅</mo><mo>(</mo><msub><mi>c</mi><mrow><mi>k</mi><mi>v</mi></mrow></msub><mo>⋅</mo><msub><mi>W</mi><mrow><mi>U</mi><mi>V</mi></mrow></msub><mo>)</mo><mo>)</mo><mo>⋅</mo><msub><mi>W</mi><mi>O</mi></msub><mo>=</mo><mo>(</mo><mi>s</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi><mo>⋅</mo><msub><mi>c</mi><mrow><mi>k</mi><mi>v</mi></mrow></msub><mo>)</mo><mo>⋅</mo><mo>(</mo><msub><mi>W</mi><mrow><mi>U</mi><mi>V</mi></mrow></msub><mo>⋅</mo><msub><mi>W</mi><mi>O</mi></msub><mo>)</mo></mrow><annotation encoding="application/x-tex">(score\cdot (c_{kv}\cdot W_{UV}))\cdot W_{O}=(score\cdot c_{kv})\cdot (W_{UV}\cdot W_{O})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathdefault">s</span><span class="mord mathdefault">c</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault">e</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span><span class="mord mathdefault mtight" style="margin-right:0.03588em;">v</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.10903em;">U</span><span class="mord mathdefault mtight" style="margin-right:0.22222em;">V</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.02778em;">O</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathdefault">s</span><span class="mord mathdefault">c</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault">e</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span><span class="mord mathdefault mtight" style="margin-right:0.03588em;">v</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.10903em;">U</span><span class="mord mathdefault mtight" style="margin-right:0.22222em;">V</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.02778em;">O</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></p>
<p>然后算出分数之后与value相成时，也可以不用解析出 value, 而是先算矩阵乘，然后一直使用 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>c</mi><mrow><mi>k</mi><mi>v</mi></mrow></msub></mrow><annotation encoding="application/x-tex">c_{kv}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span><span class="mord mathdefault mtight" style="margin-right:0.03588em;">v</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>。</p>
<p>‍</p>
<p>然后我们就看到 key、value根本没有发生像 transformers 那样的broadcast，而且用的是低秩压缩的 kv，确实省了显存，sglang确实牛的。</p>
<p>‍</p>
<h2 id="代码部分">代码部分</h2>
<p>那我们来把 sglang 中的实现跟这个公式对应一下</p>
<p>跟 transformers的实现一样，sglang 也是有对于kv的低秩压缩矩阵和恢复矩阵的linear 层</p>
<p>主要看这个 <a href="https://github.com/sgl-project/sglang/pull/905/files#diff-5b9e34dd492bd8a14702a18b594721091092276fad1cf8736fba6ef1f33c1b04">deepseek_v2.py</a></p>
<pre><code class="language-python">        self.kv_a_proj_with_mqa = ReplicatedLinear(
            self.hidden_size,
            self.kv_lora_rank + self.qk_rope_head_dim,
            bias=False,
            quant_config=quant_config,
        )
        self.kv_a_layernorm = RMSNorm(self.kv_lora_rank, eps=config.rms_norm_eps)
        self.kv_b_proj = ColumnParallelLinear(
            self.kv_lora_rank,
            self.num_heads * (self.qk_nope_head_dim + self.v_head_dim),
            bias=False,
            quant_config=quant_config,
        )
</code></pre>
<p>不一样的地方在于：</p>
<pre><code class="language-python">        kv_b_proj = self.kv_b_proj
        w_kc, w_vc = kv_b_proj.weight.unflatten(
            0, (-1, qk_nope_head_dim + v_head_dim)
        ).split([qk_nope_head_dim, v_head_dim], dim=1)
        self.w_kc = w_kc
        self.w_vc = w_vc

</code></pre>
<p>‍</p>
<p>这里把恢复维度的 linear 层给 split 了，split 成恢复key 的 w_kc 和恢复value 的 w_vc，这是为什么呢，就得继续往下看了：</p>
<p>‍</p>
<pre><code class="language-python">q_nope, q_pe = q.split([self.qk_nope_head_dim, self.qk_rope_head_dim], dim=-1)
q_nope_out = q_input[..., : self.kv_lora_rank]
torch.bmm(q_nope.transpose(0, 1), self.w_kc, out=q_nope_out.transpose(0, 1))

</code></pre>
<p>第一样行跟 transformers 的实现差不多，拆出做 NoPE 的1/3部分，和不做NoPE 的 2/3部分。</p>
<p>然后后面就不一样了 。这里第三行就是公式部分中的</p>
<p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msubsup><mi>c</mi><mi>q</mi><mi>T</mi></msubsup><mo>⋅</mo><msubsup><mi>W</mi><mrow><mi>U</mi><mi>Q</mi></mrow><mi>T</mi></msubsup><mo>⋅</mo><msub><mi>W</mi><mrow><mi>U</mi><mi>K</mi></mrow></msub></mrow><annotation encoding="application/x-tex">c_q^T \cdot W_{UQ}^T \cdot W_{UK}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2244389999999998em;vertical-align:-0.383108em;"></span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">q</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.383108em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.25277em;vertical-align:-0.411439em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-2.424669em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.10903em;">U</span><span class="mord mathdefault mtight">Q</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.411439em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.10903em;">U</span><span class="mord mathdefault mtight" style="margin-right:0.07153em;">K</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></p>
<p>所以就知道为什么要把 linear 层拆开了。</p>
<p>另外 <code>torch.bmm</code>​ 是 PyTorch 中的批量矩阵乘法（Batch Matrix Multiplication）操作。它用于同时计算多个矩阵乘法。</p>
<p>具体来说：</p>
<ul>
<li>bmm 接受两个三维张量作为输入：(batch_size, n, m) 和 (batch_size, m, p)</li>
<li>输出也是一个三维张量：(batch_size, n, p)</li>
<li>对每个 batch 中的矩阵执行矩阵乘法运算</li>
</ul>
<p>‍</p>
<pre><code class="language-python">	attn_output = self.attn(q_input, k_input, v_input, input_metadata)
        attn_output = attn_output.view(-1, self.num_local_heads, self.kv_lora_rank)
        attn_bmm_output = attn_output.new_empty(
            q_len, self.num_local_heads, self.v_head_dim
        )
        torch.bmm(
            attn_output.transpose(0, 1),
            self.w_vc.transpose(1, 2).contiguous(),
            out=attn_bmm_output.transpose(0, 1),
        )

        attn_output = attn_bmm_output.flatten(1, 2)
        output, _ = self.o_proj(attn_output)
</code></pre>
<p>这里就对应第二个吸收公式了，把 v_input （其实是 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>C</mi><mrow><mi>k</mi><mi>v</mi></mrow></msub></mrow><annotation encoding="application/x-tex">C_{kv}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span><span class="mord mathdefault mtight" style="margin-right:0.03588em;">v</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>）传进去，当成普通的attention 去算score，然后外面的 output 的linear 权重跟解析 value 的矩阵去做矩阵乘。一直用比较低维的压缩的 kv，节省了显存。但模型大做TP的话，就像MQA那样，每张卡上应该都得存一份。</p>
<p>‍</p>
<p>‍</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[deepseek V2 MLA 的理解]]></title>
        <id>https://DragonFive.github.io/post/deepseek-v2-mla-de-li-jie/</id>
        <link href="https://DragonFive.github.io/post/deepseek-v2-mla-de-li-jie/">
        </link>
        <updated>2024-08-10T12:47:55.000Z</updated>
        <content type="html"><![CDATA[<p>上个月deepseekv2 发布了， https://arxiv.org/pdf/2405.04434，学习了一下论文，感觉MLA还挺重要的，不过只看到了 transformers 的实现，感觉deepseek内部应该有更快的实现，期待vllm的版本。</p>
<p>‍<img src="https://DragonFive.github.io/post-images/1740919815298.png" alt="" loading="lazy"></p>
<figure data-type="image" tabindex="1"><img src="https://DragonFive.github.io/post-images/1740919825744.png" alt="" loading="lazy"></figure>
<p>MLA 中 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msubsup><mi>C</mi><mi>t</mi><mrow><mi>K</mi><mi>V</mi></mrow></msubsup></mrow><annotation encoding="application/x-tex">C_t^{KV}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.088331em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-2.4530000000000003em;margin-left:-0.07153em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.07153em;">K</span><span class="mord mathdefault mtight" style="margin-right:0.22222em;">V</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span></span></span></span> 表示低秩压缩的KV，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msubsup><mi>K</mi><mi>t</mi><mi>R</mi></msubsup></mrow><annotation encoding="application/x-tex">K_t^R</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.088331em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-2.4530000000000003em;margin-left:-0.07153em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.00773em;">R</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span></span></span></span> 是多query共享的 k， 是key做了RoPE的部分，</p>
<p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msubsup><mi>C</mi><mi>t</mi><mrow><mi>K</mi><mi>V</mi></mrow></msubsup></mrow><annotation encoding="application/x-tex">C_t^{KV}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.088331em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-2.4530000000000003em;margin-left:-0.07153em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.07153em;">K</span><span class="mord mathdefault mtight" style="margin-right:0.22222em;">V</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span></span></span></span> 的shape 为 (4*d_h)=512, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msubsup><mi>K</mi><mi>t</mi><mi>R</mi></msubsup></mrow><annotation encoding="application/x-tex">K_t^R</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.088331em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-2.4530000000000003em;margin-left:-0.07153em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.00773em;">R</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span></span></span></span> 的维度为 d_h/2 = 64, d_h=128</p>
<p>‍</p>
<pre><code class="language-python">&quot;hidden_size&quot;: 5120,
&quot;q_lora_rank&quot;: 1536,
&quot;num_attention_heads&quot;: 128,
&quot;kv_lora_rank&quot;: 512,
&quot;v_head_dim&quot;: 128,
&quot;qk_nope_head_dim&quot;: 128,
&quot;qk_rope_head_dim&quot;: 64,
</code></pre>
<p>‍</p>
<p>query 的 linear 层</p>
<p>代码来源： https://huggingface.co/deepseek-ai/DeepSeek-V2/blob/main/modeling_deepseek.py</p>
<pre><code class="language-python">self.q_head_dim = config.qk_nope_head_dim + config.qk_rope_head_dim
self.q_a_proj = nn.Linear(
   self.hidden_size, config.q_lora_rank, bias=config.attention_bias
)
self.q_a_layernorm = DeepseekV3RMSNorm(config.q_lora_rank)
self.q_b_proj = nn.Linear(
   config.q_lora_rank, self.num_heads * self.q_head_dim, bias=False
)
</code></pre>
<p>q_a_proj:  (5120, 1536)</p>
<p>q_b_proj: (1536, 128*192=24576)</p>
<p>‍</p>
<p>kv 低秩压缩层</p>
<pre><code class="language-python">self.kv_a_proj_with_mqa = nn.Linear(
            self.hidden_size,
            config.kv_lora_rank + config.qk_rope_head_dim,
            bias=config.attention_bias,
        )
self.kv_a_layernorm = DeepseekV3RMSNorm(config.kv_lora_rank)
self.kv_b_proj = nn.Linear(
            config.kv_lora_rank,
            self.num_heads
            * (self.q_head_dim - self.qk_rope_head_dim + self.v_head_dim),
            bias=False,
        )
</code></pre>
<p>kv_a_proj_with_mqa: (5120, 576)</p>
<p>kv_b_proj（512, 128*(192-64+128) ）</p>
<pre><code class="language-python"># 7168-&gt;576
compressed_kv = self.kv_a_proj_with_mqa(hidden_states)
# compressed_kv:512, k_pe:64 
compressed_kv, k_pe = torch.split(
     compressed_kv, [self.kv_lora_rank, self.qk_rope_head_dim], dim=-1
   )
# 升维：[bsz, 1, q_len,64]
k_pe = k_pe.view(bsz, q_len, 1, self.qk_rope_head_dim).transpose(1, 2)

# 【bsz, 128, q_len， 128+128】
kv = (
  self.kv_b_proj(self.kv_a_layernorm(compressed_kv))
      .view(bsz, q_len, self.num_heads, self.qk_nope_head_dim + self.v_head_dim)
      .transpose(1, 2)
)
# 然后拆分出value 部分和不做 rope 的 key部分
k_nope, value_states = torch.split(
     kv, [self.qk_nope_head_dim, self.v_head_dim], dim=-1
)
</code></pre>
<p>整个过程就是先把 hidden_states 经过一个低秩压缩（kv_lora_rank），然后从中取出要做rope的那部分key，剩下的部分升维到多head 的 key 和 value 。（128， 128）</p>
<p>然后拆分出value 部分和不做 rope 的 key部分。</p>
<p>不过 这个版本并看出来MLA kvcache 哪里省显存，下边 key_states 还是分配了 num_heads</p>
<pre><code class="language-python">q_pe, k_pe = apply_rotary_pos_emb(q_pe, k_pe, cos, sin, position_ids)

query_states = k_pe.new_empty(bsz, self.num_heads, q_len, self.q_head_dim)
query_states[:, :, :, : self.qk_nope_head_dim] = q_nope
query_states[:, :, :, self.qk_nope_head_dim :] = q_pe

key_states = k_pe.new_empty(bsz, self.num_heads, q_len, self.q_head_dim)
key_states[:, :, :, : self.qk_nope_head_dim] = k_nope
key_states[:, :, :, self.qk_nope_head_dim :] = k_pe
</code></pre>
<p>query_states 和 key_states 都是 nope 部分和做了 rope的部分拼接而成的</p>
<p>query_states 的 shape 为 (bsz, num_heads, q_len, q_head_dim)</p>
<p>key_states 的 shape 为 (bsz, num_heads, kv_seq_len, q_head_dim)</p>
<p>得到的 q,k包括两部分拼接而成：一部分是做了低秩压缩得到的 q,k<br>
向量，一部分是增加了RoPE位置编码的 q,k 向量。</p>
<p>‍</p>
<p>query 与 key 的转置进行矩阵乘，后面的操作就跟普通 MHA差不多了</p>
<p>attn_weight shape 为 （bsz, num_heads, q_len, kv_seq_len）</p>
<p>value_states 的 shape 为（bsz, num_heads, kv_seq_len, v_head_dim)</p>
<p>‍</p>
<p>最后 attn_weight 与 value_states 矩阵乘得到 bsz, num_heads, q_len, v_head_dim)</p>
<p>可以看到 q,k包括两部分拼接而成：一部分是做了低秩压缩得到的 q,k<br>
向量，一部分是增加了RoPE位置编码的 q,k 向量。v 是只有一份的，key 的最后一维比value 多了 rope部分。</p>
<p>‍</p>
<p>‍</p>
<p>单独计算了两个带着位置编码的q_pe, k_pe, 维度为 单Attention Head维度的一半：128/2=64， K_pe 也是一层一份的，多query的头共享同一个，注意它的shape, num_head 的那一维是1</p>
<pre><code class="language-python">k_pe = k_pe.view(bsz, q_len, 1, self.qk_rope_head_dim).transpose(1, 2)
</code></pre>
<p>然后在复制给 key_states 时进行了broadcast</p>
<pre><code class="language-python">key_states[:, :, :, self.qk_nope_head_dim :] = k_pe
</code></pre>
<p>‍https://spaces.ac.cn/archives/10091        2024-05-13</p>
<p>原理上苏剑林大佬已经解释的非常清晰了，“RoPE与低秩KV不兼容，没法做矩阵吸收计<br>
算”，  2/3 的key不需要做位置相关的 ROPE 所以可以通过矩阵吸收在算Q的时候先算好对key的运算，可以缓存低秩压缩低部分，做ROPE的key部分是多head 共享的，所以也可以省kc cache 的显存。这也是MLA的压缩KV Cache的核心原理。</p>
<p>‍<br>
​<img src="https://DragonFive.github.io/post-images/1740919895912.png" alt="" loading="lazy"></p>
<p>从效果上看，虽然MLA缓存的Latent KV比较短（相当于2.25个MQA的缓存量），但MLA有恢复全 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>k</mi><mo separator="true">,</mo><mi>v</mi></mrow><annotation encoding="application/x-tex">k,v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">v</span></span></span></span> 的能力，特征表达能力显著比GQA、MQA要强。</p>
<p>‍</p>
<p>这里 transformers 的实现明显并不是实际的实现，后续有时间再看看别的仓库的实现吧。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[sglang 的模型执行]]></title>
        <id>https://DragonFive.github.io/post/sglang-model-run/</id>
        <link href="https://DragonFive.github.io/post/sglang-model-run/">
        </link>
        <updated>2024-07-29T09:10:53.000Z</updated>
        <content type="html"><![CDATA[<p>往期相关文章：</p>
<p><a href="https://dragonfive.github.io/post/vllm-prefill-decode-kernel/">vllm prefill 和 decode 的kernel代码解读</a></p>
<p>‍</p>
<p>‍</p>
<p>前天7月27日，sglang 发布了 <a href="https://github.com/sgl-project/sglang/releases/tag/v0.2.5">v0.2.5</a>，还发了一篇博客：https://lmsys.org/blog/2024-07-25-sglang-llama3/ ，我们来分析一下这个代码，就像分析vllm 代码一样</p>
<p>‍</p>
<p>代码的起点是在 python/sglang/srt/server.py 中</p>
<pre><code class="language-python">def launch_server(
    server_args: ServerArgs,
    model_overide_args: Optional[dict] = None,
    pipe_finish_writer: Optional[mp.connection.Connection] = None,
):
    server_args.check_server_args()
	if server_args.dp_size == 1:
        start_process = start_controller_process_single
    else:
        start_process = start_controller_process_multi
    proc_controller = mp.Process(
        target=start_process,
        args=(server_args, port_args, pipe_controller_writer, model_overide_args),
    )
</code></pre>
<p>会根据 dp_size 分别调用 <code>start_controller_process_single</code>​ 和 <code>start_controller_process_multi</code>​，简单点我们就来看 single 版本，会构造一个 <code>ControllerSingle</code>​并陷入loop。</p>
<p>python/sglang/srt/managers/controller_single.py</p>
<pre><code class="language-python">    try:
        controller = ControllerSingle(
            server_args,
            port_args,
            model_overide_args,
            gpu_ids,
            is_data_parallel_worker,
            dp_worker_id,
            queue,
        )
    except Exception:
        pipe_writer.send(get_exception_traceback())
        raise

    pipe_writer.send(&quot;init ok&quot;)

    try:
        controller.loop_for_forward()
</code></pre>
<p>‍</p>
<p>controller 的 loop_for_forward 是一个接收请求并调用 tp_server 进行执行的简单控制器</p>
<pre><code class="language-python">    def loop_for_forward(self):
        while True:
            if not self.is_dp_worker:
                recv_reqs = self.recv_requests_from_zmq()
            else:
                recv_reqs = self.recv_requests_from_mp_queue()

            if self.tp_size &gt; 1:
                broadcast_recv_input(recv_reqs, 0, self.tp_cpu_group)

            out_pyobjs = self.tp_server.exposed_step(recv_reqs)

            for obj in out_pyobjs:
                self.send_to_detokenizer.send_pyobj(obj)
</code></pre>
<p>接收消息可以从 zmq 或着 multiprocessing.Queue 中拿数据。</p>
<p>‍</p>
<p>tp_server 是用来真正执行调用 forward 的：</p>
<p>python/sglang/srt/managers/tp_worker.py</p>
<pre><code class="language-python">    def exposed_step(self, recv_reqs):
        try:
            # Recv requests
            for recv_req in recv_reqs:
                if isinstance(recv_req, TokenizedGenerateReqInput):
                    self.handle_generate_request(recv_req)
                elif isinstance(recv_req, FlushCacheReq):
                    self.flush_cache()
                elif isinstance(recv_req, AbortReq):
                    self.abort_request(recv_req)
                else:
                    raise ValueError(f&quot;Invalid request: {recv_req}&quot;)

            # Forward
            self.forward_step()
</code></pre>
<p>通过 handle_generate_request 来将请求放入 waiting 队列，forward_step 来处理队列里的请求。</p>
<p>‍</p>
<p>python/sglang/srt/managers/tp_worker.py</p>
<pre><code class="language-python">    @torch.inference_mode()
    def forward_step(self):
        new_batch = self.get_new_prefill_batch()

        if new_batch is not None:
            # Run a new prefill batch
            self.forward_prefill_batch(new_batch)
        else:
            # Run a decode batch
            if self.running_batch is not None:
                # Run a few decode batches continuously for reducing overhead
                for _ in range(global_config.num_continue_decode_steps):
                    self.num_generated_tokens += len(self.running_batch.reqs)
                    self.forward_decode_batch(self.running_batch)
</code></pre>
<p>forward_step 函数会根据 batch 的情况来执行 prefill 或者 decode</p>
<p>‍</p>
<p>如果是 prefill 就会调用 batch 的 prepare_for_extend，然后以 extend 的模式调用 model_runner 的 forward</p>
<p>如果是 decode 模式就会调用 batch 的 prepare_for_decode，然后以 decode 的模式调用 model_runner 的 forward</p>
<pre><code class="language-python">    def prepare_for_decode(self, input_ids=None):
        if input_ids is None:
            input_ids = [
                r.output_ids[-1] if r.output_ids else r.input_ids[-1] for r in self.reqs
            ]
</code></pre>
<p>prepare_for_decode 会取每个句子的最后一个 token，因为decode就是以最后一个token去计算然后生成下一个token的。</p>
<p>‍</p>
<p>在 model_runner 里才会真正调用到具体模型的 forward 函数</p>
<p>‍</p>
<pre><code class="language-python">    def forward_extend(self, batch: Batch):
        input_metadata = InputMetadata.create(
            self,
            forward_mode=ForwardMode.EXTEND,
            req_pool_indices=batch.req_pool_indices,
            seq_lens=batch.seq_lens,
            prefix_lens=batch.prefix_lens,
            position_ids_offsets=batch.position_ids_offsets,
            out_cache_loc=batch.out_cache_loc,
            top_logprobs_nums=batch.top_logprobs_nums,
            return_logprob=batch.return_logprob,
        )
        return self.model.forward(
            batch.input_ids, input_metadata.positions, input_metadata
        )
</code></pre>
<p>‍</p>
<p>在模型中会调用对应的 attention 的 forward，对于 decode 和 extend 都会先把本步的 kv 保存在kvcache 中，然后调用各自的 kernel.</p>
<p>python/sglang/srt/layers/radix_attention.py</p>
<pre><code class="language-python">    def extend_forward_triton(self, q, k, v, input_metadata: InputMetadata):
        o = torch.empty_like(q)
        self.store_kv_cache(k, v, input_metadata)
        extend_attention_fwd(
            q.view(-1, self.tp_q_head_num, self.head_dim),
            k.contiguous(),
            v.contiguous(),
            o.view(-1, self.tp_q_head_num, self.head_dim),
            input_metadata.token_to_kv_pool.get_key_buffer(self.layer_id),
            input_metadata.token_to_kv_pool.get_value_buffer(self.layer_id),
            input_metadata.req_to_token_pool.req_to_token,
            input_metadata.req_pool_indices,
            input_metadata.triton_start_loc,
            input_metadata.seq_lens,
            input_metadata.triton_prefix_lens,
            input_metadata.extend_start_loc,
            input_metadata.extend_seq_lens,
            input_metadata.triton_max_seq_len,
            input_metadata.triton_max_extend_len,
            sm_scale=self.scaling,
            logit_cap=self.logit_cap,
        )

        return o

    def decode_forward_triton(self, q, k, v, input_metadata: InputMetadata):
        o = torch.empty_like(q)
        self.store_kv_cache(k, v, input_metadata)

        token_attention_fwd(
            q.view(-1, self.tp_q_head_num, self.head_dim),
            input_metadata.token_to_kv_pool.get_key_buffer(self.layer_id),
            input_metadata.token_to_kv_pool.get_value_buffer(self.layer_id),
            o.view(-1, self.tp_q_head_num, self.head_dim),
            input_metadata.req_to_token_pool.req_to_token,
            input_metadata.req_pool_indices,
            input_metadata.triton_start_loc,
            input_metadata.seq_lens,
            input_metadata.triton_max_seq_len,
            input_metadata.total_num_tokens,
            sm_scale=self.scaling,
            logit_cap=self.logit_cap,
        )
</code></pre>
<p>‍</p>
<p>​<code>input_metadata.token_to_kv_pool.get_key_buffer(self.layer_id)</code>​</p>
<p>可以拿到 key cache 的位置。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[vllm prefill 和 decode 的kernel代码解读 ]]></title>
        <id>https://DragonFive.github.io/post/vllm-prefill-decode-kernel/</id>
        <link href="https://DragonFive.github.io/post/vllm-prefill-decode-kernel/">
        </link>
        <updated>2024-04-10T06:01:28.000Z</updated>
        <content type="html"><![CDATA[<p><a href="https://github.com/vllm-project/vllm/pull/3462">https://github.com/vllm-project/vllm/pull/3462</a></p>
<p>在3月25号 vllm 对 attention 进行了一下重构，估计一月一次的版本发布马上就来了，可以先看看代码了解它的工作机制。</p>
<p>我们已经知道推理是分位 prefill 阶段和 decode 阶段的，这个是在 vllm scheudler里控制的，</p>
<p>prefill阶段 先把句子设置为 prefill 状态，执行一下图，这prefill阶段会填充 kv_cache。</p>
<h2 id="0-整体执行">0. 整体执行</h2>
<p>在 vllm 的 model_runner.py 里 execute_model 时调用模型的 forward 生成logit 然后做 sample.</p>
<p>代码位于 vllm/worker/model_runner.py</p>
<pre><code class="language-python">    def execute_model(
        self,
        seq_group_metadata_list: Optional[List[SequenceGroupMetadata]],
        kv_caches: List[torch.Tensor],
    ) -&gt; Optional[SamplerOutput]:
        # ......
        # 0. 准备数据
		(input_tokens, input_positions, attn_metadata, sampling_metadata,
         lora_requests, lora_mapping, multi_modal_input
         ) = self.prepare_input_tensors(seq_group_metadata_list)
		model_executable = self.model
        # 1. 执行模型前向
		hidden_states = model_executable(**execute_model_kwargs)
        # Compute the logits.
        logits = self.model.compute_logits(hidden_states, sampling_metadata)

        # Only perform sampling in the driver worker.
        if not sampling_metadata.perform_sampling:
            return None

        # Sample the next token.
 		# 3. 采样出下一个token
        output = self.model.sample(
            logits=logits,
            sampling_metadata=sampling_metadata,
        )
        return output
</code></pre>
<h2 id="1-准备输入">1、准备输入</h2>
<p>这里是第一个关键点，也就是 prefill 把整个query 输入模型，decode 把当前token输入模型</p>
<pre><code class="language-python">    def prepare_input_tensors(
        self,
        seq_group_metadata_list: Optional[List[SequenceGroupMetadata]],
    ) -&gt; Tuple[torch.Tensor, torch.Tensor, AttentionMetadata, SamplingMetadata,
               Set[int], LoRAMapping, torch.Tensor]:
        if self.is_driver_worker:
            # NOTE: We assume that all sequences in the group are all prompts or
            # all decodes.
            is_prompt = seq_group_metadata_list[0].is_prompt
            # Prepare input tensors.
            if is_prompt:
                # 1、如果是 prefill 就调用 _prepare_prompt 
                (input_tokens, input_positions, attn_metadata, prompt_lens,
                 subquery_lens, lora_index_mapping, lora_prompt_mapping,
                 lora_requests, multi_modal_input
                 ) = self._prepare_prompt(seq_group_metadata_list)
            else:
				# 2、# 1、如果是 decode 就调用 _prepare_decode 
                (input_tokens, input_positions, attn_metadata,
                 lora_index_mapping, lora_prompt_mapping,
                 lora_requests) = self._prepare_decode(seq_group_metadata_list)
                prompt_lens = []
                subquery_lens = None
</code></pre>
<p>​<code>_prepare_prompt</code>​ 是去拿到 prefill 的 token</p>
<pre><code class="language-python">def _prepare_prompt(
        self,
        seq_group_metadata_list: List[SequenceGroupMetadata],
    ) -&gt; Tuple[torch.Tensor, torch.Tensor, AttentionMetadata, List[int],
               List[int], List[int], List[int], Set[LoRARequest],
               torch.Tensor]:
		prefill_end = min(seq_data.get_len(),
                              computed_len + token_chunk_size)
            # TODO(sang): Rename it after chunked prefill is introduced.
            prompt_tokens = seq_data.get_token_ids()[computed_len:prefill_end]
</code></pre>
<p>可以看到 prefill 的时候是把整个seq 的 prompt 的 token 都拿到</p>
<pre><code class="language-python">def _prepare_decode(
        self,
        seq_group_metadata_list: List[SequenceGroupMetadata],
    ) -&gt; Tuple[torch.Tensor, torch.Tensor, AttentionMetadata, List[int],
               List[int], Set[LoRARequest]]:
        assert len(seq_group_metadata_list) &gt; 0
        input_tokens: List[int] = []
        for seq_group_metadata in seq_group_metadata_list:
        
            for seq_id in seq_ids:
                seq_data = seq_group_metadata.seq_data[seq_id]
                generation_token = seq_data.get_last_token_id()
                input_tokens.append(generation_token)
</code></pre>
<p>可以看到 decode 得时候是拿到最后的一个 token （get_last_token_id）</p>
<p>‍</p>
<p>‍</p>
<h2 id="2-attention-准备">2、attention 准备</h2>
<p>在模型 forward的时候会调到每一层的forward，在 attention 这一层就进入 attention 的 forward，以llama 为例，在 vllm/model_executor/models/llama.py 中，LlamaAttention 部分会通过 qkw linear层 算出 q，k，v，这里在 prefill 阶段算的 q k v是seqlen 的长query对应的值，这里算出了 k 和 v，就可以把 k 和 v 写进 kvcache 里。</p>
<pre><code class="language-python">    def forward(
        self,
        positions: torch.Tensor,
        hidden_states: torch.Tensor,
        kv_cache: torch.Tensor,
        attn_metadata: AttentionMetadata,
    ) -&gt; torch.Tensor:
        qkv, _ = self.qkv_proj(hidden_states)
        q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
        q, k = self.rotary_emb(positions, q, k)
        attn_output = self.attn(q, k, v, kv_cache, attn_metadata)
        output, _ = self.o_proj(attn_output)
        return output
</code></pre>
<p>‍</p>
<p>在 vllm/attention/layer.py 中，会调用真正的attention 实现</p>
<pre><code class="language-python">    def forward(
        self,
        query: torch.Tensor,
        key: torch.Tensor,
        value: torch.Tensor,
        kv_cache: Optional[torch.Tensor],
        attn_metadata: AttentionMetadata,
    ) -&gt; torch.Tensor:
        return self.impl.forward(query, key, value, kv_cache, attn_metadata)
</code></pre>
<p>‍</p>
<h2 id="3-写入kv-cache">3、写入kv cache</h2>
<p>以flash_attention 为例，在 vllm/attention/backends/flash_attn.py 中，调用 forward 函数时，会把当前传入的 key 和 value 写入 kv cache 里，如果是prefill，那写的就是整个prompt的key 和 value，如果是decode 阶段，那就是写当前token的</p>
<pre><code class="language-python">class FlashAttentionImpl(AttentionImpl):
    def forward(
        self,
        query: torch.Tensor,
        key: torch.Tensor,
        value: torch.Tensor,
        kv_cache: torch.Tensor,
        attn_metadata: FlashAttentionMetadata,
    ) -&gt; torch.Tensor:
      
        num_tokens, hidden_size = query.shape
        # Reshape the query, key, and value tensors.
        query = query.view(-1, self.num_heads, self.head_size)
        key = key.view(-1, self.num_kv_heads, self.head_size)
        value = value.view(-1, self.num_kv_heads, self.head_size)

        if kv_cache is not None:
            key_cache, value_cache = PagedAttention.split_kv_cache(
                kv_cache, self.num_kv_heads, self.head_size)

            # Reshape the input keys and values and store them in the cache.
            # If kv_cache is not provided, the new key and value tensors are
            # not cached. This happens during the initial memory profiling run.
            PagedAttention.write_to_paged_cache(key, value, key_cache,
                                                value_cache,
                                                attn_metadata.slot_mapping,
                                                attn_metadata.kv_cache_dtype)
</code></pre>
<p>通过 write_to_paged_cache 把 key 和 value 写入 kv cache。</p>
<p>‍</p>
<h2 id="4-prefill-和-decode-执行kernel">4、prefill 和 decode 执行kernel</h2>
<p>‍</p>
<p>然后 prefill 和 decode 调用自己的 kernel，传入 kcache 和 vcache 进行 load</p>
<p>‍</p>
<pre><code class="language-python">        if attn_metadata.is_prompt:
            # Prompt run.
            if kv_cache is None or attn_metadata.block_tables.numel() == 0:
                # normal attention
                # When block_tables are not filled, it means q and k are the
                # prompt, and they have the same length.
                output = flash_attn_varlen_func(
                    q=query,
                    k=key,
                    v=value,
                    cu_seqlens_q=attn_metadata.seq_start_loc,
                    cu_seqlens_k=attn_metadata.seq_start_loc,
                    max_seqlen_q=attn_metadata.max_prompt_len,
                    max_seqlen_k=attn_metadata.max_prompt_len,
                    softmax_scale=self.scale,
                    causal=True,
                    window_size=self.sliding_window,
                    alibi_slopes=self.alibi_slopes,
                )
            else:
                # prefix-enabled attention
                output = PagedAttention.forward_prefix(
                    query,
                    key,
                    value,
                    key_cache,
                    value_cache,
                    attn_metadata.block_tables,
                    attn_metadata.subquery_start_loc,
                    attn_metadata.prompt_lens_tensor,
                    attn_metadata.context_lens,
                    attn_metadata.max_subquery_len,
                    self.alibi_slopes,
                )
        else:
            # Decoding run.
            output = PagedAttention.forward_decode(
                query,
                key_cache,
                value_cache,
                attn_metadata.block_tables,
                attn_metadata.context_lens,
                attn_metadata.max_context_len,
                attn_metadata.kv_cache_dtype,
                self.num_kv_heads,
                self.scale,
                self.alibi_slopes,
            )
</code></pre>
<p>‍</p>
<p>prefill 调用 <code>flash_attn_varlen_func</code>​ 对应的 kernel</p>
<p>以 forward_decode 为例，在 csrc/attention/attention_kernels.cu 中真正执行 kernel</p>
<pre><code class="language-python">__device__ void paged_attention_kernel(
  float* __restrict__ exp_sums,           // [num_seqs, num_heads, max_num_partitions]
  float* __restrict__ max_logits,         // [num_seqs, num_heads, max_num_partitions]
  scalar_t* __restrict__ out,             // [num_seqs, num_heads, max_num_partitions, head_size]
  const scalar_t* __restrict__ q,         // [num_seqs, num_heads, head_size]
  const cache_t* __restrict__ k_cache,    // [num_blocks, num_kv_heads, head_size/x, block_size, x]
  const cache_t* __restrict__ v_cache,    // [num_blocks, num_kv_heads, head_size, block_size]
  const int num_kv_heads,                 // [num_heads]
</code></pre>
<p>‍</p>
<pre><code class="language-cpp">// 向量化访问设置
constexpr int VEC_SIZE = MAX(16 / (THREAD_GROUP_SIZE * sizeof(scalar_t)), 1);
using K_vec = typename Vec&lt;scalar_t, VEC_SIZE&gt;::Type;
using Q_vec = typename Vec&lt;scalar_t, VEC_SIZE&gt;::Type;
</code></pre>
<p>为了提高内存访问效率，代码使用向量化加载，每次加载16字节数据。</p>
<p>‍</p>
<ul>
<li>加载Query数据</li>
</ul>
<pre><code class="language-cpp">// 加载query到寄存器
const scalar_t* q_ptr = q + seq_idx * q_stride + head_idx * HEAD_SIZE;
__shared__ Q_vec q_vecs[THREAD_GROUP_SIZE][NUM_VECS_PER_THREAD];

#pragma unroll
for (int i = thread_group_idx; i &lt; NUM_VECS_PER_THREAD; i += NUM_THREAD_GROUPS) {
  const int vec_idx = thread_group_offset + i * THREAD_GROUP_SIZE;
  q_vecs[thread_group_offset][i] = *reinterpret_cast&lt;const Q_vec*&gt;(q_ptr + vec_idx * VEC_SIZE);
}
__syncthreads();
</code></pre>
<p>每个线程组协作加载一个完整的query向量到共享内存。</p>
<ul>
<li>计算Query-Key点积</li>
</ul>
<pre><code class="language-cpp">// 共享内存规划
extern __shared__ char shared_mem[];
float* logits = reinterpret_cast&lt;float*&gt;(shared_mem);
__shared__ float red_smem[2 * NUM_WARPS];

// 初始化最大logit值
float qk_max = -FLT_MAX;

// 遍历key块
const int* block_table = block_tables + seq_idx * max_num_blocks_per_seq;
for (int block_idx = start_block_idx + warp_idx; block_idx &lt; end_block_idx; block_idx += NUM_WARPS) {
  const int64_t physical_block_number = static_cast&lt;int64_t&gt;(block_table[block_idx]);
  
  // 加载key并计算点积
  for (int i = 0; i &lt; NUM_TOKENS_PER_THREAD_GROUP; i++) {
    const int physical_block_offset = (thread_group_idx + i * WARP_SIZE) % BLOCK_SIZE;
    const int token_idx = block_idx * BLOCK_SIZE + physical_block_offset;
    K_vec k_vecs[NUM_VECS_PER_THREAD];
  
    // 加载key向量
    #pragma unroll
    for (int j = 0; j &lt; NUM_VECS_PER_THREAD; j++) {
      // 计算key缓存指针
      const cache_t* k_ptr = k_cache + physical_block_number * kv_block_stride
                                     + kv_head_idx * kv_head_stride
                                     + physical_block_offset * x;
      // 加载key向量(支持FP8量化)
      // ...
    }
  
    // 计算点积
    float qk = scale * Qk_dot&lt;scalar_t, THREAD_GROUP_SIZE&gt;::dot(q_vecs[thread_group_offset], k_vecs);
  
    // 添加ALiBi位置编码偏置
    qk += (alibi_slope != 0) ? alibi_slope * (token_idx - context_len + 1) : 0;
  
    // 存储结果到共享内存
    if (thread_group_offset == 0) {
      const bool mask = token_idx &gt;= context_len;
      logits[token_idx - start_token_idx] = mask ? 0.f : qk;
      qk_max = mask ? qk_max : fmaxf(qk_max, qk);
    }
  }
}
</code></pre>
<p>这部分是核心计算，每个线程组加载key并计算与query的点积，同时应用ALiBi位置编码。</p>
<p>在这里读取 v_cache 并与logit 进行计算</p>
<pre><code class="language-python">    const cache_t* v_ptr = v_cache + physical_block_number * kv_block_stride
                                   + kv_head_idx * kv_head_stride;
#pragma unroll
    for (int i = 0; i &lt; NUM_ROWS_PER_THREAD; i++) {
      const int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER;
      if (row_idx &lt; HEAD_SIZE) {
        const int offset = row_idx * BLOCK_SIZE + physical_block_offset;
        V_vec v_vec;
        if constexpr (IS_FP8_E5M2_KV_CACHE) {
#ifdef ENABLE_FP8_E5M2
          V_quant_vec v_quant_vec = *reinterpret_cast&lt;const V_quant_vec*&gt;(v_ptr + offset);
          // Vector conversion from V_quant_vec to V_vec.
          v_vec = fp8_e5m2_unscaled::vec_conversion&lt;V_vec, V_quant_vec&gt;(v_quant_vec);
#else
          assert(false);
#endif
        } else {
          v_vec = *reinterpret_cast&lt;const V_vec*&gt;(v_ptr + offset);
        }
        if (block_idx == num_context_blocks - 1) {
          // NOTE(woosuk): When v_vec contains the tokens that are out of the context,
          // we should explicitly zero out the values since they may contain NaNs.
          // See https://github.com/vllm-project/vllm/issues/641#issuecomment-1682544472
          scalar_t* v_vec_ptr = reinterpret_cast&lt;scalar_t*&gt;(&amp;v_vec);
#pragma unroll
          for (int j = 0; j &lt; V_VEC_SIZE; j++) {
            v_vec_ptr[j] = token_idx + j &lt; context_len ? v_vec_ptr[j] : zero_value;
          }
        }
        accs[i] += dot(logits_vec, v_vec);
      }
    }
  }
</code></pre>
<p>‍</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[megetron server精度问题排查记录]]></title>
        <id>https://DragonFive.github.io/post/megetron-server-accuracy/</id>
        <link href="https://DragonFive.github.io/post/megetron-server-accuracy/">
        </link>
        <updated>2023-10-22T09:20:47.000Z</updated>
        <content type="html"><![CDATA[<p>不同队列megatron server logits不一致，逐层对比第一次出现diff是在all reduce之后，NCCL默认调用Ring，<strong>改为Tree后前向logits能bit位一致</strong>I</p>
<p>通过逐层对比激活值发现，在第一次attention里经过linear_proj后开始<strong>出现diff</strong>，跟进发现是在进行<strong>all_reduce通信后</strong>激活无法bit位一致, 怀疑NCCL通信导致的</p>
<p>NCCL_NVLS_ENABLE = 0 禁用NVLink SHARP</p>
<ul>
<li><strong>export</strong> NCCL_MAX_NCHANNELS=8 （对齐不同队列的channel数）</li>
<li><strong>export</strong> NCCL_MIN_NCHANNELS=8（对齐不同队列的channel数）</li>
</ul>
<p>在使用 NCCL_ALGO 为 Ring 时，如果不配置 channel 数，不同队列多次评测logits bit位不一致。<br>
如果 NCCL_ALGO 为 Tree 时不配置 channel 数，不同队列多次评测logits bit位是一致的。</p>
<p>nvlink带宽不一样 channel数不一样，最后nccl选择的算法也不一样, 两个队列强制设置相同channel后，前向logits能<strong>bit位一致</strong></p>
<pre><code class="language-python">    os.environ['NCCL_NVLS_ENABLE'] = '0'
    os.environ['NCCL_ALGO'] = 'Tree'
</code></pre>
<pre><code class="language-bash">export NCCL_MAX_NCHANNELS=8
export NCCL_MIN_NCHANNELS=8
</code></pre>
<p>参考资料：<a href="https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/env.html">nccl env variable</a></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[LLM 的参数量与计算量]]></title>
        <id>https://DragonFive.github.io/post/llm-params-flops/</id>
        <link href="https://DragonFive.github.io/post/llm-params-flops/">
        </link>
        <updated>2023-08-13T09:12:53.000Z</updated>
        <content type="html"><![CDATA[<p>‍</p>
<p>‍</p>
<p>结论来自：<a href="https://zhuanlan.zhihu.com/p/624740065">分析transformer模型的参数量、计算量、中间激活、KV cache</a></p>
<p>‍</p>
<p>从托马斯那里的一些takeaway</p>
<p>‍</p>
<p><img src="https://DragonFive.github.io/post-images/1741338851093.png" alt="" loading="lazy"><br>
‍</p>
<p>当 h &gt;&gt; L 时 前向计算量约为 24BLH^2  * layers，参数量约为 12H^2 * layers</p>
<p>前向计算量/参数量 = 2</p>
<p>后向计算量=2前向计算量</p>
<p>所以总计算量  = 6 * 参数量，一次训练迭代中，对于每个token，每个模型参数，需要进行 2∗3=6 次浮点数运算。</p>
<p>所以我们知道了参数量，知道了总的token的 token 数，就可以算出训练完一个 epoch 需要的 flops，这样除以 gpu 的可达算力，就可以得到训练时间</p>
<p>一般来讲，<strong>GPU利用率一般在 0.3∼0.55 之间</strong>。token数最好为 模型参数的10倍到100倍。</p>
<p>对于每个token，每个模型参数，进行2次浮点数计算。使用激活重计算技术来减少中间激活显存（下文会详细介绍）需要进行一次额外的前向传递，因此前向传递 + 后向传递 + 激活重计算的系数=1+2+1=4。使用<strong>激活重计算</strong>的一次训练迭代中，对于每个token，每个模型参数，需要进行 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>2</mn><mo>∗</mo><mn>4</mn><mo>=</mo><mn>8</mn></mrow><annotation encoding="application/x-tex">2*4=8</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">4</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">8</span></span></span></span></p>
<p><strong>在给定训练tokens数、硬件环境配置的情况下，训练transformer模型的计算时间为</strong>：</p>
<p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mtext> 训练时间 </mtext><mo>≈</mo><mfrac><mrow><mn>8</mn><mo>×</mo><mtext> tokens数 </mtext><mo>×</mo><mtext> 模型参数量 </mtext></mrow><mrow><mi>G</mi><mi>P</mi><mi>U</mi><mtext> 数 </mtext><mo>×</mo><mi>G</mi><mi>P</mi><mi>U</mi><mtext> 峰值 </mtext><mi>f</mi><mi>l</mi><mi>o</mi><mi>p</mi><mi>s</mi><mo>×</mo><mi>G</mi><mi>P</mi><mi>U</mi><mtext> 利用率 </mtext></mrow></mfrac></mrow><annotation encoding="application/x-tex">\text { 训练时间 } \approx \frac{8 \times \text { tokens数 } \times \text { 模型参数量 }}{G P U \text { 数 } \times G P U \text { 峰值 } f l o p s \times G P U \text { 利用率 }}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord text"><span class="mord"> </span><span class="mord cjk_fallback">训练时间</span><span class="mord"> </span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.3612159999999998em;vertical-align:-0.481108em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8801079999999999em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">G</span><span class="mord mathdefault mtight" style="margin-right:0.13889em;">P</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">U</span><span class="mord text mtight"><span class="mord mtight"> </span><span class="mord cjk_fallback mtight">数</span><span class="mord mtight"> </span></span><span class="mbin mtight">×</span><span class="mord mathdefault mtight">G</span><span class="mord mathdefault mtight" style="margin-right:0.13889em;">P</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">U</span><span class="mord text mtight"><span class="mord mtight"> </span><span class="mord cjk_fallback mtight">峰值</span><span class="mord mtight"> </span></span><span class="mord mathdefault mtight" style="margin-right:0.10764em;">f</span><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">p</span><span class="mord mathdefault mtight">s</span><span class="mbin mtight">×</span><span class="mord mathdefault mtight">G</span><span class="mord mathdefault mtight" style="margin-right:0.13889em;">P</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">U</span><span class="mord text mtight"><span class="mord mtight"> </span><span class="mord cjk_fallback mtight">利用率</span><span class="mord mtight"> </span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">8</span><span class="mbin mtight">×</span><span class="mord text mtight"><span class="mord mtight"> tokens</span><span class="mord cjk_fallback mtight">数</span><span class="mord mtight"> </span></span><span class="mbin mtight">×</span><span class="mord text mtight"><span class="mord mtight"> </span><span class="mord cjk_fallback mtight">模型参数量</span><span class="mord mtight"> </span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.481108em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></p>
<p>‍</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[fp16训练的问题]]></title>
        <id>https://DragonFive.github.io/post/fp16-train-problem/</id>
        <link href="https://DragonFive.github.io/post/fp16-train-problem/">
        </link>
        <updated>2022-07-16T08:06:39.000Z</updated>
        <content type="html"><![CDATA[<p>‍</p>
<p>float16的组成分为了三个部分：</p>
<ol>
<li>最高位表示符号位；</li>
<li>有5位表示exponent位;</li>
<li>有10位表示fraction位;</li>
</ol>
<figure data-type="image" tabindex="1"><img src="https://DragonFive.github.io/post-images/1741334892432.png" alt="" loading="lazy"></figure>
<h2 id="fp16bf16-在计算中的精度问题">FP16/BF16 在计算中的精度问题</h2>
<h3 id="141-溢出问题">1.4.1 溢出问题</h3>
<ol>
<li>FP16 的可表示范围较 FP32 等更小，容易触发上、下溢出问题。</li>
</ol>
<p>关于 FP16 的上下溢出可以参考图 1-3。对于绝对值大于 65504 的数，触发上溢出会舍入到±INF；对于绝对值小于的数，触发下溢出舍入到 0，具体示例可参考下表。</p>
<p>‍</p>
<p>​<img src="https://www.paddlepaddle.org.cn/documentation/docs/zh/_images/overflow.png" alt="../../_images/overflow.png" loading="lazy"></p>
<p>​</p>
<ol>
<li>BF16 则因为阶码同 FP32 等长，因此并不容易出现上下溢出问题。</li>
</ol>
<h3 id="142-舍入问题">1.4.2 舍入问题</h3>
<ol>
<li>
<p>FP16 格式的浮点数最多只能表示 3 位有效数字，所以各浮点区间的固定间隔都是 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>I</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>r</mi><mi>v</mi><mi>a</mi><mi>l</mi><mo>=</mo><mi>M</mi><mi>i</mi><mi>n</mi><mi mathvariant="normal">∗</mi><msup><mn>2</mn><mrow><mi mathvariant="normal">−</mi><mn>10</mn></mrow></msup></mrow><annotation encoding="application/x-tex">Interval=Min∗2^{−10}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.07847em;">I</span><span class="mord mathdefault">n</span><span class="mord mathdefault">t</span><span class="mord mathdefault">e</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault" style="margin-right:0.03588em;">v</span><span class="mord mathdefault">a</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="mord mathdefault">i</span><span class="mord mathdefault">n</span><span class="mord">∗</span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">1</span><span class="mord mtight">0</span></span></span></span></span></span></span></span></span></span></span></span>，也就是区间最小值的1/1024。</p>
<ol>
<li>
<p>因此当 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mrow><mi mathvariant="normal">累</mi><mi mathvariant="normal">加</mi><mi mathvariant="normal">值</mi></mrow><mrow><mi mathvariant="normal">加</mi><mi mathvariant="normal">数</mi></mrow></mfrac><mo>&gt;</mo><mn>211</mn></mrow><annotation encoding="application/x-tex">\frac{累加值}{加数}&gt;211</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8841em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.394em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord cjk_fallback mtight">加</span><span class="mord cjk_fallback mtight">数</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord cjk_fallback mtight">累</span><span class="mord cjk_fallback mtight">加</span><span class="mord cjk_fallback mtight">值</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span><span class="mord">1</span><span class="mord">1</span></span></span></span> 时，计算结果超过了 3 位有效数字；这会造成累加值最终无法被有效表示，累加结果会被舍入到累加值本身。</p>
<ol>
<li>数值:1+0.0001=1.0001</li>
<li>FP16: 1+0.0001=1.0001</li>
</ol>
</li>
<li>
<p>FP32 格式拥有 7 位有效数字的表达效果，因此当 FP32 格式向 FP16 格式转化时，也会出现精度的舍入问题。</p>
<ol>
<li>FP32：0.1234567</li>
<li>FP16：0.1235</li>
</ol>
</li>
</ol>
</li>
<li>
<p>BF16 格式的浮点数，在各个区间内的固定间隔是 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>I</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>r</mi><mi>v</mi><mi>a</mi><mi>l</mi><mo>=</mo><mi>M</mi><mi>i</mi><mi>n</mi><mo>∗</mo><msup><mn>2</mn><mrow><mo>−</mo><mn>7</mn></mrow></msup></mrow><annotation encoding="application/x-tex">Interval=Min*2^{-7}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.07847em;">I</span><span class="mord mathdefault">n</span><span class="mord mathdefault">t</span><span class="mord mathdefault">e</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault" style="margin-right:0.03588em;">v</span><span class="mord mathdefault">a</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="mord mathdefault">i</span><span class="mord mathdefault">n</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">7</span></span></span></span></span></span></span></span></span></span></span></span>​ ，故 BF16 相较于 FP16 的精度更低，也更容易出现 FP16 中所阐述的计算舍入和转换精度丢失的问题。</p>
</li>
</ol>
<p>‍</p>
<p>下表摘自 <a href="https://en.wikipedia.org/wiki/Half-precision_floating-point_format">wikipedia fp16</a>，表示fp16不同区间的间隔，可见fp16的加法如果直接往param上加，会有很多失效。</p>
<h3 id="precision-limitations">Precision limitations</h3>
<table>
<thead>
<tr>
<th>Min</th>
<th>Max</th>
<th>interval</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>2<sup>−13</sup></td>
<td>2<sup>−24</sup></td>
</tr>
<tr>
<td>2<sup>−13</sup></td>
<td>2<sup>−12</sup></td>
<td>2<sup>−23</sup></td>
</tr>
<tr>
<td>2<sup>−12</sup></td>
<td>2<sup>−11</sup></td>
<td>2<sup>−22</sup></td>
</tr>
<tr>
<td>2<sup>−11</sup></td>
<td>2<sup>−10</sup></td>
<td>2<sup>−21</sup></td>
</tr>
<tr>
<td>2<sup>−10</sup></td>
<td>2<sup>−9</sup></td>
<td>2<sup>−20</sup></td>
</tr>
<tr>
<td>2<sup>−9</sup></td>
<td>2<sup>−8</sup></td>
<td>2<sup>−19</sup></td>
</tr>
<tr>
<td>2<sup>−8</sup></td>
<td>2<sup>−7</sup></td>
<td>2<sup>−18</sup></td>
</tr>
<tr>
<td>2<sup>−7</sup></td>
<td>2<sup>−6</sup></td>
<td>2<sup>−17</sup></td>
</tr>
<tr>
<td>2<sup>−6</sup></td>
<td>2<sup>−5</sup></td>
<td>2<sup>−16</sup></td>
</tr>
<tr>
<td>2<sup>−5</sup></td>
<td>2<sup>−4</sup></td>
<td>2<sup>−15</sup></td>
</tr>
<tr>
<td>2<sup>−4</sup></td>
<td>⁠1/8⁠</td>
<td>2<sup>−14</sup></td>
</tr>
<tr>
<td>⁠1/8⁠</td>
<td>⁠1/4⁠</td>
<td>2<sup>−13</sup></td>
</tr>
<tr>
<td>⁠1/4⁠</td>
<td>⁠1/2⁠</td>
<td>2<sup>−12</sup></td>
</tr>
<tr>
<td>⁠1/2⁠</td>
<td>1</td>
<td>2<sup>−11</sup></td>
</tr>
<tr>
<td>1</td>
<td>2</td>
<td>2<sup>−10</sup></td>
</tr>
<tr>
<td>2</td>
<td>4</td>
<td>2<sup>−9</sup></td>
</tr>
<tr>
<td>4</td>
<td>8</td>
<td>2<sup>−8</sup></td>
</tr>
<tr>
<td>8</td>
<td>16</td>
<td>2<sup>−7</sup></td>
</tr>
<tr>
<td>16</td>
<td>32</td>
<td>2<sup>−6</sup></td>
</tr>
<tr>
<td>32</td>
<td>64</td>
<td>2<sup>−5</sup></td>
</tr>
<tr>
<td>64</td>
<td>128</td>
<td>2<sup>−4</sup></td>
</tr>
<tr>
<td>128</td>
<td>256</td>
<td>⁠1/8⁠</td>
</tr>
<tr>
<td>256</td>
<td>512</td>
<td>⁠1/4⁠</td>
</tr>
<tr>
<td>512</td>
<td>1024</td>
<td>⁠1/2⁠</td>
</tr>
<tr>
<td>1024</td>
<td>2048</td>
<td>1</td>
</tr>
<tr>
<td>2048</td>
<td>4096</td>
<td>2</td>
</tr>
<tr>
<td>4096</td>
<td>8192</td>
<td>4</td>
</tr>
<tr>
<td>8192</td>
<td>16384</td>
<td>8</td>
</tr>
<tr>
<td>16384</td>
<td>32768</td>
<td>16</td>
</tr>
<tr>
<td>32768</td>
<td>65520</td>
<td>32</td>
</tr>
<tr>
<td>65520</td>
<td>∞</td>
<td>∞</td>
</tr>
</tbody>
</table>
<p>‍</p>
<h2 id="解决方法">解决方法</h2>
<h3 id="溢出问题">溢出问题</h3>
<p><strong>损失放大（Loss Scaling）</strong>  即使用了混合精度训练，还是会存在无法收敛的情况，原因是激活梯度的值太小，造成了下溢出（Underflow）。损失放大的思路是：</p>
<ol>
<li>反向传播前，将损失变化（dLoss）手动增大 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mn>2</mn><mi>k</mi></msup></mrow><annotation encoding="application/x-tex">2^k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.849108em;vertical-align:0em;"></span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span></span></span></span></span></span></span></span></span></span></span> 倍，因此反向传播时得到的中间变量（激活函数梯度）则不会溢出；</li>
<li>反向传播后，将权重梯度缩 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mn>2</mn><mi>k</mi></msup></mrow><annotation encoding="application/x-tex">2^k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.849108em;vertical-align:0em;"></span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span></span></span></span></span></span></span></span></span></span></span> 倍，恢复正常值。</li>
</ol>
<p>‍</p>
<p>混合精度训练并不意味着所有的模型参数都用fp16训练，例如做layernorm、batchnorm时的参数一般就保持fp32的形式，Loss也保持fp32的形式，主要原因还是这些数值的精度对模型训练过程影响较大，同时它们占据的存储也不大，因此维持原始形式，越高精越好。</p>
<p>loss scale 有两种方式</p>
<ul>
<li>常量损失放大</li>
<li>动量损失放大</li>
</ul>
<p>‍</p>
<p>上溢出可以通过 clip gradient 解决</p>
<p>‍</p>
<p>‍</p>
<p>‍</p>
<h3 id="舍入误差">舍入误差</h3>
<p>可以在参数更新时将 fp16 转成fp32，weights, activations, gradients 等数据在训练中都利用FP16来存储，同时拷贝一份FP32的weights，用于更新。 参考论文：<a href="https://arxiv.org/pdf/1710.03740">MIXED PRECISION TRAINING</a></p>
<p>‍<img src="https://DragonFive.github.io/post-images/1741334933305.png" alt="" loading="lazy"></p>
<p>‍</p>
<p>参考资料：</p>
<p><a href="https://zhuanlan.zhihu.com/p/79887894">【PyTorch】唯快不破：基于Apex的混合精度加速</a></p>
<p><a href="https://www.paddlepaddle.org.cn/documentation/docs/zh/dev_guides/amp_precision/amp_op_dev_guide_cn.html">paddle低精度算子支持开发规范</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/103685761">浅谈混合精度训练</a></p>
<p>‍</p>
]]></content>
    </entry>
</feed>