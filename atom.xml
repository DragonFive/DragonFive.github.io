<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://DragonFive.github.io/</id>
    <title>dragon</title>
    <updated>2021-07-01T12:52:50.682Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://DragonFive.github.io/"/>
    <link rel="self" href="https://DragonFive.github.io/atom.xml"/>
    <subtitle>Code is cheap, show me the theory</subtitle>
    <logo>https://DragonFive.github.io/images/avatar.png</logo>
    <icon>https://DragonFive.github.io/favicon.ico</icon>
    <rights>All rights reserved 2021, dragon</rights>
    <entry>
        <title type="html"><![CDATA[华为的《ScaleFreeCTR:a MixCache-based distributed training system for CTR》]]></title>
        <id>https://DragonFive.github.io/post/hua-wei-de-scalefreectr/</id>
        <link href="https://DragonFive.github.io/post/hua-wei-de-scalefreectr/">
        </link>
        <updated>2021-06-15T12:23:03.000Z</updated>
        <content type="html"><![CDATA[<p>华为诺亚方舟提出了SFCTR <a href="https://arxiv.org/pdf/2104.08542.pdf">ScaleFreeCTR: a MixCache-based distributed training system for CTR</a>，scalefree 可能是因为数据规模对训练吞吐没有影响，后面实验部分有具体的数据。</p>
<h1 id="一-动机与主要创新">一、动机与主要创新</h1>
<p>现有的分布式CTR训练框架使用CPU内存来保存和更新参数，使用gpu 进行前向和反向计算（也有用CPU的），会有两个瓶颈</p>
<ol>
<li>
<p>CPU 和 gpu 之间的pull 和 push 操作有一定的延迟</p>
</li>
<li>
<p>cpu 进行参数的同步和更新比较慢</p>
</li>
</ol>
<p>分布式训练的关键在于</p>
<ol>
<li>关键在于减少host_gpu之间的延迟</li>
<li>减少host-gpu 及gpu之间的数据传输量也很重要</li>
</ol>
<p>推荐中的参数有两个特点：</p>
<ol>
<li>实际 working parameters 比较少，sparse 参数和 MLP参数都很少</li>
<li>sparse 特征符合幂律分布，小部分特征被高频访问</li>
</ol>
<p>根据两个特点，可以有两个方法</p>
<ol>
<li>使用缓存机制减少 host-gpu 延迟</li>
<li>通过重组batch数据来减少参数传输量(unique?)</li>
</ol>
<p>由此提出了 SFCTR:<br>
在CPU中通过 虚拟sparse id op 来减少host-gpu 和gpu-gpu 的数据传输量，使用 mixcache 实验特征预取来减少传输延迟，使用3级pipeline 来减少整体训练时长。</p>
<p>系统将会在MindSpore 上开源，现在看似乎还没有开源。</p>
<h1 id="二-相关工作">二、相关工作</h1>
<h2 id="一业界前沿">（一）业界前沿</h2>
<p>为了充分利用GPU的能力和高速带宽<br>
英伟达的 hugeCtr 和 脸书 的 DLRM 把emb参数分成不同的份放在GPU HMB中，需要需要昂贵的GPU，不实用。<br>
腾讯的DES 和 百度的 HierPs 使用主存保存emb ，DES采用 field-aware 分片策略来reduce(减少or规约)GPU间的数据通信，但没有进行主存和GPU之间通信优化。<br>
HierPS使用大batch策略来在gpu中缓存使用大参数，以此减少传输延时。</p>
<p>Tensorflow, MxNet 和 PyTorch 并不能很好的支持大规模embedding的训练:</p>
<ul>
<li>PyTorch 中没有官方支持的ps</li>
<li>Mxnet 支持的模型大小因为实现问题受到了限制</li>
<li>tensorflow 使用它的ps后吞吐会严重下降</li>
</ul>
<p>为了提升tensorflow, money, pytorch较差的分布式性能，百度的horovod和字节的byteps 都支持不同的平台:</p>
<ul>
<li>horovod 使用百度的Ring-AllReduce实现来加速dense模型的训练</li>
<li>byteps 通过调度优先来在不同的层加速同步参数，优化不同层的顺序来在反向传播和前向计算的时候同步参数</li>
</ul>
<h2 id="二论文介绍的跟sfctr无关但挺有用的经验知识">（二）论文介绍的跟SFCTR无关，但挺有用的经验知识</h2>
<p>为了提高训练效率，有两种通用的做法：</p>
<ol>
<li>增量学习（batch训练的补充，用最近的数据更新模型）</li>
<li>分布式训练（使用额外的训练资源）</li>
</ol>
<p>CTR模型稀疏部分参数量太大，所以不能使用reduce 数据并行，大多数考虑用了模型并行。<br>
模型并行解决方案ps 架构的局限性：<br>
Ps server 保存并同步参数，worker执行前向和反向计算，</p>
<ul>
<li>worker pull and push from ps</li>
<li>Ps 从worker接收梯度之后进行同步<br>
分布式训练包括两个阶段：计算和参数同步。</li>
</ul>
<p>百度的综述 <a href="https://arxiv.org/abs/2003.05622">Distributed Hierarchical GPU Parameter Server for Massive Scale Deep Learning Ads Systems</a> 介绍了三种同步模式：</p>
<ul>
<li>BSP(bulk sync parallel)，严格对所有worker的更新进行同步</li>
<li>SSP(stale sync parallel)，对快worker 进行同步</li>
<li>ASP（async parallel）, 不同步gradient</li>
</ul>
<p>后两种方式虽然提升了训练效率，但是降低了模型性能。SFCTR 使用的是BSP，XDL使用的是ASP。</p>
<h1 id="三-sfctr-架构">三、SFCTR 架构</h1>
<p>SFCTR 由三部分构成</p>
<ul>
<li>Data-Loader, 提出虚拟sparse id op 来减少batch中重复的特征emb(unique?)</li>
<li>Host-Manager, 使用混合缓存策略来减少host-gpu延迟，MixCache 的管理器部分在 CPU 的内存中，MixCache 的缓冲区在 GPU 的 HBM 中</li>
<li>GPU-WORKER<br>
<img src="https://DragonFive.github.io//post-images/1625142600332.png" alt="" loading="lazy"></li>
</ul>
<p><strong>3级pipeline</strong><br>
把 Data-Loader,Host-Manager 和gpu worker 中，三阶段资源不同 Disk, CPU and GPU在三个不同的线程里完成</p>
<figure data-type="image" tabindex="1"><img src="https://DragonFive.github.io//post-images/1625142730082.png" alt="" loading="lazy"></figure>
<h2 id="1data-loader">（1）data loader</h2>
<p>sparse id op 来减少batch中重复的特征emb，就是xdl中的unique。减少host-gpu 和gpu-gpu 的数据传输量。</p>
<h2 id="2host-manager">（2）HOST-MANAGER</h2>
<p>MixCache用来减少延迟，在每个GPU的HBM 上申请一个cache buffer，使用modulo 哈希方法对 working parameters 进行分组，放在不同的GPU 上，embedding参数在data loader执行完VSI op 之后检查哪些参数 gpu 上没有就把那些参数传输到gpu 上。</p>
<p>当cache满了之后，满足两种情况的emb 会回传到host。</p>
<ol>
<li>参数完成了更新</li>
<li>下个batch不需要这个参数</li>
</ol>
<h2 id="3gpu-worker">（3）GPU-WORKER</h2>
<p>不同的GPU保存了不同的参数，所以前向和反向的时候都需要同步。<br>
前向传播，每个worekr从其它worker拿到batch所需要的参数，使用all-reduce 通信方式，一个gpu只需要跟另外两个gpu通信两次，首先通过gather_cache ，从cache buffer 中获得local common emb, 因为global_id 顺序一致，所以可以做all_reduce同步, 然后通过all_reduce 或者 global common emb，最后通过vis 算出来自己worker需要执行的batch emb<br>
<img src="https://DragonFive.github.io//post-images/1625142803471.png" alt="" loading="lazy"></p>
<p>梯度更新<br>
<img src="https://DragonFive.github.io//post-images/1625142812224.png" alt="" loading="lazy"></p>
<h1 id="四-sfctr-执行流程">四、SFCTR 执行流程</h1>
<p>执行流程<br>
<img src="https://DragonFive.github.io//post-images/1625142836322.png" alt="" loading="lazy"></p>
<ul>
<li>
<p>2-3行是 data loader 部分，有个虚拟Sparse Id OP，对batch Sparse ID 去重后形成 global_id，对于batch 中每个实例有个virtual_id，可以找到其对应的global_id ，跟XDL 的unique 操作很像。使用global_id, 各个gpu在同步的时候数据量就会少很多。</p>
</li>
<li>
<p>4-7行是 Host-Manager 部分，负责在主存中报错embedding参数（存得下吗？），使用mixcache把working parameters 放到 gpu 的cache buffer中。mixcache 还更新gpu cache buffer， 检查下一个batch需要哪些embedding ，预测哪些embedding未来一段时间不需要，在buffer满的时候进行pull, 发送数据到gpu，并对每个特征在gpu设置一个local_id (?)</p>
</li>
<li>
<p>9-15行是GPU部分，包括embedding查表，前向反向和参数更新</p>
</li>
</ul>
<p>host 和 gpu 是生产者与消费者模式</p>
<h1 id="五-一些对我们有用的实验">五、一些对我们有用的实验：</h1>
<h2 id="1环境">（1）环境</h2>
<p>GPU 集群使用 InfiniBand 连接，4台GPU服务器通过100Gb  RDMA提速</p>
<p>Intel Xeon Gold-5118 CPUs with 18 cores (36 threads), 8 Tesla V100 GPUs with 32 GB HBM，1GB内存。GPU之间PCI连接<br>
使用 Criteo-TB 数据库，使用filter构造10GB和100GB两个数据集，因为包括了优化器的信息，33×4B×80=10GB，所以实际parameter table是embedding table的3倍，所以实际上是 30GB和300GB的模型参数。</p>
<p>使用 DeepFM 模型，与 hugectr与ps mxnet对比</p>
<h2 id="2框架对比实验">（2）框架对比实验</h2>
<p>基于VSI OP，混合缓存机制，和三级pipeline，在10GB数据上SFCTR 在4机32卡上的吞吐量是psmxnet的1.6倍，hugectr的5.6倍，100GB 数据上是1.3倍和6.9倍<br>
如果GPU卡只有8个，hugectr在100GB数据上根本无法训练</p>
<figure data-type="image" tabindex="2"><img src="https://DragonFive.github.io//post-images/1625143001706.png" alt="" loading="lazy"></figure>
<h2 id="3vsi-op">（3）vsi op</h2>
<p>Host-gpu 数据传输量减少 94% ，g pu-gpu数据量减少88%</p>
<figure data-type="image" tabindex="3"><img src="https://DragonFive.github.io//post-images/1625143009606.png" alt="" loading="lazy"></figure>
<h2 id="4mixcache">（4）mixcache</h2>
<p>Cache 大小对传数据的影响<br>
2GB的cache可以把数据传输推迟到1000步之后<br>
如果cache 大小比较大，batch中要传输的数据的比例就会小，因为可以存更多高频特征<br>
12%（2GB）, 27%（0.5GB） and 29%（0.25GB）</p>
<figure data-type="image" tabindex="4"><img src="https://DragonFive.github.io//post-images/1625143020154.png" alt="" loading="lazy"></figure>
<h2 id="53级pipeline">（5）3级pipeline</h2>
<p>GPU-Worker 训练时间在pipeline中占比最高<br>
一个节点跑100GB数据，使用pipeline需要 75 s，不用pipeline就需要150s</p>
<h1 id="六-总结与思考">六、总结与思考</h1>
<p>文章写的通俗易懂，很有条理，related worker 也总结了很多训练的经验，工作很有实用性。<br>
作者提到未来的工作有两个方向</p>
<ol>
<li>提升通信效率，（使用all2all）</li>
<li>调查提升收敛速度的方法</li>
</ol>
<p>思考借鉴意义</p>
<ol>
<li>
<p>SFCTR相当于把图完全放在GPU中执行，没有进行图的分隔，所以实现起来更容易一些。CPU只是一个ps的存储和更新后落盘以及dataloader</p>
</li>
<li>
<p>CPU内存1T，而实验中的数据最大的为300GB，所以可以放在CPU内存中，其实我们的模型大小似乎也在几百GB，如果可以放在worker内存中，就没有必要单独弄一个ps server；<br>
如果模型超过1T，也可以融合AIBox的做法，使用SSD做cpu mem的缓存</p>
</li>
<li>
<p>pipeline 和 vsiop 其实 XDL 都有，只缺了缓存机制，但XDL如果不动ps这一块，参数的更新其实是在ps 上完成的，所以 ps 的 push 也会继续有延迟，参数预取只能解决pull 的问题，</p>
</li>
<li>
<p>但如果都在本地更新，那不同worker之间参数同步就会比较麻烦，所以缓存预取、更新后缓存失效再回传的机制必然依赖多机间 RDMA  单机allreduce 同步通信技术，ps存在的意义不大</p>
</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[百度的《AIBox: CTR Prediction Model Training on a Single Node》]]></title>
        <id>https://DragonFive.github.io/post/bai-du-de-lesslessaibox-ctr-prediction-model-training-on-a-single-nodegreatergreater/</id>
        <link href="https://DragonFive.github.io/post/bai-du-de-lesslessaibox-ctr-prediction-model-training-on-a-single-nodegreatergreater/">
        </link>
        <updated>2021-02-25T12:43:07.000Z</updated>
        <content type="html"><![CDATA[<p>AIBox 是百度提出的训练框架，论文 AIBox: CTR Prediction Model Training on a Single Node 进行了相关介绍。</p>
<h1 id="一-aibox-的技术创新及优势">一、AIBox 的技术创新及优势</h1>
<p>创新点与动机：<br>
AIBox 的核心想法就是想在一台机器上用GPU加速训练，但是参数实在太大了，所以就把计算密集的模型运算部分（joint learning）放在GPU完成，把取embedding 部分放在cpu部分完成(embedding learning)，这就是AiBox 的第一个创新：把网络切分为两部分。</p>
<p>但即便主存用了1TB 的内存，embedding 还是太大了，10^12 个key，每个key 的 weight 即使用8个字节存，key用8个字节存，也需要1.6TB , 所以论文提出了第二个创新：把embedding 存在SSD上，同时为了降低延迟和减少写操作对ssd寿命的影响，建立了二级缓存机制。</p>
<p>为了提高速度，AIBOX使用了流水线，把从hdfs 读数据(socket IO)，从SSD查Embedding （SSD io） 和 cpu+gpu 计算组成3阶段的 pipeline</p>
<p>优势：<br>
AIBOX不存在像分布式系统普遍存在的网络通信开销问题，然后在系统稳定性方面AIBOX与具有数千台机器的分布式集群相比更加稳定不会轻易宕机，而且在同步开销方面AIBOX只是涉及到一些内存锁和GPU片之间的少量通信。</p>
<h1 id="二-关于网络结构切分">二、关于网络结构切分</h1>
<p>the first module focuses on the embedding learning with high-dimensional &amp; sparse features and the second module is for joint learning with dense features resulted from the first module.</p>
<p>The embedding learning is processed on CPUs to help learn low dimensional dense embedding representations.</p>
<p>By transferring the learned embedding vectors from CPUs to GPUs, the computation-intensive joint learning module can make full use of the powerful GPUs for CTR prediction.</p>
<p>CPU 部分把数据从稀疏特征转化成 embedding （embedding learning），然后把embedding 传到 GPU，GPU进行一轮训练 (joint learning)</p>
<p>论文这部分讲了一些网络的设计细节，但这块感觉跟 AIBox本身没什么关系，论文写到：</p>
<p>把第一隐含层和最后一层隐含层的结果合并起来，第一层包含了low-level 的与输入信息最相关的feature，最后一层包含了high-level 的最抽象和有用的信息。这样会得到更准确的CTR预估结果。</p>
<p>训练两阶段(cpu+gpu)，梯度更新也是两阶段(gpu+cpu)。</p>
<h1 id="三-aibox-架构划分">三、AIBox 架构划分</h1>
<p><strong>架构：</strong><br>
分为三部分：CPU、GPU和 sparse table buff</p>
<ul>
<li>
<p>cpu模块：协调调度和embedding学习<br>
从hdfs读数据(一个pass)，向Sparse Table模块查embedding，然后发给GPU<br>
拿到gpu传来的梯度，更新sparse table<br>
定期save ckpt 到 hdfs</p>
</li>
<li>
<p>sparse table：把10^12 的离散特征的数据存储到ssd上的kv系统里<br>
内存中的key hash 索引存了特征到文件的映射关系，<br>
in-memory cache strategy 构造cache 和 buffer 来减少延迟</p>
</li>
<li>
<p>gpu模块：联合学习<br>
cpu传来的 embedding 被放入 HBMs 中，然后被fed 给 dense 联合学习网络<br>
emb通过pci-e总线进行传输<br>
一个CUDA stream进行数据传输，另一个cuda stream 进行学习计算<br>
HBMs如同片上ps一样工作，<br>
每个pass 在每个gpu 上计算新的参数，各gpu通过NVLink进行同步</p>
</li>
</ul>
<p><strong>3阶段流水线：network, SSDs and CPUs + GPUs</strong></p>
<figure data-type="image" tabindex="1"><img src="https://DragonFive.github.io//post-images/1625143570973.png" alt="" loading="lazy"></figure>
<figure data-type="image" tabindex="2"><img src="https://DragonFive.github.io//post-images/1625143575338.png" alt="" loading="lazy"></figure>
<h1 id="四-sparsetable-架构">四、sparseTable 架构</h1>
<p>由两部分构成：key hash index and bi-level cache management</p>
<h2 id="一key-hash-index">（一）key hash index</h2>
<p>Key Hash Index 存的是 10^12 个 feature key 到 ssd 文件的映射关系，直接每个key 存一个文件需要1.6TB大小的内存，放不下。</p>
<p>通过对key 取模进行分组建立 group 与 file 的对应关系，放在内存中。</p>
<p>group(key) → key mod 1012/m. We set m = ⌊BLOCK/(8 + sizeof(value))⌋,其中Block 是每次从ssd取数据的最小单元。</p>
<p>hash函数可以通过预训练一个模型来最大化feature 共现，把共现的feature 分到同样的桶里面。</p>
<h2 id="二二级缓存机制">（二）二级缓存机制</h2>
<p>ssd 的延迟是内存的1000倍，ssd是微秒级别延迟，内存是纳秒级别延迟</p>
<p>在一个 pass of mini batch 中只有1%的参数会被用到，所以我们可以用in-memory cache 来存储高频访问的hot parameters</p>
<p>SSD有物理性能限制：每个存储单元只能被写入（擦除）数千次，cache机制可以作为参数缓存，来减少更新参数对SSD使用寿命的影响</p>
<p>使用两个分离的链表进行拉链来提升探测性能。对每个ssd文件使用Bloom filter来减少不必要的读取。</p>
<p><strong>第一级缓存</strong></p>
<p>使用 si =hash1(g_id) 来算出一个 cache slot 槽，对应一个ssd 文件，对于参数并未进行真正初始化，而是在第一次访问到参数的时候，先用 bloom filter 探测key 是否在 slot 集合里，如果不在就不用读取这个文件，而是直接使用默认值，以此来减少不必要的ssd读取。</p>
<p><strong>二级缓存</strong></p>
<p>hash2(g_id, bucket)</p>
<p>对 一级的槽进行分桶bucket，来使得拉的链比较短。bucket 参数通过调节可以权衡空间和探测效率</p>
<p><strong>两条拉链</strong></p>
<ul>
<li>LRU 链用于保存最近访问过的key，以此来减少探测次数</li>
<li>LFU链按访问频次来保存key，用于缓存管理，只有当LFU满了需要删除低频key时，相应的数据才会写回到ssd上面</li>
</ul>
<p>由于经常有链条中的节点进行增删，所以使用线程池以Slab memory allocation mechanism机制 进行管理。</p>
<figure data-type="image" tabindex="3"><img src="https://DragonFive.github.io//post-images/1625143705399.png" alt="" loading="lazy"></figure>
<p><strong>文件管理系统</strong><br>
batch产生的小文件对于先有的文件系统有很大的压力，把许多小文件组成一个组来创建较少的文件。小文件的名字由大文件的名字加上offset构成，保存在第一级cache slot 中</p>
<p>监控文件系统的大小，合并访问量少的小问题，当model_size 达到最大冗余度的时候删掉访问少的文件，MAX_REPLICATION=SSD capacity ∗ (85% + overprovisioning)/model size<br>
<img src="https://DragonFive.github.io//post-images/1625143710125.png" alt="" loading="lazy"></p>
<h1 id="五-实验部分">五、实验部分</h1>
<p><strong>实验</strong><br>
AIBox 8 个GPU， 服务器级别的cpu, 1T 内存，Raid 0 nvme ssd<br>
MPI集群方式用75个计算节点</p>
<ul>
<li>
<p>AIBox 的硬件和维护费用比集群训练方式少 10%，执行时间多25%</p>
</li>
<li>
<p>AIBox 的auc 比集群方式稍好，可能是因为AIBox 这种单节点的方式，同步参数频率更高</p>
</li>
</ul>
<figure data-type="image" tabindex="4"><img src="https://DragonFive.github.io//post-images/1625143749656.png" alt="" loading="lazy"></figure>
<p>六、总结与一些细节问题：<br>
论文介绍了 AIBox 架构的一些细节方面，借助一系列系统设计方案如缓存机制来解决问题，通过这些并不是很fashion的技术合并，论文实现了集中式训练的技术突破，这种通过技术积累然后撬动难题的解决问题的方式值得我们学习。</p>
<p>但是还有一些细节没有讲清楚：</p>
<ol>
<li>
<p>AIBox 有几个worker 进行工作，他们是数据并行，还是使用同样的数据进行训练（文中提到AIBox 会在每个pass of mini batch 进行同步，所以应该不是一个worker 在参与训练）</p>
</li>
<li>
<p>AIBox 使用集中的训练方式，那如果这台机器挂掉，是不是根本没有办法进行恢复，只能另找一个机器从 ckpt 训练</p>
</li>
<li>
<p>文章没有介绍使用的具体计算引擎 (怀疑跟 horovod 接近）</p>
</li>
<li>
<p>同样文章没有介绍参数同步的细节，没有相关 all_reduce 的介绍（可以是使用了一个开源的框架，而这部分论文没有进行改进，所以没有做深入介绍）</p>
</li>
<li>
<p>文章开头提到使用 in-HBM ps 来减少数据传输，但是后面没有详细进行介绍</p>
</li>
</ol>
<p>总体上感觉这篇论文实用性强，但是细节介绍得不多</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[深度学习框架的并行优化方法小结]]></title>
        <id>https://DragonFive.github.io/post/deeplearning-parrel/</id>
        <link href="https://DragonFive.github.io/post/deeplearning-parrel/">
        </link>
        <updated>2018-08-11T08:17:26.000Z</updated>
        <content type="html"><![CDATA[<p>title: 深度学习框架的并行优化方法小结</p>
<p>categories:</p>
<ul>
<li>深度学习<br>
tags:</li>
<li>deeplearning</li>
<li>mpi</li>
<li>caffe</li>
</ul>
<p>目前的深度学习领域就是海量的数据加上大量的数学运算，所以计算量相当的大，训练一个模型跑上十天半个月啥的是常事。那此时分布式的意义就出现了，既然一张GPU卡跑得太慢就来两张，一台机器跑得太慢就用多台机器。</p>
<p><strong>数据并行</strong></p>
<figure data-type="image" tabindex="1"><img src="https://www.github.com/DragonFive/CVBasicOp/raw/master/1505026360037.jpg" alt="数据并行" loading="lazy"></figure>
<p>每一个节点（或者叫进程）都有一份模型，然后各个节点取不同的数据，通常是一个batch_size，然后各自完成前向和后向的计算得到梯度，这些进行训练的进程我们成为<strong>worker</strong>，除了worker，还有<strong>参数服务器</strong>，简称ps server，这些worker会把各自计算得到的梯度送到ps server，然后由ps server来进行update操作，然后把update后的模型再传回各个节点。因为在这种并行模式中，被划分的是数据，所以这种并行方式叫<strong>数据并行</strong>。</p>
<p>数据并行有<strong>同步模式和异步模式</strong>之分。同步模式中，所有训练程序同时训练一个批次的训练数据，完成后经过同步，再同时交换参数。参数交换完成后所有的训练程序就有了共同的新模型作为起点，再训练下一个批次。而异步模式中，训练程序完成一个批次的训练数据，立即和参数服务器交换参数，不考虑其他训练程序的状态。异步模式中一个训练程序的最新结果不会立刻体现在其他训练程序中，直到他们进行下次参数交换。</p>
<p><a href="http://blog.csdn.net/xsc_c/article/details/42420167"> 卷积神经网络的并行化模型</a></p>
<h1 id="parameter-server">parameter server</h1>
<p>limu的parameter server， MSRA的adam和google的tensorflow。</p>
<p><a href="https://www.zhihu.com/question/26998075">最近比较火的parameter server是什么？</a></p>
<p><a href="http://www.cs.cmu.edu/~muli/file/ps.pdf">李沐：Parameter Server for Distributed Machine Learning</a></p>
<p>参数服务器是个编程框架，用于方便分布式并行程序的编写，其中重点是对大规模参数的分布式存储和协同的支持。</p>
<p>参数服务器就类似于MapReduce，是大规模机器学习在不断使用过程中，抽象出来的框架之一。重点支持的就是<strong>参数的分布式</strong>，毕竟巨大的模型其实就是巨大的参数。</p>
<h2 id="架构">架构：</h2>
<p>集群中的节点可以分为<strong>计算节点和参数服务节点</strong>两种。其中，计算节点负责对分配到自己本地的训练数据（块）计算学习，并更新对应的参数；参数服务节点采用分布式存储的方式，各自存储全局参数的一部分，并作为服务方接受计算节点的参数查询和更新请求。简而言之吧，计算节点负责干活和更新参数，参数服务节点则负责存储参数。</p>
<h2 id="冗余和恢复">冗余和恢复：</h2>
<p>类似MapReduce，每个参数在参数服务器的集群中都在多个不同节点上备份（<strong>3个</strong>也是极好的），这样当出现节点失效时，冗余的参数依旧能够保证服务的有效性。当有新的节点插入时，把原先失效节点的参数从冗余参数那边复制过来，失效节点的接班人就加入队伍了。</p>
<h2 id="并行计算">并行计算：</h2>
<p>并行计算这部分主要在计算节点上进行。 类似于MapReduce，分配任务时，会将数据拆分给每个worker节点。参数服务器在开始学习前，也会把大规模的训练数据拆分到每个计算节点上。单个计算节点就对本地数据进行学习就可以了。学习完毕再把参数的更新梯度上传给对应的参数服务节点进行更新。</p>
<h2 id="流程">流程</h2>
<p>1.分发训练数据 -&gt; 节点1 节点2   节点3   ... 节点i  ... 节点N<br>
2.节点i 学习过程：遍历本地的训练数据，统计所有需要的参数(key)向分布式的参数服务器查询需要的参数（注意，本地数据对应到的参数只是全局参数的一小部分）得到查询到的参数值，用于模型的本地训练一轮训练完毕，得到所有对应参数的更新，将更新上传给参数服务器<br>
3.参数服务器更新参数过程：参数服务器得到计算节点传过来的局部更新，<strong>汇总后更新本地数据</strong></p>
<h1 id="并行程序">并行程序</h1>
<h2 id="并行实现实现方式">并行实现实现方式：</h2>
<ol>
<li>任务并行：将任务分配带若干计算核上;</li>
<li><strong>数据并行</strong>：将数据进行分割，然后由不同的计算核进行处理，<strong>每个核在规模相当的数据集上大致采用相同的操作</strong>。这不由使我想到了<strong>CAFFE中的对GPU的运用来实现并行训练</strong>的思路，就是将数据集进行分割，每个GPU并行处理各自对应的数据集。</li>
</ol>
<p>多指令多数据流又分为分布式内存系统和共享内存系统。<br>
<strong>分布式内存系统</strong>：<br>
每个处理器由独立的内存，通过<strong>消息传递函数</strong>来通信。<br>
共享式内存系统：<br>
多个处理器能访问内存系统中的相同内存，通过共享内存进行通信。<br>
<strong>MPI</strong>就是用来在分布式系统中为各处理器进行消息传递的API。</p>
<p>各个核能够直接访问自己的内存，而运行在不同核之间的进程需要交换内存数据的时候，只能通过消息传递API来实现。消息传递的API至少要提供一个发送函数和接收函数。**进程之间通过它们的序号（rank）**进行识别。</p>
<h2 id="并行程序的流程">并行程序的流程</h2>
<p>a、任务或者<strong>数据划分</strong>，就是要识别出任务中可以进行并行执行的部分。<br>
b、不同任务之间的<strong>通信</strong>;<br>
c、<strong>聚合</strong>，将任务和通信进行集合，聚合成更大的任务;<br>
d、<strong>分配</strong>，将聚合的任务分配到进程或线程中。</p>
<p>1、MPI是进程级别的，通过通信在进程之间进行消息传递。<br>
2、编程模型复杂：<br>
a、需要进行任务划分;<br>
b、通信延迟和负载不均衡;通信延迟很好理解，负载不均衡是因为分布式的系统，每个处理的任务量不同？待进一步的解释 ；<br>
c、可靠性差，一个进程出错，整个程序崩溃。第一感觉就是这简直是MPI的命门。在分布式系统中某一个进程出错是很容易的，为MPI的命运担忧。</p>
<h1 id="通信函数">通信函数</h1>
<h2 id="一般函数">一般函数</h2>
<pre><code class="language-cpp">int MPI_Send (void *buf, int count, MPI_Datatype datatype,int dest, int tag,MPI_Comm comm)
</code></pre>
<p>参数buf为发送缓冲区；count为发送的数据个数；datatype为发送的数据类型；dest为消息的目的地址(进程号)，其取值范围为0到np－1间的整数(np代表通信器comm中的进程数) 或MPI_PROC_NULL；tag为消息标签，其取值范围为0到MPI_TAG_UB间的整数；<strong>comm为通信器</strong></p>
<pre><code class="language-cpp">mpi_recv:接收信息   MPI_Probe：预测一下消息的size
</code></pre>
<h2 id="mpi聚合通信">mpi聚合通信</h2>
<p>collective communication。聚合通信是在通信子中的所有的进程都参与的通信方式。</p>
<h3 id="同步-mpi_barrier">同步 MPI_Barrier</h3>
<p>MPI_Barrier就是这样的一个函数，他确保除非所有的进程同时调用，否则他不会允许任何进程通过这个节点<br>
对于所有的进程来说，聚合通信必然包含了一个<strong>同步点</strong>。也就是说所有的进程必须在他们又一次执行新动作之前都到达某个点。这跟GPU中线程同步的概念很相似，很好理解。</p>
<h3 id="广播">广播</h3>
<p>广播机制：<br>
一个进程将相同的数据发送给通信子中所有的进程。该机制最主要的应用是将输入数据发送给并行程序，或者将<strong>配置参数</strong>发送给所有的进程</p>
<pre><code class="language-cpp">MPI_Bcast(
    void* data,//数据
    int count,//数据个数
    MPI_Datatype datatype,
    int root,//根进程编号
    MPI_Comm communicator)
</code></pre>
<h3 id="mpi_scatter-数据分发">MPI_Scatter 数据分发</h3>
<p>MPI_Scatter与MPI_Bcast非常相似，都是<strong>一对多</strong>的通信方式，不同的是后者的<strong>0号进程</strong>将相同的信息发送给所有的进程，而前者则是将一段array 的不同部分发送给所有的进程</p>
<figure data-type="image" tabindex="2"><img src="https://www.github.com/DragonFive/CVBasicOp/raw/master/1502761049076.jpg" alt="scatter与bcast的区别" loading="lazy"></figure>
<pre><code class="language-cpp">MPI_Scatter(
    void* send_data,//存储在0号进程的数据，array
    int send_count,//具体需要给每个进程发送的数据的个数
    //如果send_count为1，那么每个进程接收1个数据；如果为2，那么每个进程接收2个数据
    MPI_Datatype send_datatype,//发送数据的类型
    void* recv_data,//接收缓存，缓存 recv_count个数据
    int recv_count,
    MPI_Datatype recv_datatype,
    int root,//root进程的编号
    MPI_Comm communicator)
</code></pre>
<p>通常send_count等于array的元素个数除以进程个数。</p>
<h3 id="mpi_gather">MPI_Gather</h3>
<p>MPI_Gather和MPI_scatter刚好相反，他的作用是从所有的进程中将每个进程的数据集中到根进程中，<strong>同样根据进程的编号对array元素排序</strong></p>
<figure data-type="image" tabindex="3"><img src="https://www.github.com/DragonFive/CVBasicOp/raw/master/1502761558789.jpg" alt="mpi_gather" loading="lazy"></figure>
<pre><code class="language-cpp">MPI_Gather(
    void* send_data,
    int send_count,
    MPI_Datatype send_datatype,
    void* recv_data,
    int recv_count,//注意该参数表示的是从单个进程接收的数据个数，不是总数
    MPI_Datatype recv_datatype,
    int root,
    MPI_Comm communicator)
</code></pre>
<h3 id="mpi_allgather-多对多通信">MPI_Allgather 多对多通信</h3>
<p>当数据分布在所有的进程中时，MPI_Allgather将所有的数据聚合到每个进程中。</p>
<figure data-type="image" tabindex="4"><img src="https://www.github.com/DragonFive/CVBasicOp/raw/master/1502761637900.jpg" alt="mpi_Allgather" loading="lazy"></figure>
<h2 id="数据归约-reduce">数据归约 Reduce</h2>
<p>Reduce——规约是来自函数式编程的一个经典概念。数据规约包含通过一个函数将一批数据分成较小的一批数据。比如将一个数组的元素通过加法函数规约为一个数字。</p>
<h3 id="mpi_reduce">mpi_reduce</h3>
<p>与MPI_Gather类似，MPI_Reduce在每个进程上都有一组输入元素，并将<strong>一个输出元素数组返回给根进程</strong>。 输出元素包含被规约的结果。</p>
<pre><code class="language-cpp">MPI_Reduce(
    void* send_data,
    void* recv_data,
    int count,
    MPI_Datatype datatype,
    MPI_Op op,
    int root,
    MPI_Comm communicator)
</code></pre>
<blockquote>
<p>send_data参数指向的是每个进程想要规约的datatype类型的元素数组。<br>
recv_data仅与根进程相关。<br>
recv_data数组包含规约的结果，并具有sizeof（datatype）* count的大小的内存。<br>
op参数是要应用于数据的操作。</p>
</blockquote>
<p>mpi支持的操作有</p>
<blockquote>
<p>MPI_MAX - 返回最大值.<br>
MPI_MIN - 返回最小值.<br>
MPI_SUM -元素和.<br>
MPI_PROD - 元素乘积.<br>
MPI_LAND - 逻辑与.<br>
MPI_LOR - 逻辑或<br>
MPI_BAND -按位与<br>
MPI_BOR - 按位或<br>
MPI_MAXLOC - 返回最大值和拥有该值的进程编号<br>
MPI_MINLOC - 返回最小值和拥有该值的进程编号.```</p>
</blockquote>
<p>如果每个进程中的数组拥有两个元素，那么规约结果是对两个对位的元素进行规约的。</p>
<figure data-type="image" tabindex="5"><img src="https://www.github.com/DragonFive/CVBasicOp/raw/master/1502762764619.jpg" alt="两个元素的归约结果" loading="lazy"></figure>
<h3 id="mpi_allreduce">mpi_allReduce</h3>
<figure data-type="image" tabindex="6"><img src="https://www.github.com/DragonFive/CVBasicOp/raw/master/1502762804609.jpg" alt="归约后分发给所有的进程" loading="lazy"></figure>
<h1 id="parameter-server-2">parameter-server</h1>
<h1 id="cuda-c编程">CUDA C编程</h1>
<h2 id="cuda运行时函数">cuda运行时函数</h2>
<p>cuda运行时提供了丰富的函数，功能涉及设备管理、存储管理、数据传输、线程管理、流管理、事件管理、纹理管理、执行控制等。</p>
<h3 id="设备管理函数">设备管理函数</h3>
<p>函数声明一般这样</p>
<pre><code>extern __host__ cudaError_t CUDARTAPI 函数名(参数列表)
</code></pre>
<p><strong>cudaGetDeviceCount</strong><br>
获得计算能力大于等于1.0的GPU数量</p>
<pre><code class="language-cpp">int count;
cudaGetDeviceCount(&amp;count);
</code></pre>
<p><strong>cudaSetDevice</strong><br>
设置使用的GPU索引号，如果不设置默认使用0号GPU</p>
<pre><code class="language-cpp">int gpuid = 0;
cudaSetDevice(gpuid);

</code></pre>
<p><strong>cudaGetDevice</strong><br>
获得当前线程的GPU设备号</p>
<pre><code class="language-cpp">int gpuid;
cudaGetDevice(&amp;gpuid);

</code></pre>
<p><strong>cudaSetValidDevices</strong></p>
<p>设置多个device,len表示签名设备号数组的长度;</p>
<pre><code class="language-cpp">cudaSetValidDevices(int &amp;device_arr, int len);
</code></pre>
<h3 id="存储管理函数">存储管理函数</h3>
<p><strong>cudaMalloc</strong></p>
<p>在GPU上分配大小为size的现行存储空间，起始地址为 *devPtr</p>
<pre><code class="language-cpp">cudaMalloc(void **devPtr,size_t size);

</code></pre>
<p><strong>cudaMallocPitch</strong></p>
<p>在GPU上分配大小为PitchxHight的逻辑2D线性存储空间，首地址为<code>*devPtr</code>, 其中Pitch是返回的width对齐后的存储空间的宽度</p>
<pre><code class="language-cpp">cudaMallocPitch(void **devPtr, size_t *pitch, size_t width, size_t height);
</code></pre>
<pre><code>devPtr[x] = devPtr[rowid*pitch+column]
</code></pre>
<p><strong>cudaFree</strong><br>
清空指定的GPU存储区域，可释放cudaMalloc和cudaMallocPitch分类的GPU存储区域</p>
<pre><code class="language-cpp">cudaFree(void *devPtr);

</code></pre>
<p><strong>cudaMemset</strong><br>
将GPU端的devPtr指针指向的count长度的存储空间赋值为value.</p>
<pre><code class="language-cpp">cudaMemset(void 8DevPTR， int value,size_t count);
</code></pre>
<p><strong>cudaHostAlloc</strong><br>
在主机端(CPU)根据flag值来分配页锁定存储,</p>
<pre><code class="language-cpp">cudaHostAlloc(void **pHost, size_t size, usigned int flags);
</code></pre>
<p>flags可以有四种取值</p>
<pre><code class="language-cpp">cudaHostAllocDefault   分配默认存储
cudaHostAllocPortable  分配的存储可以被cuda索引
cudaHostAllocMapped 分配的存储映射到GPU
。。。

</code></pre>
<h3 id="数据传输函数">数据传输函数</h3>
<p><strong>cudaMemcpy</strong></p>
<pre><code class="language-cpp">cudaMemcpy(void * dst, const void *src, size_t count, enum cudaMemcpyKind kind);
</code></pre>
<p>主机(cpu内存)与设备间的数据传输函数，源地址是<code>*src</code>，目标地址是<code>*dst</code>,传输长度为<code>count</code>,kind指定了传输的方向，kind可选值域如下：</p>
<pre><code class="language-cpp">cudaMemcpyHostToHost = 0;
cudaMemcpyHostToDevice = 0;
cudaMemcpyDeviceToHost = 0;
cudaMemcpyDeviceToDevice = 0;
</code></pre>
<p>还有其它的形式</p>
<h3 id="线程管理函数">线程管理函数</h3>
<p><strong>cudaThreadSynchronize</strong></p>
<p>CPU与GPU之间的同步函数，保证该函数前的CPU和GPU上的任务均执行完成，并在该函数位置汇合。一般是CPU在该函数处等待GPU函数执行完。</p>
<pre><code class="language-cpp">cudaThreadSynchronize(void);
</code></pre>
<h1 id="reference">reference</h1>
<p>《GPU编程与优化》——方民权</p>
<h1 id="reference-2">reference</h1>
<p><a href="http://blog.csdn.net/sinat_22336563/article/details/69486937">MPI学习笔记之并行程序概述</a></p>
<p><a href="http://blog.csdn.net/xsc_c/article/details/42420167"> 卷积神经网络的并行化模型</a></p>
<p><a href="https://www.zhihu.com/search?type=content&amp;q=parameter+server">知乎 parameter server</a></p>
<p><a href="http://blog.csdn.net/xbinworld/article/details/74781605">分布式机器学习系统笔记（一）——模型并行，数据并行，参数平均，ASGD</a></p>
<p><a href="http://djt.qq.com/article/view/1245">深度学习及并行化实现概述</a></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[基于知识迁移的深度神经网络压缩方法研究]]></title>
        <id>https://DragonFive.github.io/post/networker-thransfer/</id>
        <link href="https://DragonFive.github.io/post/networker-thransfer/">
        </link>
        <updated>2018-05-20T03:40:48.000Z</updated>
        <content type="html"><![CDATA[<hr>
<p>title: 基于知识迁移的深度神经网络压缩方法研究<br>
date: 2018/5/20 12:04:12<br>
tags:</p>
<ul>
<li>神经网络压缩</li>
<li>深度学习</li>
<li>神经网络</li>
</ul>
<hr>
<p>我的毕设题目是基于知识迁移的深度神经网络压缩方法研究，由于本文涉及实验室的后续研究与项目开发，暂时删除该内容，等待时机合适再公开</p>
<p>——待续</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[gluon学习笔记]]></title>
        <id>https://DragonFive.github.io/post/gluon-study/</id>
        <link href="https://DragonFive.github.io/post/gluon-study/">
        </link>
        <updated>2018-03-20T03:39:08.000Z</updated>
        <content type="html"><![CDATA[<hr>
<p>title: gluon学习笔记<br>
date: 2018/3/20 12:04:12<br>
categories:</p>
<ul>
<li>深度学习<br>
tags:</li>
<li>目标检测</li>
<li>深度学习</li>
<li>神经网络</li>
</ul>
<hr>
<h1 id="学到的新知识">学到的新知识</h1>
<h2 id="bn放在relu后面">bn放在relu后面</h2>
<p><a href="http://minibatch.net/2017/06/11/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87-Batch-Normalization/">BN应该放在relu后</a></p>
<p><a href="https://mp.weixin.qq.com/s/xJromD5Q30KlRhB_kM4kfA">用于分类、检测和分割的移动网络 MobileNetV2</a></p>
<p><a href="https://www.zhihu.com/question/265709710">如何评价MobileNetV2</a></p>
<h2 id="卷积核的数量">卷积核的数量</h2>
<p><a href="http://zh.gluon.ai/chapter_convolutional-neural-networks/cnn-scratch.html">卷积神经网络 — 从0开始</a></p>
<p>当输入数据有多个通道的时候，每个通道会有对应的权重，然后会对每个通道做卷积之后在通道之间求和。所以当输出只有一个的时候，卷积的channel数目和data的channel数目是一样的。</p>
<p>当输出需要多通道时，每个输出通道有对应权重，然后每个通道上做卷积。所以当输入有n个channel，输出有h个channel时，卷积核channel数目为n * h，每个输出channel对应一个bias ,卷积核的维度为(h,n,w,h)</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>c</mi><mi>o</mi><mi>n</mi><mi>v</mi><mo>(</mo><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi><mo separator="true">,</mo><mi>w</mi><mo separator="true">,</mo><mi>b</mi><mo>)</mo><mo>[</mo><mo>:</mo><mo separator="true">,</mo><mi>i</mi><mo separator="true">,</mo><mo>:</mo><mo separator="true">,</mo><mo>:</mo><mo>]</mo><mo>=</mo><mi>c</mi><mi>o</mi><mi>n</mi><mi>v</mi><mo>(</mo><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi><mo separator="true">,</mo><mi>w</mi><mo>[</mo><mi>i</mi><mo separator="true">,</mo><mo>:</mo><mo separator="true">,</mo><mo>:</mo><mo separator="true">,</mo><mo>:</mo><mo>]</mo><mo separator="true">,</mo><mi>b</mi><mo>[</mo><mi>i</mi><mo>]</mo><mo>)</mo></mrow><annotation encoding="application/x-tex">conv(data, w, b)[:,i,:,:] = conv(data, w[i,:,:,:], b[i])
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">c</span><span class="mord mathdefault">o</span><span class="mord mathdefault">n</span><span class="mord mathdefault" style="margin-right:0.03588em;">v</span><span class="mopen">(</span><span class="mord mathdefault">d</span><span class="mord mathdefault">a</span><span class="mord mathdefault">t</span><span class="mord mathdefault">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">b</span><span class="mclose">)</span><span class="mopen">[</span><span class="mrel">:</span></span><span class="base"><span class="strut" style="height:0.85396em;vertical-align:-0.19444em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">i</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">:</span></span><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">:</span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mclose">]</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">c</span><span class="mord mathdefault">o</span><span class="mord mathdefault">n</span><span class="mord mathdefault" style="margin-right:0.03588em;">v</span><span class="mopen">(</span><span class="mord mathdefault">d</span><span class="mord mathdefault">a</span><span class="mord mathdefault">t</span><span class="mord mathdefault">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mopen">[</span><span class="mord mathdefault">i</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">:</span></span><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">:</span></span><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">:</span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mclose">]</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">b</span><span class="mopen">[</span><span class="mord mathdefault">i</span><span class="mclose">]</span><span class="mclose">)</span></span></span></span></span></p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">Γ</mi><mo>(</mo><mi>z</mi><mo>)</mo><mo>=</mo><msubsup><mo>∫</mo><mn>0</mn><mi mathvariant="normal">∞</mi></msubsup><msup><mi>t</mi><mrow><mi>z</mi><mo>−</mo><mn>1</mn></mrow></msup><msup><mi>e</mi><mrow><mo>−</mo><mi>t</mi></mrow></msup><mi>d</mi><mi>t</mi><mtext> </mtext><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\Gamma(z) = \int_0^\infty t^{z-1}e^{-t}dt\,.
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">Γ</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.04398em;">z</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.326242em;vertical-align:-0.9119499999999999em;"></span><span class="mop"><span class="mop op-symbol large-op" style="margin-right:0.44445em;position:relative;top:-0.0011249999999999316em;">∫</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.414292em;"><span style="top:-1.7880500000000001em;margin-left:-0.44445em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span><span style="top:-3.8129000000000004em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">∞</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.9119499999999999em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">t</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.864108em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.04398em;">z</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.843556em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathdefault mtight">t</span></span></span></span></span></span></span></span></span><span class="mord mathdefault">d</span><span class="mord mathdefault">t</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">.</span></span></span></span></span></p>
<p>123</p>
<figure data-type="image" tabindex="1"><img src="https://www.github.com/DragonFive/CVBasicOp/raw/master/1513949196873.jpg" alt="inception v1" loading="lazy"></figure>
<figure data-type="image" tabindex="2"><img src="https://www.github.com/DragonFive/CVBasicOp/raw/master/1514013854016.jpg" alt="residual" loading="lazy"></figure>
<figure data-type="image" tabindex="3"><img src="https://www.github.com/DragonFive/CVBasicOp/raw/master/1514012389756.jpg" alt="resnet各种结构" loading="lazy"></figure>
<h1 id="gluon语法">gluon语法</h1>
<h2 id="nnblock与nnsequential的嵌套使用">nn.Block与nn.sequential的嵌套使用</h2>
<pre><code class="language-python">class RecMLP(nn.Block):
    def __init__(self, **kwargs):
        super(RecMLP, self).__init__(**kwargs)
        self.net = nn.Sequential()
        with self.name_scope():
            self.net.add(nn.Dense(256, activation=&quot;relu&quot;))
            self.net.add(nn.Dense(128, activation=&quot;relu&quot;))
            self.dense = nn.Dense(64)

    def forward(self, x):
        return nd.relu(self.dense(self.net(x)))

rec_mlp = nn.Sequential()
rec_mlp.add(RecMLP())
rec_mlp.add(nn.Dense(10))
print(rec_mlp)
</code></pre>
<h2 id="初始化与参数访问">初始化与参数访问</h2>
<pre><code class="language-python">from mxnet import init
params.initialize(init=init.Normal(sigma=0.02), force_reinit=True)
print(net[0].weight.data(), net[0].bias.data())
</code></pre>
<p>我们也可以通过collect_params来访问Block里面所有的参数（这个会包括所有的子Block）。它会返回一个名字到对应Parameter的dict。</p>
<p>也可以自定义各层的初始化方法，没有自定义的按照net.initialize里面的方法进行定义</p>
<pre><code class="language-python">from mxnet.gluon import nn
from mxnet import nd
from mxnet import init

def get_net():
    net = nn.Sequential()
    with net.name_scope():
        net.add(nn.Dense(4,activation=&quot;relu&quot;))#,weight_initializer=init.Xavier()))
        net.add(nn.Dense(2,weight_initializer=init.Zero(),bias_initializer=init.Zero()) )
    return net

x = nd.random.uniform(shape=(3,5))
net = get_net()
net.initialize(init.One())
net(x)
print(net[1].weight.data
</code></pre>
<h2 id="gpu访问">GPU访问</h2>
<ol>
<li>删除cpu版本mxnet</li>
</ol>
<pre><code class="language-bash">pip uninstall mxnet
</code></pre>
<ol start="2">
<li>更新GPU版本mxnet</li>
</ol>
<pre><code class="language-bash">pip install -U --pre mxnet-cu80
</code></pre>
<ol start="3">
<li>查看版本号</li>
</ol>
<pre><code class="language-python">import pip
for pkg in ['mxnet', 'mxnet-cu75', 'mxnet-cu80']:
    pip.main(['show', pkg])
</code></pre>
<h2 id="使用jupyter的相关插件">使用jupyter的相关插件</h2>
<ol>
<li>notedown插件<br>
可以在jupyter 中查看markdown文件</li>
<li>nb_conda<br>
是conda的插件，可以在jupyter里面修改python内核版本</li>
</ol>
<h2 id="优化方法">优化方法</h2>
<p><strong>momentum</strong><br>
gluon.Trainer的learning_rate属性和set_learning_rate函数可以随意调整学习率。</p>
<pre><code class="language-python">trainer = gluon.Trainer(net.collect_params(), 'sgd',
                            {'learning_rate': lr, 'momentum': mom})
</code></pre>
<p><strong>adagrad</strong><br>
Adagrad是一个在迭代过程中不断自我调整学习率，并让模型参数中每个元素都使用不同学习率的优化算法。</p>
<pre><code class="language-python">    trainer = gluon.Trainer(net.collect_params(), 'adagrad',
                            {'learning_rate': lr})
</code></pre>
<p><strong>Adam</strong></p>
<pre><code class="language-python">trainer = gluon.Trainer(net.collect_params(), 'adam',
                            {'learning_rate': lr})

</code></pre>
<p>通过以上分析, 理论上可以说, 在数据比较稀疏的时候, adaptive 的方法能得到更好的效果, 例如, adagrad, adadelta, rmsprop, adam 等. 在数据稀疏的情况下, adam 方法也会比 rmsprop 方法收敛的结果要好一些, 所以, 通常在没有其它更好的理由的前框下, 我会选用 adam 方法, 可以比较快地得到一个预估结果. 但是, 在论文中, 我们看到的大部分还是最原始的 mini-batch 的 SGD 方法. 因为马鞍面的存在等问题, SGD 方法有时候较难收敛. 另外, SGD 对于参数的初始化要求也比较高. 所以, 如果要是想快速收敛的话, 建议使用 adam 这类 adaptive 的方法</p>
<h2 id="延迟执行">延迟执行</h2>
<p>延后执行使得系统有更多空间来做性能优化。但我们推荐每个批量里至少有一个同步函数，例如对损失函数进行评估，来避免将过多任务同时丢进后端系统。</p>
<pre><code class="language-python">from mxnet import autograd

mem = get_mem()

total_loss = 0
for x, y in get_data():
    with autograd.record():
        L = loss(y, net(x))
    total_loss += L.sum().asscalar()
    L.backward()
    trainer.step(x.shape[0])

nd.waitall()
print('Increased memory %f MB' % (get_mem() - mem))

</code></pre>
<h2 id="多gpu训练">多GPU训练</h2>
<pre><code class="language-python">ctx = [gpu(i) for i in range(num_gpus)]
data_list = gluon.utils.split_and_load(data, ctx)
label_list = gluon.utils.split_and_load(label, ctx)



</code></pre>
<h2 id="fintune-微调">fintune 微调</h2>
<p><a href="https://fiercex.github.io/post/gluon_features_fine/">gluon微调</a></p>
<h1 id="一些可以重复使用的代码">一些可以重复使用的代码</h1>
<h2 id="读取数据">读取数据</h2>
<pre><code class="language-python">from mxnet import gluon
from mxnet import ndarray as nd

def transform(data, label):
    return data.astype('float32')/255, label.astype('float32')
mnist_train = gluon.data.vision.FashionMNIST(train=True, transform=transform)
mnist_test = gluon.data.vision.FashionMNIST(train=False, transform=transform)

</code></pre>
<h2 id="计算精度">计算精度</h2>
<pre><code class="language-python">def accuracy(output, label):
    return nd.mean(output.argmax(axis=1)==label).asscalar()


</code></pre>
<p>我们先使用Flatten层将输入数据转成 batch_size x ? 的矩阵，然后输入到10个输出节点的全连接层。照例我们不需要制定每层输入的大小，gluon会做自动推导。</p>
<h2 id="激活函数">激活函数</h2>
<p><strong>sigmoid</strong></p>
<pre><code class="language-python">from mxnet import nd
def softmax(X):
    exp = nd.exp(X)
    # 假设exp是矩阵，这里对行进行求和，并要求保留axis 1，
    # 就是返回 (nrows, 1) 形状的矩阵
    partition = exp.sum(axis=1, keepdims=True)
    return exp / partition


</code></pre>
<p><strong>relu</strong></p>
<pre><code class="language-python">def relu(X):
    return nd.maximum(X, 0)

</code></pre>
<h2 id="损失函数">损失函数</h2>
<p><strong>平方误差</strong></p>
<pre><code class="language-python">square_loss = gluon.loss.L2Loss()


</code></pre>
<pre><code class="language-python">def square_loss(yhat, y):
    # 注意这里我们把y变形成yhat的形状来避免矩阵形状的自动转换
    return (yhat - y.reshape(yhat.shape)) ** 2
 

</code></pre>
<p><strong>交叉熵损失</strong></p>
<pre><code class="language-python">def cross_entropy(yhat, y):
    return - nd.pick(nd.log(yhat), y)

</code></pre>
<pre><code class="language-python">softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()

</code></pre>
<h2 id="取一个batch_size的代码">取一个batch_size的代码</h2>
<p><strong>scratch版本</strong></p>
<pre><code class="language-python">import random
batch_size = 1
def data_iter(num_examples):
    idx = list(range(num_examples))
    random.shuffle(idx)
    for i in range(0, num_examples, batch_size):
        j = nd.array(idx[i:min(i+batch_size,num_examples)])
        yield X.take(j), y.take(j)

</code></pre>
<p><strong>gluon版本</strong></p>
<pre><code class="language-python">batch_size = 1
dataset_train = gluon.data.ArrayDataset(X_train, y_train)
data_iter_train = gluon.data.DataLoader(dataset_train, batch_size, shuffle=True)



</code></pre>
<h2 id="初始化权值">初始化权值</h2>
<p><strong>scratch版本</strong></p>
<pre><code class="language-python">def get_params():
    w = nd.random.normal(shape=(num_inputs, 1))*0.1
    b = nd.zeros((1,))
    for param in (w, b):
        param.attach_grad()
    return (w, b)

</code></pre>
<p><strong>gluon版本</strong></p>
<pre><code class="language-python">net.initialize()


net.collect_params().initialize(mx.init.Normal(sigma=1))

</code></pre>
<h2 id="sgd">SGD</h2>
<p><strong>scratch版本</strong></p>
<pre><code class="language-python">def SGD(params, lr):
    for param in params:
        param[:] = param - lr * param.grad


</code></pre>
<p>L2正则</p>
<pre><code class="language-python">def L2_penalty(w, b):
    return ((w**2).sum() + b**2) / 2

</code></pre>
<p><strong>gluon版本</strong></p>
<pre><code>    trainer = gluon.Trainer(net.collect_params(), 'sgd', {
        'learning_rate': learning_rate, 'wd': weight_decay})

</code></pre>
<p>这里的weight_decay表明这里添加了L2正则，正则化<br>
w = w -lr * grad - wd * w</p>
<h2 id="训练过程">训练过程</h2>
<p><strong>scratch版本</strong></p>
<pre><code class="language-python">    for e in range(epochs):        
        for data, label in data_iter(num_train):
            with autograd.record():
                output = net(data, lambd, *params)
                loss = square_loss(
                    output, label) + lambd * L2_penalty(*params)
            loss.backward()
            SGD(params, learning_rate)
        train_loss.append(test(params, X_train, y_train))
        test_loss.append(test(params, X_test, y_test))

</code></pre>
<p><strong>gluon版本</strong></p>
<pre><code class="language-python">    for e in range(epochs):        
        for data, label in data_iter_train:
            with autograd.record():
                output = net(data)
                loss = square_loss(output, label)
            loss.backward()
            trainer.step(batch_size)            
        train_loss.append(test(net, X_train, y_train))
        test_loss.append(test(net, X_test, y_test))


</code></pre>
<pre><code class="language-python">%matplotlib inline
import matplotlib as mpl
mpl.rcParams['figure.dpi']= 120
import matplotlib.pyplot as plt

def train(X_train, X_test, y_train, y_test):
    # 线性回归模型
    net = gluon.nn.Sequential()
    with net.name_scope():
        net.add(gluon.nn.Dense(1))
    net.initialize()
    # 设一些默认参数
    learning_rate = 0.01
    epochs = 100
    batch_size = min(10, y_train.shape[0])
    dataset_train = gluon.data.ArrayDataset(X_train, y_train)
    data_iter_train = gluon.data.DataLoader(
        dataset_train, batch_size, shuffle=True)
    # 默认SGD和均方误差
    trainer = gluon.Trainer(net.collect_params(), 'sgd', {
        'learning_rate': learning_rate})
    square_loss = gluon.loss.L2Loss()
    # 保存训练和测试损失
    train_loss = []
    test_loss = []
    for e in range(epochs):
        for data, label in data_iter_train:
            with autograd.record():
                output = net(data)
                loss = square_loss(output, label)
            loss.backward()
            trainer.step(batch_size)
        train_loss.append(square_loss(
            net(X_train), y_train).mean().asscalar())
        test_loss.append(square_loss(
            net(X_test), y_test).mean().asscalar())
    # 打印结果
    plt.plot(train_loss)
    plt.plot(test_loss)
    plt.legend(['train','test'])
    plt.show()
    return ('learned weight', net[0].weight.data(),
            'learned bias', net[0].bias.data())

</code></pre>
<p>最终版</p>
<pre><code class="language-python">def train(train_data, test_data, net, loss, trainer, ctx, num_epochs, print_batches=None):
    &quot;&quot;&quot;Train a network&quot;&quot;&quot;
    print(&quot;Start training on &quot;, ctx)
    if isinstance(ctx, mx.Context):
        ctx = [ctx]
    for epoch in range(num_epochs):
        train_loss, train_acc, n, m = 0.0, 0.0, 0.0, 0.0
        if isinstance(train_data, mx.io.MXDataIter):
            train_data.reset()
        start = time()
        for i, batch in enumerate(train_data):
            data, label, batch_size = _get_batch(batch, ctx)
            losses = []
            with autograd.record():
                outputs = [net(X) for X in data]
                losses = [loss(yhat, y) for yhat, y in zip(outputs, label)]
            for l in losses:
                l.backward()
            train_acc += sum([(yhat.argmax(axis=1)==y).sum().asscalar()
                              for yhat, y in zip(outputs, label)])
            train_loss += sum([l.sum().asscalar() for l in losses])
            trainer.step(batch_size)
            n += batch_size
            m += sum([y.size for y in label])
            if print_batches and (i+1) % print_batches == 0:
                print(&quot;Batch %d. Loss: %f, Train acc %f&quot; % (
                    n, train_loss/n, train_acc/m
                ))

        test_acc = evaluate_accuracy(test_data, net, ctx)
        print(&quot;Epoch %d. Loss: %.3f, Train acc %.2f, Test acc %.2f, Time %.1f sec&quot; % (
            epoch, train_loss/n, train_acc/m, test_acc, time() - start
        ))

</code></pre>
<h1 id="reference">reference</h1>
<p><a href="https://zhuanlan.zhihu.com/p/28867241">从零开始码一个皮卡丘检测器</a></p>
<p><a href="http://blog.csdn.net/jesse_mx/article/details/53606897">图片标注工具</a></p>
<p><a href="http://blog.csdn.net/u014696921/article/details/56877979"> mxnet 使用自己的图片数据训练CNN模型</a></p>
<p><a href="https://mxnet.incubator.apache.org/api/python/image.html#Image">mxnet image API</a></p>
<p><a href="https://mxnet.incubator.apache.org/how_to/recordio.html?highlight=recordio">Create a Dataset Using RecordIO</a></p>
<p><a href="http://blog.csdn.net/muyouhang/article/details/77727381">基于MXNet gluon 的SSD模型训练</a></p>
<p><a href="https://groups.google.com/a/continuum.io/forum/m/#!topic/anaconda/RuSpZVPEio8">解决conda与ipython notebook的python版本问题</a></p>
<p><a href="http://blog.csdn.net/sunshine_in_moon/article/details/51434908">神经网络计算参数量的方法</a></p>
<p><a href="https://www.jianshu.com/p/c56a37093cfa">神经网络计算特征图的大小的方法</a></p>
<p><a href="http://minibatch.net/2017/06/11/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87-Batch-Normalization/">BN应该放在relu后</a></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[c++ 的关键字回顾]]></title>
        <id>https://DragonFive.github.io/post/cpp-keyword/</id>
        <link href="https://DragonFive.github.io/post/cpp-keyword/">
        </link>
        <updated>2018-03-03T04:15:07.000Z</updated>
        <content type="html"><![CDATA[<hr>
<p>Title: c++ 的关键字回顾<br>
date: 2018/3/03 17:38:58<br>
categories:</p>
<ul>
<li>编程<br>
tags:</li>
<li>cpp</li>
<li>关键字</li>
</ul>
<hr>
<p>2013年的时候写了一些学c++时候的经验<a href="https://blog.csdn.net/zhzz2012/article/details/46346115">由底层和逻辑深入剖析c++系列</a>，<br>
两年前写了一篇<a href="https://zhuanlan.zhihu.com/p/21930436">c++11新特性详解</a>，<br>
去年总结了一些c++的一些使用技巧 <a href="https://mxl.bitcron.com/post/engineering/cpp_new_feature">c++使用7年后的经验总结</a>。<br>
现在看来需要经常复习使用，才能更好地掌握c++这门编程语言。</p>
<h1 id="关键字">关键字</h1>
<h2 id="static与extern-c">static与extern &quot;C&quot;</h2>
<p><strong>区别</strong><br>
这两个是不能同时使用的一对词。</p>
<ol>
<li>
<p>static 修饰的名字只能在当前模块中使用，且不能被extern所修饰</p>
</li>
<li>
<p>extern &quot;C&quot; 表示修饰的名字来自其它模块，且要按C语言的方式进行编译和链接<br>
<strong>extern &quot;C&quot;从问题中学习</strong></p>
</li>
<li>
<p>既然c++是C的超集，为什么还要用C语言的方式编译和链接？<br>
&gt; 因为可能编写c语言的人给你的不是源代码。而是编译之后的.so文件</p>
</li>
<li>
<p>extern &quot;C&quot; 与 extern 是什么关系</p>
<blockquote>
<p>extern &quot;C&quot;包含双重含义，从字面上可以知道，首先，被它修饰的目标是&quot;extern&quot;的；其次，被它修饰的目标代码是&quot;C&quot;的。extern 告诉编译器，其申明的函数和变量可以在本模块或其他模块中使用。</p>
</blockquote>
</li>
<li>
<p>什么是按C语言的方式进行编译和链接</p>
<blockquote>
<p>函数被C++编译后在符号库中的名字是与C语言不同的；C++编译后的函数需要加上参数的类型才能唯一标定重载后的函数，而加上extern &quot;C&quot;后，是为了向编译器指明这段代码按照C语言的方式进行编译和链接。比如对于<code>int foo(int x, int y)</code>C++会编译为类似<code>__foo_int_int_</code>的形式，而c语言则会编译为<code>__foo__</code>的形式。</p>
</blockquote>
</li>
<li>
<p>在c语言中想要C++类里面的东西怎么办</p>
<blockquote>
<p><a href="https://www.cnblogs.com/Yogurshine/p/3913073.html">C代码中如何调用C++ C++中如何调用C</a><br>
<a href="https://blog.csdn.net/caspiansea/article/details/9676153">如何用C语言封装 C++的类，在 C里面使用</a></p>
</blockquote>
</li>
</ol>
<p><strong>extern &quot;C&quot;从例子中学</strong><br>
1.修饰单个句子</p>
<pre><code class="language-CPP">extern &quot;C&quot; double sqrt(double);
</code></pre>
<ol start="2">
<li>修饰复合句子</li>
</ol>
<pre><code class="language-C">extern &quot;C&quot;
 {
      double sqrt(double);
      int min(int, int);
  }
</code></pre>
<p>3.包含include头文件，相当于头文件中的声明都加了 extern &quot;C&quot;</p>
<pre><code class="language-C">extern &quot;C&quot;
{
     #include &lt;cmath&gt;
}
</code></pre>
<p>4.在C语言的一些标准头文件里经常有这样的表示</p>
<pre><code class="language-CPP">#ifdef  __cplusplus
extern &quot;C&quot; {
#endif
……// (C函数声明)
#ifdef  __cplusplus
}
#endif
</code></pre>
<p><strong>参考资料</strong><br>
<a href="https://blog.csdn.net/jiqiren007/article/details/5933599">extern C的作用详解</a></p>
<h2 id="const关键字">const关键字</h2>
<p><strong>常量指针与指针常量</strong></p>
<ol>
<li>区分方法：从右向左读 <code>char * const A</code>, A是一个不可变的指针，指向的是char数据，<code>char const * B</code> , 表示B是一个指针，这个指针指向char常量；</li>
<li>对于常量指针，不能通过该指针来改变所指的内容(可以通过其它方式修改)<br>
<strong>常量对象与常量成员函数</strong><br>
一个类的常量对象只能调用该类的常量成员函数，因为常量成员函数不能修改对象的成员变量，当然可以在一个成员变量前加上mutable关键字，这样常量成员函数就能修改它了。 <code>void func() const</code></li>
</ol>
<p><strong>常量成员变量与类常量</strong><br>
常量成员变量是说，它是属于对象的不可变的变量，所以初始化只能在构造函数的初始化列表里。const数据成员只在某个对象生存期内是常量，而对于整个类而言却是可变的。因为类可以创建多个对象，不同的对象其const数据成员的值可以不同。所以不能在类声明中初始化const数据成员，因为类的对象未被创建时，编译器不知道const 数据成员的值是什么。</p>
<p>要想建立在整个类中都恒定的常量，应该用类中的枚举常量来实现。</p>
<pre><code class="language-CPP">class A
{
 enum {size1=100, size2 = 200 };
 int array1[size1];
 int array2[size2];
}
</code></pre>
<p><strong>const修饰函数返回值</strong><br>
一般用const修饰返回值为对象本身（非引用和指针）的情况多用于二目操作符重载函数并产生新对象的时候。 防止产生的临时对象被赋值。<br>
比如两个复数的乘法</p>
<pre><code class="language-CPP">const Rational operator*(const Rational&amp; lhs, const Rational&amp; rhs) 
{ 
 return Rational(lhs.numerator() * rhs.numerator(), 
 lhs.denominator() * rhs.denominator()); 
}
</code></pre>
<p>这样做可以预防出现<code>(a*b) = c</code>的情况。<br>
<strong>引用传递的返回值不要用const修饰</strong><br>
在类的本地操作符（=，&lt;&lt;等）重载函数中，函数返回值常采用“引用传递”，目的是为了实现链式表达。</p>
<pre><code class="language-CPP">class A
{
 A &amp;operate = (const A &amp;other);  //赋值函数
}
A a,b,c;              //a,b,c为A的对象
a=b=c;            //正常
(a=b)=c;          //不正常，但是合法
</code></pre>
<p>若赋值函数的返回值加const修饰，那么该返回值的内容不允许修改。所以一般赋值函数都不会这样设置。<br>
<strong>C语言与CPP中const的区别</strong></p>
<ol>
<li>C++中的const正常情况下是看成编译期的常量,编译器并不为const分配空间,只是在编译的时候将期值保存在名字表中,并在适当的时候折合在代码中，而c语言认为是不变的变量，在编译期不知道值。</li>
</ol>
<p>C++中,是否为const分配空间要看具体情况.如果加上关键字extern或者取const变量地址,则编译器就要为const分配存储空间，下面的代码在c++中会通过，而c语言不会通过</p>
<pre><code class="language-CPP">const int a=10;
int b[a];
</code></pre>
<p>2.C++中,const默认使用内部连接，定义时必须初始化(类中的成员变量除外)，或者使用extern修饰. 而C中使用外部连接，可以只声明不初始化<code>const int size;</code></p>
<p><strong>顶层const与底层const</strong><br>
是用const修饰时，如果修饰的是定义的变量就是顶层const，如果修饰的是定义的变量指向的对象那就是底层const。<br>
值得注意的是顶层const在初始化和赋值的时候，等号左右两边的对象是否const并无影响。<br>
而底层const赋值的时候，可以把非常量赋值给指向常量对象的地址，却不可以把常量初始化给指向非常量对象的地址。</p>
<p><strong>constexpr变量</strong><br>
c++11标准规定，允许将变量声明为constexpr类型，以便由编译器来验证变量的值是否是一个常量表达式</p>
<p><strong>使用const的建议</strong></p>
<ol>
<li>要大胆的使用const，这将给你带来无尽的益处，但前提是你必须搞清楚原委；</li>
<li>在参数中使用const应该使用引用或指针，而不是一般的对象实例；</li>
<li>不要轻易的将函数的返回值类型定为const;</li>
<li>除了重载操作符外一般不要将返回值类型定为对某个对象的const引用;</li>
</ol>
<blockquote>
<p>非 const 变量默认为 extern。要使 const 变量能够在其他的文件中访问，必须地指定它为 extern——《cpp primer》</p>
</blockquote>
<p><strong>参考资料</strong><br>
<a href="https://www.cnblogs.com/yc_sunniwell/archive/2010/07/14/1777416.html">c/c++中的const关键字</a><br>
《c++ primer 第五版》</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[从编译器的辅助信息看c++对象内存布局]]></title>
        <id>https://DragonFive.github.io/post/cpp-memory/</id>
        <link href="https://DragonFive.github.io/post/cpp-memory/">
        </link>
        <updated>2018-02-17T04:16:29.000Z</updated>
        <content type="html"><![CDATA[<hr>
<p>Title: 从编译器的辅助信息看c++对象内存布局<br>
date: 2018/02/17 14:28:58<br>
categories:</p>
<ul>
<li>编程<br>
tags:</li>
<li>cpp</li>
</ul>
<hr>
<h1 id="预知识">预知识</h1>
<p>本文的内容使用的是32位的编译器编译出的结果，可以打印出类的内存布局信息</p>
<h2 id="devcpp-ide">DevCPP IDE</h2>
<p>这个IDE是我比较喜欢的windows下的cpp的IDE之一，它有一个工具-&gt;编译选项，可以选择编译器类型，也可以在编译选项中加入一些信息，为了能够输出内存布局信息，我在编译时加入以下命令</p>
<pre><code class="language-cpp">--std=c++11  -fdump-class-hierarchy 
</code></pre>
<p>-fdump-class-hierarchy 这个选项能够在gcc编译时生成类的布局信息，生成的文件名类似为6.cpp.002t.class</p>
<h2 id="vs2017">vs2017</h2>
<p>vs使用的编译器是cl，它的命令为<br>
/d1reportAllClassLayout： 输出所有类相关布局</p>
<h2 id="clang">clang</h2>
<p>clang -Xclang -fdump-record-layouts</p>
<h2 id="理解内存布局信息">理解内存布局信息</h2>
<p>测试代码如下</p>
<pre><code class="language-cpp">class father{
	int a;
	int b;
};

class child: public father{
};

int main(){
	return 0;
}
</code></pre>
<p>使用 g++ -fdump-class-hierarchy test.cpp 生成了 test.cpp.002t.class内容如下</p>
<pre><code>Class father
   size=8 align=4
   base size=8 base align=4
father (0x0x16662d8) 0

Class child
   size=8 align=4
   base size=8 base align=4
child (0x0x3984e00) 0
  father (0x0x1666310) 0
</code></pre>
<p>显示了类的大小和对齐信息。</p>
<h2 id="题话外-理解内存对齐">题话外: 理解内存对齐</h2>
<ol>
<li>结构体第一个成员的偏移量（offset）为0，以后每个成员相对于结构体首地址的 offset 都是该成员大小与有效对齐值中较小那个的整数倍，如有需要编译器会在成员之间加上填充字节。</li>
<li>结构体的总大小为 有效对齐值 的整数倍，如有需要编译器会在最末一个成员之后加上填充字节。</li>
</ol>
<h1 id="多态与虚表">多态与虚表</h1>
<p>多态，简单来说，是指在继承层次中，父类的指针可以具有多种形态——当它指向某个子类对象时，通过它能够调用到子类的函数，而非父类的函数。</p>
<p>虚函数指针一般都放在对象内存布局的第一个位置上，这是为了保证在多层继承或多重继承的情况下能以最高效率取到虚函数表。当vprt位于对象内存最前面时，<strong>对象的地址即为虚函数指针地址</strong>。我们可以取得<strong>虚函数指针的地址</strong>。</p>
<p>下面的方式取得虚函数指针的地址（而非虚函数指针指向的地址）</p>
<pre><code class="language-cpp">Base b(1000);
int * vptrAdree = (int *)(&amp;b);
</code></pre>
<p>vptrAdree指向了虚函数指针（指向虚函数表），</p>
<p>而我们可以通过如下的方式获得第一个虚函数的地址</p>
<pre><code class="language-cpp">Base b(1000);
using fun=void(*) ();
fun fun1 = (fun)*((int *)*(int *)(&amp;b));
fun fun2 = (fun)*((int *)*(int *)(&amp;c)+1);
</code></pre>
<h2 id="简单的情况单层继承与虚函数全覆盖">简单的情况，单层继承与虚函数全覆盖</h2>
<p>通过查看编译器生成的内存布局信息来具体地看：</p>
<pre><code class="language-cpp">#include &lt;iostream&gt;
using namespace std;

class Base{
	int a;
	virtual int print(){
		cout&lt;&lt;&quot; i am base&quot;&lt;&lt;endl;
	}
	virtual int print2(){
		cout&lt;&lt;&quot; i am base2&quot; &lt;&lt;endl;
	}
}; 

class child: public Base{
	int print(){
		cout&lt;&lt; &quot; i am the child&quot; &lt;&lt;endl;	
	}
	int print2(){
		cout&lt;&lt;&quot; i am the child2&quot; &lt;&lt;endl;
	}
};



int  main(){
	Base testBase;
	//using fun=int(*)();
	typedef int(*fun) (void);
	fun print=(fun)*((int *)*(int *)(&amp;testBase));
	print();
	cout&lt;&lt;(int *)*(int *)(&amp;testBase)&lt;&lt;endl;
	cout&lt;&lt;((int *)*(int *)(&amp;testBase))+1&lt;&lt;endl;
	fun pprint2 = (fun)*((int *)*(int *)(&amp;testBase)+1);
	pprint2();
	return 0;
} 
</code></pre>
<p>g++编译后生成的内存布局信息为：</p>
<pre><code class="language-cpp">Vtable for Base
Base::_ZTV4Base: 4u entries
0     (int (*)(...))0
4     (int (*)(...))(&amp; _ZTI4Base)
8     (int (*)(...))Base::print
12    (int (*)(...))Base::print2

Class Base
   size=8 align=4
   base size=8 base align=4
Base (0x0x4e057a8) 0
    vptr=((&amp; Base::_ZTV4Base) + 8u)

Vtable for child
child::_ZTV5child: 4u entries
0     (int (*)(...))0
4     (int (*)(...))(&amp; _ZTI5child)
8     (int (*)(...))child::print
12    (int (*)(...))child::print2

Class child
   size=8 align=4
   base size=8 base align=4
child (0x0x4e4b280) 0
    vptr=((&amp; child::_ZTV5child) + 8u)
  Base (0x0x4e057e0) 0
      primary-for child (0x0x4e4b280)
</code></pre>
<p>Vtable表示的是虚函数表，可以发现Vtable for Base里面保存了vptr， 而且位置是vptr=((&amp; Base::_ZTV4Base) + 8u)（注意这里加了8的偏移），这个位置的第一个内容就是(int (*)(...))Base::print。</p>
<h2 id="复杂的情况多重继承与部分虚函数覆盖">复杂的情况：多重继承与部分虚函数覆盖</h2>
<pre><code class="language-cpp">#include &lt;iostream&gt;
using namespace std;
class Parent {
public:
    int iparent;
    Parent ():iparent (10) {}
    virtual void g() { cout &lt;&lt; &quot; Parent::g()&quot; &lt;&lt; endl; }
    virtual void f() { cout &lt;&lt; &quot; Parent::f()&quot; &lt;&lt; endl; }
    virtual void h() { cout &lt;&lt; &quot; Parent::h()&quot; &lt;&lt; endl; }
 
};
 
class Child : public Parent {
public:
    int ichild;
    Child():ichild(100) {}
    virtual void g_child() { cout &lt;&lt; &quot;Child::g_child()&quot; &lt;&lt; endl; }
    virtual void h_child() { cout &lt;&lt; &quot;Child::h_child()&quot; &lt;&lt; endl; }
    virtual void f() { cout &lt;&lt; &quot;Child::f()&quot; &lt;&lt; endl; }
};
 
class GrandChild : public Child{
public:
    int igrandchild;
    GrandChild():igrandchild(1000) {}
    virtual void g_child() { cout &lt;&lt; &quot;GrandChild::g_child()&quot; &lt;&lt; endl; }
    virtual void f() { cout &lt;&lt; &quot;GrandChild::f()&quot; &lt;&lt; endl; }
    virtual void h_grandchild() { cout &lt;&lt; &quot;GrandChild::h_grandchild()&quot; &lt;&lt; endl; }
};
int main(){
	typedef void(*Fun)(void);
	GrandChild gc;	 
	int** pVtab = (int**)&amp;gc;
	 
	cout &lt;&lt; &quot;[0] GrandChild::_vptr-&gt;&quot; &lt;&lt; endl;
	for (int i=0; (Fun)pVtab[0][i]!=NULL; i++){
	    Fun pFun = (Fun)pVtab[0][i];
	    cout &lt;&lt; &quot;    [&quot;&lt;&lt;i&lt;&lt;&quot;] &quot;;
	    pFun();
	}
	cout &lt;&lt; &quot;[1] Parent.iparent = &quot; &lt;&lt; (int)pVtab[1] &lt;&lt; endl;
	cout &lt;&lt; &quot;[2] Child.ichild = &quot; &lt;&lt; (int)pVtab[2] &lt;&lt; endl;
	cout &lt;&lt; &quot;[3] GrandChild.igrandchild = &quot; &lt;&lt; (int)pVtab[3] &lt;&lt; endl;
} 
</code></pre>
<p>内存布局信息如下</p>
<pre><code class="language-cpp">Vtable for Parent
Parent::_ZTV6Parent: 5u entries
0     (int (*)(...))0
4     (int (*)(...))(&amp; _ZTI6Parent)
8     (int (*)(...))Parent::g
12    (int (*)(...))Parent::f
16    (int (*)(...))Parent::h

Class Parent
   size=8 align=4
   base size=8 base align=4
Parent (0x0x4de37a8) 0
    vptr=((&amp; Parent::_ZTV6Parent) + 8u)

Vtable for Child
Child::_ZTV5Child: 7u entries
0     (int (*)(...))0
4     (int (*)(...))(&amp; _ZTI5Child)
8     (int (*)(...))Parent::g
12    (int (*)(...))Child::f
16    (int (*)(...))Parent::h
20    (int (*)(...))Child::g_child
24    (int (*)(...))Child::h_child

Class Child
   size=12 align=4
   base size=12 base align=4
Child (0x0x4e2a5c0) 0
    vptr=((&amp; Child::_ZTV5Child) + 8u)
  Parent (0x0x4de37e0) 0
      primary-for Child (0x0x4e2a5c0)

Vtable for GrandChild
GrandChild::_ZTV10GrandChild: 8u entries
0     (int (*)(...))0
4     (int (*)(...))(&amp; _ZTI10GrandChild)
8     (int (*)(...))Parent::g
12    (int (*)(...))GrandChild::f
16    (int (*)(...))Parent::h
20    (int (*)(...))GrandChild::g_child
24    (int (*)(...))Child::h_child
28    (int (*)(...))GrandChild::h_grandchild

Class GrandChild
   size=16 align=4
   base size=16 base align=4
GrandChild (0x0x4e2aa80) 0
    vptr=((&amp; GrandChild::_ZTV10GrandChild) + 8u)
  Child (0x0x4e2aac0) 0
      primary-for GrandChild (0x0x4e2aa80)
    Parent (0x0x4de3818) 0
        primary-for Child (0x0x4e2aac0)


</code></pre>
<p>通过查看可以发现，子类的虚函数表里面的优先级是先父类，再子类再孙类，同一优先级按照声明顺序排地址，即使父类的虚函数被覆盖了，也要写在原来的位置，这样能够保证，父类的指针能按照函数名找到那个地址。</p>
<p>程序的执行结果为：</p>
<pre><code class="language-cpp">[0] GrandChild::_vptr-&gt;
&lt;pre&gt;    [0] GrandChild::f()
    [1] Parent::g()
    [2] Parent::h()
    [3] GrandChild::g_child()
    [4] Child::h1()
    [5] GrandChild::h_grandchild()
[1] Parent.iparent = 10
[2] Child.ichild = 100
[3] GrandChild.igrandchild = 1000

</code></pre>
<p>可知</p>
<ol>
<li>虚函数表在最前面的位置。</li>
<li>成员变量根据其继承和声明顺序依次放在后面。</li>
<li>在单一的继承中，被overwrite的虚函数在虚函数表中得到了更新。</li>
</ol>
<h2 id="多重继承">多重继承</h2>
<pre><code class="language-cpp">#include &lt;iostream&gt;
using namespace std;
class Base1 {
public:
    int ibase1;
    Base1():ibase1(10) {}
    virtual void f() { cout &lt;&lt; &quot;Base1::f()&quot; &lt;&lt; endl; }
    virtual void g() { cout &lt;&lt; &quot;Base1::g()&quot; &lt;&lt; endl; }
    virtual void h() { cout &lt;&lt; &quot;Base1::h()&quot; &lt;&lt; endl; }
 
};
 
class Base2 {
public:
    int ibase2;
    Base2():ibase2(20) {}
    virtual void f() { cout &lt;&lt; &quot;Base2::f()&quot; &lt;&lt; endl; }
    virtual void g() { cout &lt;&lt; &quot;Base2::g()&quot; &lt;&lt; endl; }
    virtual void h() { cout &lt;&lt; &quot;Base2::h()&quot; &lt;&lt; endl; }
};
 
class Base3 {
public:
    int ibase3;
    Base3():ibase3(30) {}
    virtual void f() { cout &lt;&lt; &quot;Base3::f()&quot; &lt;&lt; endl; }
    virtual void g() { cout &lt;&lt; &quot;Base3::g()&quot; &lt;&lt; endl; }
    virtual void h() { cout &lt;&lt; &quot;Base3::h()&quot; &lt;&lt; endl; }
};
 
class Derive : public Base1, public Base2, public Base3 {
public:
    int iderive;
    Derive():iderive(100) {}
    virtual void f() { cout &lt;&lt; &quot;Derive::f()&quot; &lt;&lt; endl; }
    virtual void g1() { cout &lt;&lt; &quot;Derive::g1()&quot; &lt;&lt; endl; }
};
int main(){
	typedef void(*Fun)(void);
	 
	Derive d;
	 
	int** pVtab = (int**)&amp;d;
	 
	cout &lt;&lt; &quot;[0] Base1::_vptr-&gt;&quot; &lt;&lt; endl;
	Fun pFun = (Fun)pVtab[0][0];
	cout &lt;&lt; &quot;     [0] &quot;;
	pFun();
	 
	pFun = (Fun)pVtab[0][1];
	cout &lt;&lt; &quot;     [1] &quot;;pFun();
	 
	pFun = (Fun)pVtab[0][2];
	cout &lt;&lt; &quot;     [2] &quot;;pFun();
	 
	pFun = (Fun)pVtab[0][3];
	cout &lt;&lt; &quot;     [3] &quot;; pFun();
	 
	pFun = (Fun)pVtab[0][4];
	cout &lt;&lt; &quot;     [4] &quot;; cout&lt;&lt;pFun&lt;&lt;endl;
	 
	cout &lt;&lt; &quot;[1] Base1.ibase1 = &quot; &lt;&lt; (int)pVtab[1] &lt;&lt; endl;
	 
	int s = sizeof(Base1)/4;
	 
	cout &lt;&lt; &quot;[&quot; &lt;&lt; s &lt;&lt; &quot;] Base2::_vptr-&gt;&quot;&lt;&lt;endl;
	pFun = (Fun)pVtab[s][0];
	cout &lt;&lt; &quot;     [0] &quot;; pFun();
	 
	pFun = (Fun)pVtab[s][1];
	cout &lt;&lt; &quot;     [1] &quot;; pFun();
	 
	pFun = (Fun)pVtab[s][2];
	cout &lt;&lt; &quot;     [2] &quot;; pFun();
	 
	pFun = (Fun)pVtab[s][3];
	cout &lt;&lt; &quot;     [3] &quot;;
	cout&lt;&lt;pFun&lt;&lt;endl;
	 
	cout &lt;&lt; &quot;[&quot;&lt;&lt; s+1 &lt;&lt;&quot;] Base2.ibase2 = &quot; &lt;&lt; (int)pVtab[s+1] &lt;&lt; endl;
	 
	s = s + sizeof(Base2)/4;
	 
	cout &lt;&lt; &quot;[&quot; &lt;&lt; s &lt;&lt; &quot;] Base3::_vptr-&gt;&quot;&lt;&lt;endl;
	pFun = (Fun)pVtab[s][0];
	cout &lt;&lt; &quot;     [0] &quot;; pFun();
	 
	pFun = (Fun)pVtab[s][1];
	cout &lt;&lt; &quot;     [1] &quot;; pFun();
	 
	pFun = (Fun)pVtab[s][2];
	cout &lt;&lt; &quot;     [2] &quot;; pFun();
	 
	pFun = (Fun)pVtab[s][3];
	cout &lt;&lt; &quot;     [3] &quot;;
	cout&lt;&lt;pFun&lt;&lt;endl;
	 
	s++;
	cout &lt;&lt; &quot;[&quot;&lt;&lt; s &lt;&lt;&quot;] Base3.ibase3 = &quot; &lt;&lt; (int)pVtab[s] &lt;&lt; endl;
	s++;
	cout &lt;&lt; &quot;[&quot;&lt;&lt; s &lt;&lt;&quot;] Derive.iderive = &quot; &lt;&lt; (int)pVtab[s] &lt;&lt; endl;
	return 0; 
} 

</code></pre>
<p>内存分布情况如下</p>
<pre><code class="language-cpp">Vtable for Base1
Base1::_ZTV5Base1: 5u entries
0     (int (*)(...))0
4     (int (*)(...))(&amp; _ZTI5Base1)
8     (int (*)(...))Base1::f
12    (int (*)(...))Base1::g
16    (int (*)(...))Base1::h

Class Base1
   size=8 align=4
   base size=8 base align=4
Base1 (0x0x4d907a8) 0
    vptr=((&amp; Base1::_ZTV5Base1) + 8u)

Vtable for Base2
Base2::_ZTV5Base2: 5u entries
0     (int (*)(...))0
4     (int (*)(...))(&amp; _ZTI5Base2)
8     (int (*)(...))Base2::f
12    (int (*)(...))Base2::g
16    (int (*)(...))Base2::h

Class Base2
   size=8 align=4
   base size=8 base align=4
Base2 (0x0x4d907e0) 0
    vptr=((&amp; Base2::_ZTV5Base2) + 8u)

Vtable for Base3
Base3::_ZTV5Base3: 5u entries
0     (int (*)(...))0
4     (int (*)(...))(&amp; _ZTI5Base3)
8     (int (*)(...))Base3::f
12    (int (*)(...))Base3::g
16    (int (*)(...))Base3::h

Class Base3
   size=8 align=4
   base size=8 base align=4
Base3 (0x0x4d90818) 0
    vptr=((&amp; Base3::_ZTV5Base3) + 8u)

Vtable for Derive
Derive::_ZTV6Derive: 16u entries
0     (int (*)(...))0
4     (int (*)(...))(&amp; _ZTI6Derive)
8     (int (*)(...))Derive::f
12    (int (*)(...))Base1::g
16    (int (*)(...))Base1::h
20    (int (*)(...))Derive::g1
24    (int (*)(...))-8
28    (int (*)(...))(&amp; _ZTI6Derive)
32    (int (*)(...))Derive::_ZThn8_N6Derive1fEv
36    (int (*)(...))Base2::g
40    (int (*)(...))Base2::h
44    (int (*)(...))-16
48    (int (*)(...))(&amp; _ZTI6Derive)
52    (int (*)(...))Derive::_ZThn16_N6Derive1fEv
56    (int (*)(...))Base3::g
60    (int (*)(...))Base3::h

Class Derive
   size=28 align=4
   base size=28 base align=4
Derive (0x0x4ddeb40) 0
    vptr=((&amp; Derive::_ZTV6Derive) + 8u)
  Base1 (0x0x4d90850) 0
      primary-for Derive (0x0x4ddeb40)
  Base2 (0x0x4d90888) 8
      vptr=((&amp; Derive::_ZTV6Derive) + 32u)
  Base3 (0x0x4d908c0) 16
      vptr=((&amp; Derive::_ZTV6Derive) + 52u)

</code></pre>
<p>程序的输出结果为</p>
<pre><code class="language-cpp">[0] Base1::_vptr-&gt;
     [0] Derive::f()
     [1] Base1::g()
     [2] Base1::h()
     [3] Derive::g1()
     [4] 1
[1] Base1.ibase1 = 10
[2] Base2::_vptr-&gt;
     [0] Derive::f()
     [1] Base2::g()
     [2] Base2::h()
     [3] 1
[3] Base2.ibase2 = 20
[4] Base3::_vptr-&gt;
     [0] Derive::f()
     [1] Base3::g()
     [2] Base3::h()
     [3] 0
[5] Base3.ibase3 = 30
[6] Derive.iderive = 100

</code></pre>
<p>结论：</p>
<ol>
<li>每个父类都有自己的虚表。</li>
<li>子类的成员函数被放到了第一个父类的表中。</li>
<li>内存布局中，其父类布局依次按声明顺序排列。</li>
<li>每个父类的虚表中的f()函数都被overwrite成了子类的f()。这样做就是为了解决不同的父类类型的指针指向同一个子类实例，而能够调用到实际的函数。</li>
</ol>
<h2 id="多重继承-2">多重继承</h2>
<pre><code class="language-cpp">#include &lt;iostream&gt;
using namespace std;
class B
{
    public:
        int ib;
        char cb;
    public:
        B():ib(0),cb('B') {}
 
        virtual void f() { cout &lt;&lt; &quot;B::f()&quot; &lt;&lt; endl;}
        virtual void Bf() { cout &lt;&lt; &quot;B::Bf()&quot; &lt;&lt; endl;}
};
class B1 :  public B
{
    public:
        int ib1;
        char cb1;
    public:
        B1():ib1(11),cb1('1') {}
 
        virtual void f() { cout &lt;&lt; &quot;B1::f()&quot; &lt;&lt; endl;}
        virtual void f1() { cout &lt;&lt; &quot;B1::f1()&quot; &lt;&lt; endl;}
        virtual void Bf1() { cout &lt;&lt; &quot;B1::Bf1()&quot; &lt;&lt; endl;}
 
};
class B2:  public B
{
    public:
        int ib2;
        char cb2;
    public:
        B2():ib2(12),cb2('2') {}
 
        virtual void f() { cout &lt;&lt; &quot;B2::f()&quot; &lt;&lt; endl;}
        virtual void f2() { cout &lt;&lt; &quot;B2::f2()&quot; &lt;&lt; endl;}
        virtual void Bf2() { cout &lt;&lt; &quot;B2::Bf2()&quot; &lt;&lt; endl;}
 
};
 
class D : public B1, public B2
{
    public:
        int id;
        char cd;
    public:
        D():id(100),cd('D') {}
 
        virtual void f() { cout &lt;&lt; &quot;D::f()&quot; &lt;&lt; endl;}
        virtual void f1() { cout &lt;&lt; &quot;D::f1()&quot; &lt;&lt; endl;}
        virtual void f2() { cout &lt;&lt; &quot;D::f2()&quot; &lt;&lt; endl;}
        virtual void Df() { cout &lt;&lt; &quot;D::Df()&quot; &lt;&lt; endl;}
 
};
int main(){
	typedef void(*Fun)(void);
	int** pVtab = NULL;
	Fun pFun = NULL;
	 
	D d;
	pVtab = (int**)&amp;d;
	cout &lt;&lt; &quot;[0] D::B1::_vptr-&gt;&quot; &lt;&lt; endl;
	pFun = (Fun)pVtab[0][0];
	cout &lt;&lt; &quot;     [0] &quot;;    pFun();
	pFun = (Fun)pVtab[0][1];
	cout &lt;&lt; &quot;     [1] &quot;;    pFun();
	pFun = (Fun)pVtab[0][2];
	cout &lt;&lt; &quot;     [2] &quot;;    pFun();
	pFun = (Fun)pVtab[0][3];
	cout &lt;&lt; &quot;     [3] &quot;;    pFun();
	pFun = (Fun)pVtab[0][4];
	cout &lt;&lt; &quot;     [4] &quot;;    pFun();
	pFun = (Fun)pVtab[0][5];
	cout &lt;&lt; &quot;     [5] 0x&quot; &lt;&lt; pFun &lt;&lt; endl;
	 
	cout &lt;&lt; &quot;[1] B::ib = &quot; &lt;&lt; (int)pVtab[1] &lt;&lt; endl;
	cout &lt;&lt; &quot;[2] B::cb = &quot; &lt;&lt; static_cast&lt;char&gt;((int)(pVtab[2]))&lt;&lt; endl;
	cout &lt;&lt; &quot;[3] B1::ib1 = &quot; &lt;&lt; (int)pVtab[3] &lt;&lt; endl;
	cout &lt;&lt; &quot;[4] B1::cb1 = &quot; &lt;&lt; (char)(int)pVtab[4] &lt;&lt; endl;
	 
	cout &lt;&lt; &quot;[5] D::B2::_vptr-&gt;&quot; &lt;&lt; endl;
	pFun = (Fun)pVtab[5][0];
	cout &lt;&lt; &quot;     [0] &quot;;    pFun();
	pFun = (Fun)pVtab[5][1];
	cout &lt;&lt; &quot;     [1] &quot;;    pFun();
	pFun = (Fun)pVtab[5][2];
	cout &lt;&lt; &quot;     [2] &quot;;    pFun();
	pFun = (Fun)pVtab[5][3];
	cout &lt;&lt; &quot;     [3] &quot;;    pFun();
	pFun = (Fun)pVtab[5][4];
	cout &lt;&lt; &quot;     [4] 0x&quot; &lt;&lt; pFun &lt;&lt; endl;
	 
	cout &lt;&lt; &quot;[6] B::ib = &quot; &lt;&lt; (int)pVtab[6] &lt;&lt; endl;
	cout &lt;&lt; &quot;[7] B::cb = &quot; &lt;&lt; (char)(int)pVtab[7] &lt;&lt; endl;
	cout &lt;&lt; &quot;[8] B2::ib2 = &quot; &lt;&lt; (int)pVtab[8] &lt;&lt; endl;
	cout &lt;&lt; &quot;[9] B2::cb2 = &quot; &lt;&lt; (char)(int)pVtab[9] &lt;&lt; endl;
	 
	cout &lt;&lt; &quot;[10] D::id = &quot; &lt;&lt; (int)pVtab[10] &lt;&lt; endl;
	cout &lt;&lt; &quot;[11] D::cd = &quot; &lt;&lt; (char)(int)pVtab[11] &lt;&lt; endl;
	return 0; 
} 


</code></pre>
<p>内存分布情况如下</p>
<pre><code class="language-cpp">Vtable for B
B::_ZTV1B: 4u entries
0     (int (*)(...))0
4     (int (*)(...))(&amp; _ZTI1B)
8     (int (*)(...))B::f
12    (int (*)(...))B::Bf

Class B
   size=12 align=4
   base size=9 base align=4
B (0x0x4dc27a8) 0
    vptr=((&amp; B::_ZTV1B) + 8u)

Vtable for B1
B1::_ZTV2B1: 6u entries
0     (int (*)(...))0
4     (int (*)(...))(&amp; _ZTI2B1)
8     (int (*)(...))B1::f
12    (int (*)(...))B::Bf
16    (int (*)(...))B1::f1
20    (int (*)(...))B1::Bf1

Class B1
   size=20 align=4
   base size=17 base align=4
B1 (0x0x4e09780) 0
    vptr=((&amp; B1::_ZTV2B1) + 8u)
  B (0x0x4dc27e0) 0
      primary-for B1 (0x0x4e09780)

Vtable for B2
B2::_ZTV2B2: 6u entries
0     (int (*)(...))0
4     (int (*)(...))(&amp; _ZTI2B2)
8     (int (*)(...))B2::f
12    (int (*)(...))B::Bf
16    (int (*)(...))B2::f2
20    (int (*)(...))B2::Bf2

Class B2
   size=20 align=4
   base size=17 base align=4
B2 (0x0x4e09c40) 0
    vptr=((&amp; B2::_ZTV2B2) + 8u)
  B (0x0x4dc2818) 0
      primary-for B2 (0x0x4e09c40)

Vtable for D
D::_ZTV1D: 14u entries
0     (int (*)(...))0
4     (int (*)(...))(&amp; _ZTI1D)
8     (int (*)(...))D::f
12    (int (*)(...))B::Bf
16    (int (*)(...))D::f1
20    (int (*)(...))B1::Bf1
24    (int (*)(...))D::f2
28    (int (*)(...))D::Df
32    (int (*)(...))-20
36    (int (*)(...))(&amp; _ZTI1D)
40    (int (*)(...))D::_ZThn20_N1D1fEv
44    (int (*)(...))B::Bf
48    (int (*)(...))D::_ZThn20_N1D2f2Ev
52    (int (*)(...))B2::Bf2

Class D
   size=48 align=4
   base size=45 base align=4
D (0x0x4e27040) 0
    vptr=((&amp; D::_ZTV1D) + 8u)
  B1 (0x0x4e27080) 0
      primary-for D (0x0x4e27040)
    B (0x0x4dc2850) 0
        primary-for B1 (0x0x4e27080)
  B2 (0x0x4e270c0) 20
      vptr=((&amp; D::_ZTV1D) + 40u)
    B (0x0x4dc2888) 20
        primary-for B2 (0x0x4e270c0)


</code></pre>
<p>输出结果为</p>
<pre><code class="language-bash">[0] D::B1::_vptr-&gt;
     [0] D::f()
     [1] B::Bf()
     [2] D::f1()
     [3] B1::Bf1()
     [4] D::f2()
     [5] 0x1
[1] B::ib = 0
[2] B::cb = B
[3] B1::ib1 = 11
[4] B1::cb1 = 1
[5] D::B2::_vptr-&gt;
     [0] D::f()
     [1] B::Bf()
     [2] D::f2()
     [3] B2::Bf2()
     [4] 0x0
[6] B::ib = 0
[7] B::cb = B
[8] B2::ib2 = 12
[9] B2::cb2 = 2
[10] D::id = 100
[11] D::cd = D

</code></pre>
<p>运行结果为：</p>
<pre><code class="language-cpp">
</code></pre>
<p>发现最顶端的父类B其成员变量和虚函数存在于B1和B2中，并被D给继承下去了。而在D中，其有B1和B2的实例，于是B的成员在D的实例中存在两份，一份是B1继承而来的，另一份是B2继承而来的，因此就会浪费内存。</p>
<p>尽管我们可以通过明确指明调用路径以消除二义性，但二义性的潜在性还没有消除，我们可以通过虚继承来使D类只拥有一个ib实体。</p>
<h1 id="虚继承">虚继承</h1>
<p>虚继承解决了菱形继承中最派生类拥有多个间接父类实例的情况。虚继承的派生类的内存布局与普通继承很多不同，主要体现在：</p>
<ol>
<li>虚继承的子类，如果本身定义了新的虚函数，则编译器为其生成一个虚函数指针（vptr）以及一张虚函数表。该vptr位于对象内存最前面。vs非虚继承：直接扩展父类虚函数表。</li>
<li>虚继承的子类也单独保留了父类的vprt与虚函数表。这部分内容接与子类内容以一个四字节的0来分界。</li>
<li>虚继承的子类对象中，含有四字节的虚表指针偏移值。</li>
</ol>
<h2 id="简单虚继承">简单虚继承</h2>
<pre><code class="language-cpp">#include &lt;iostream&gt;
using namespace std;
class B
{
public:
    int ib;
public:
    B(int i=1) :ib(i){}
    virtual void f() { cout &lt;&lt; &quot;B::f()&quot; &lt;&lt; endl; }
    virtual void Bf() { cout &lt;&lt; &quot;B::Bf()&quot; &lt;&lt; endl; }
};
 
class B1 : virtual public B
{
public:
    int ib1;
public:
    B1(int i = 100 ) :ib1(i) {}
    virtual void f() { cout &lt;&lt; &quot;B1::f()&quot; &lt;&lt; endl; }
    virtual void f1() { cout &lt;&lt; &quot;B1::f1()&quot; &lt;&lt; endl; }
    virtual void Bf1() { cout &lt;&lt; &quot;B1::Bf1()&quot; &lt;&lt; endl; }
};
int main(){
	B1 b;
	using Fun=void(*)();
	Fun pFun = NULL;
	int ** pvtable = (int**)&amp;b;
	for (int i=0; i&lt;9; i++){
		if (pvtable[0][i]==NULL) {
			cout&lt;&lt;&quot;here:&quot;&lt;&lt;(int)pvtable[0][i]&lt;&lt;endl;
			i+=4;
			continue;
		}
		pFun = (Fun)pvtable[0][i];
		pFun();
	}
	cout&lt;&lt;&quot;size == &quot;&lt;&lt;sizeof(b)&lt;&lt;endl;
	cout&lt;&lt;&quot;[1]:B1::ib1=&quot;&lt;&lt;(int)pvtable[1]&lt;&lt;endl;
	cout&lt;&lt;&quot;[3]:B::ib=&quot;&lt;&lt;(int)pvtable[3]&lt;&lt;endl;
	cout&lt;&lt;&quot;[2]:B::ib=&quot;&lt;&lt;(int)pvtable[2]&lt;&lt;endl;
//	pFun = (Fun)pvtable[2];
//	pFun();
//	for (int i=1; pvtable[1][i]!=NULL; i++){
//		pFun = (Fun)pvtable[1][i];
//		pFun();
//	}
	
	return 0; 
} 



</code></pre>
<p>内存布局</p>
<pre><code class="language-cpp">Vtable for B
B::_ZTV1B: 4u entries
0     (int (*)(...))0
4     (int (*)(...))(&amp; _ZTI1B)
8     (int (*)(...))B::f
12    (int (*)(...))B::Bf

Class B
   size=8 align=4
   base size=8 base align=4
B (0x0x4f527a8) 0
    vptr=((&amp; B::_ZTV1B) + 8u)

Vtable for B1
B1::_ZTV2B1: 12u entries
0     8u
4     (int (*)(...))0
8     (int (*)(...))(&amp; _ZTI2B1)
12    (int (*)(...))B1::f
16    (int (*)(...))B1::f1
20    (int (*)(...))B1::Bf1
24    0u
28    4294967288u
32    (int (*)(...))-8
36    (int (*)(...))(&amp; _ZTI2B1)
40    (int (*)(...))B1::_ZTv0_n12_N2B11fEv
44    (int (*)(...))B::Bf

VTT for B1
B1::_ZTT2B1: 2u entries
0     ((&amp; B1::_ZTV2B1) + 12u)
4     ((&amp; B1::_ZTV2B1) + 40u)

Class B1
   size=16 align=4
   base size=8 base align=4
B1 (0x0x4f9a540) 0
    vptridx=0u vptr=((&amp; B1::_ZTV2B1) + 12u)
  B (0x0x4f527e0) 8 virtual
      vptridx=4u vbaseoffset=-12 vptr=((&amp; B1::_ZTV2B1) + 40u)

</code></pre>
<p>程序运行结果</p>
<pre><code class="language-cpp">B1::f()
B1::f1()
B1::Bf1()
here:0
B::Bf()
size == 16
[1]:B1::ib1=100
[3]:B::ib=1
[2]:B::ib=4781096

</code></pre>
<p>这里的内存分布比较奇怪，与vc++的结果不一样。</p>
<p>但是这里size是16 说明有两个指针；</p>
<h2 id="多重虚继承">多重虚继承</h2>
<p>菱形虚拟继承下，最派生类D类的对象模型又有不同的构成了。在D类对象的内存构成上，有以下几点：</p>
<ol>
<li>在D类对象内存中，基类出现的顺序是：先是B1（最左父类），然后是B2（次左父类），最后是B（虚祖父类）</li>
<li>D类对象的数据成员id放在B类前面，两部分数据依旧以0来分隔。</li>
<li>编译器没有为D类生成一个它自己的vptr，而是覆盖并扩展了最左父类的虚基类表，与简单继承的对象模型相同。</li>
<li>超类B的内容放到了D类对象内存布局的最后。</li>
</ol>
<h1 id="参考资料">参考资料</h1>
<p><a href="https://zhuanlan.zhihu.com/p/30007037">C/C++内存对齐详解</a></p>
<p><a href="https://coolshell.cn/articles/12176.html">C++ 对象的内存布局——coolshell</a></p>
<p><a href="https://www.cnblogs.com/QG-whz/p/4909359.html">图说C++对象模型：对象内存布局详解</a></p>
<p>[各编译器显示内存布局](</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[linux常用脚本]]></title>
        <id>https://DragonFive.github.io/post/linux-bash/</id>
        <link href="https://DragonFive.github.io/post/linux-bash/">
        </link>
        <updated>2018-02-13T08:15:25.000Z</updated>
        <content type="html"><![CDATA[<hr>
<p>title: linux常用工具<br>
date: 2018/2/13 22:04:12<br>
tags:</p>
<ul>
<li>linux</li>
</ul>
<hr>
<p><strong>Expect脚本</strong><br>
安装方法：</p>
<pre><code class="language-bash">sudo apt-get install tcl tk expect

</code></pre>
<p>脚本例子自动SSH连接：</p>
<pre><code class="language-YAML">#!/usr/bin/expect
set timeout 30
set ip [lindex $argv 0]
spawn ssh -l maxiaolong 10.0.8.241
expect &quot;password:&quot;
send &quot;mxl@lt.776688\r&quot;
expect &quot;Ip:*&quot;
send &quot;$ip\r&quot;
expect &quot;$*&quot;
send &quot;hostname\r&quot;
expect &quot;$*&quot;
interact
</code></pre>
<h1 id="gcc升级完整不留遗憾的方法">gcc升级完整不留遗憾的方法</h1>
<p>自己编译安装gcc4.9——&gt;替换软链接——&gt;替换libstdc++.so的软链接</p>
<p>通过下面的命令可以查看Glibcxx的版本</p>
<pre><code class="language-bash"> strings /usr/lib/x86_64-linux-gnu/libstdc++.so.6 | grep GLIBCXX
</code></pre>
<p>nvidia-docker run -it --rm -v /usr/local/cuda:/usr/local/cuda -v /usr/local/cuda-8.0:/usr/local/cuda-8.0 -v /home/maxiaolong/:/workdir centos:cuda8_gcc4.9.4 bash</p>
<h1 id="linux-使用方法">linux 使用方法</h1>
<p><a href="https://blog.csdn.net/fdipzone/article/details/24329523">生成随机数和随机字符串</a></p>
<p><a href="https://blog.csdn.net/StephenLu0422/article/details/78471551">docker 后台运行</a><br>
<a href="http://blog.sina.com.cn/s/blog_17b46b5e40102xilj.html">docker 非root运行| linux新建用户</a></p>
<p><a href="https://blog.csdn.net/Apollon_krj/article/details/70148022">shell环境变量</a></p>
<p><a href="https://stackoverflow.com/questions/9639103/is-there-a-goto-statement-in-bash">bash实现goto</a><br>
<a href="https://blog.csdn.net/train006l/article/details/79007483">linux 修改用户uid</a></p>
<p><a href="http://daizj.iteye.com/blog/2212559">shell 脚本切换用户执行命令</a></p>
<p>[centos安装gcc4.9.4](</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[tensorflow学习笔记]]></title>
        <id>https://DragonFive.github.io/post/tensorflow-note/</id>
        <link href="https://DragonFive.github.io/post/tensorflow-note/">
        </link>
        <updated>2017-11-20T03:54:27.000Z</updated>
        <content type="html"><![CDATA[<hr>
<p>title: tensorflow学习笔记</p>
<p>date: 2017/11/20 12:04:12</p>
<p>categories:</p>
<ul>
<li>深度学习<br>
tags:</li>
<li>目标检测</li>
<li>深度学习</li>
<li>神经网络</li>
</ul>
<hr>
<p>matplotlib.pyplot 的使用：</p>
<pre><code class="language-python">import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt 
fig = plt.figure()
...
...
plt.plot()
fig.savefig(&quot;&quot;)

</code></pre>
<h1 id="reference">reference</h1>
<p><a href="https://groups.google.com/a/tensorflow.org/forum/#!topic/discuss/TX32t7NHkpA">google group, kl 散度 讨论</a></p>
<p><a href="https://stackoverflow.com/questions/41863814/kl-divergence-in-tensorflow">KL Divergence in TensorFlow</a></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[CNN中卷积计算的内存和速度优化]]></title>
        <id>https://DragonFive.github.io/post/cnn-improve/</id>
        <link href="https://DragonFive.github.io/post/cnn-improve/">
        </link>
        <updated>2017-09-20T02:31:37.000Z</updated>
        <content type="html"><![CDATA[<hr>
<p>title: CNN中卷积计算的内存和速度优化</p>
<p>date: 2017/9/20 12:04:12</p>
<p>categories:</p>
<ul>
<li>深度学习<br>
tags:</li>
<li>deeplearning</li>
<li>网络优化</li>
<li>神经网络</li>
</ul>
<hr>
<p>在现在的DNN中，不管是前向传播还是反向传播，绝大多数时间花费在卷积计算中。因此对于速度提升来说，优化卷积层意义重大。</p>
<p>虽说从参数量来讲，早期的一些网络(alexbnet,VGG，googlnet等)70%以上的参数都是全连接层的。但是现在从架构上的改进已经开始减少全连接层了，比如squeezenet,mobilenet已经使用global avg pooling层取代全连接层了。那么接下来再想提速那就得从卷积层下手了。当然还有一中思路是从量化的方式减少参数量和内存消耗的（如BNN，eBNN），对于提速来说意义并不大。</p>
<h1 id="以往的卷积计算方法">以往的卷积计算方法</h1>
<h2 id="sum循环法">sum循环法</h2>
<p>时间复杂度最高，为 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><mi>H</mi><mi>W</mi><mi>M</mi><mi>K</mi><mi>K</mi><mi>C</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">O(HWMKKC)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.08125em;">H</span><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="mord mathdefault" style="margin-right:0.07153em;">K</span><span class="mord mathdefault" style="margin-right:0.07153em;">K</span><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="mclose">)</span></span></span></span> 最笨的方法，只是用来理解。</p>
<pre><code>input[C][H][W];
kernels[M][K][K][C];
output[M][H][W];
for h in 1 to H do
	for w in 1 to W do
		for o in 1 to M do
			sum = 0;
			for x in 1 to K do
				for y in 1 to K do
					for i in 1 to C do
						sum += input[i][h+y][w+x]
						*kernels[o][x][y][i];
			output[o][w][h] = sum;
</code></pre>
<h2 id="patch-building-dnn-convolution-algorithms">patch-building DNN convolution algorithms</h2>
<p>based on gemm convolution algorithm</p>
<p>优点是：比较简单，方便理解和计算</p>
<p>缺点是：需要大量的内存做中间存储</p>
<h3 id="im2col过程">im2col过程</h3>
<p>图片来自 ：贾扬清的demo convolution in caffe<br>
<a href="https://www.zhihu.com/question/28385679">在 Caffe 中如何计算卷积？</a></p>
<p><img src="https://www.github.com/DragonFive/CVBasicOp/raw/master/1507194578418.jpg" alt="1" loading="lazy"><br>
把卷积的第一个感受野里的矩阵转化成一个vector，并把各个channel的feature连接起来。<br>
<img src="https://www.github.com/DragonFive/CVBasicOp/raw/master/1507194598695.jpg" alt="2" loading="lazy"><br>
随着划窗的进行，把接下来的窗口都转化乘vector,并排放在下面<br>
<img src="https://www.github.com/DragonFive/CVBasicOp/raw/master/1507194615038.jpg" alt="3" loading="lazy"></p>
<p><img src="https://www.github.com/DragonFive/CVBasicOp/raw/master/1507194630559.jpg" alt="4" loading="lazy"><br>
最后把有Cout个卷积核，每个卷积核有C个channel,那么转化乘Cout行的vector组。最后卷积就编程矩阵乘法了。</p>
<h2 id="各种方法占用内存量">各种方法占用内存量</h2>
<figure data-type="image" tabindex="1"><img src="https://www.github.com/DragonFive/CVBasicOp/raw/master/1507511974026.jpg" alt="enter description here" loading="lazy"></figure>
<h1 id="reference">reference</h1>
<p><a href="https://www.zhihu.com/question/28385679">在 Caffe 中如何计算卷积？</a></p>
]]></content>
    </entry>
</feed>