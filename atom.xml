<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://DragonFive.github.io/</id>
    <title>dragon</title>
    <updated>2025-03-02T04:57:12.754Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://DragonFive.github.io/"/>
    <link rel="self" href="https://DragonFive.github.io/atom.xml"/>
    <subtitle>邮箱(base64)：MTY5MDMwMjk2M0BxcS5jb20=
</subtitle>
    <logo>https://DragonFive.github.io/images/avatar.png</logo>
    <icon>https://DragonFive.github.io/favicon.ico</icon>
    <rights>All rights reserved 2025, dragon</rights>
    <entry>
        <title type="html"><![CDATA[Bert 结构]]></title>
        <id>https://DragonFive.github.io/post/bert-jie-gou/</id>
        <link href="https://DragonFive.github.io/post/bert-jie-gou/">
        </link>
        <updated>2022-03-15T10:51:10.000Z</updated>
        <content type="html"><![CDATA[<p>BERT 模型是 Google 发表的论文《BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding》而提出的模型。它一种 Transformer 的双向编码器。<br>
BERT 的输入是由三个 Embedding 相加而成的，分别是 Token Embeddings，Segment Embeddings，Position Embeddings。</p>
<ul>
<li>Token Embeddings：把输入句子中每个字通过查询字向量表的方式转换为一维向量，作为模型的输入。在 Tokenization 之前，先把特殊标记符 [CLS] 和 [SEP] 额外添加到句首和句尾。在 BERT 中，Tokenization 是用 WordPiece 来完成的</li>
<li>Segment Embeddings：用于区分两个不同句子的，第一个句子是 0，第二个句子是 1。如果只有一个句子，那就都使用索引 0</li>
<li>Position Embeddings：Position Embeddings 用于给模型提供序列顺序信息的。与 Transformer 中 Positional Encoding 不同，Positional Encoding 通过三角函数计算得到的，而 Position Embeddings 是通过模型训练学习得到的。BERT 使用 Position Embeddings 是因为 BERT 作为通用预训练模型，下游任务通常对词序特征要求比较高，所以选了 Postion Embeddings 这种因通过模型训练学习而潜能比较大的方式</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transformer结构]]></title>
        <id>https://DragonFive.github.io/post/transformer-jie-gou/</id>
        <link href="https://DragonFive.github.io/post/transformer-jie-gou/">
        </link>
        <updated>2022-01-27T10:11:54.000Z</updated>
        <content type="html"><![CDATA[<p>Transformer由论文<a href="https%3A//arxiv.org/abs/1706.03762">《Attention is All You Need》</a>提出，Tensorflow的代码可以从 GitHub 上的 Tensor2Tensor 仓库获取。</p>
<h1 id="1-transformer-的结构">1. Transformer 的结构</h1>
<p>Transformer 由 Encoder 和 Decoder 两个部分组成，Encoder 和 Decoder 都包含 6 个 block。每一个encoder和decoder的内部结构如下图：</p>
<figure data-type="image" tabindex="1"><img src="https://DragonFive.github.io//post-images/1706350731616.png" alt="" loading="lazy"></figure>
<p>encoder，包含两层，一个self-attention层和一个前馈神经网络，self-attention能帮助当前节点不仅仅只关注当前的词，从而能获取到上下文的语义。<br>
decoder也包含encoder提到的两层网络，但是在这两层中间还有一层attention层，帮助当前节点获取到当前需要关注的重点内容。</p>
<p>Transformer 的工作流程大体如下：<br>
第一步：获取输入句子的每一个单词的表示向量 X，X由单词的 Embedding（Embedding就是从原始数据提取出来的Feature） 和单词位置的 Embedding 相加得到。</p>
<p>第二步：将得到的单词表示向量矩阵传入 Encoder 中，经过 6 个 Encoder block 后可以得到句子所有单词的编码信息矩阵 C。</p>
<p>第三步：将 Encoder 输出的编码信息矩阵 C传递到 Decoder 中，Decoder 依次会根据当前翻译过的单词 1~ i 翻译下一个单词 i+1，如下图所示。在使用的过程中，翻译到单词 i+1 的时候需要通过 Mask (掩盖) 操作遮盖住 i+1 之后的单词。</p>
<p>Encoder block 包含一个 Multi-Head Attention，而 Decoder block 包含两个 Multi-Head Attention (其中有一个用到 Masked)。Multi-Head Attention 上方还包括一个 Add &amp; Norm 层，Add 表示残差连接 (Residual Connection) 用于防止网络退化，Norm 表示 Layer Normalization，用于对每一层的激活值进行归一化。</p>
<figure data-type="image" tabindex="2"><img src="https://DragonFive.github.io//post-images/1706352386198.png" alt="" loading="lazy"></figure>
<p>上图是 Self-Attention 的结构，在计算的时候需要用到矩阵Q(查询),K(键值),V(值)。在实际中，Self-Attention 接收的是输入(单词的表示向量x组成的矩阵X) 或者上一个 Encoder block 的输出。而Q,K,V正是通过 Self-Attention 的输入进行线性变换得到的。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[gpu 编程]]></title>
        <id>https://DragonFive.github.io/post/gpu-bian-cheng/</id>
        <link href="https://DragonFive.github.io/post/gpu-bian-cheng/">
        </link>
        <updated>2021-11-29T11:04:18.000Z</updated>
        <content type="html"><![CDATA[<p>CPU 和 GPU 的主要区别在于它们的设计目标。CPU 的设计初衷是执行顺序指令。一直以来，为提高顺序执行性能，CPU 设计中引入了许多功能。其重点在于减少指令执行时延，使CPU 能够尽可能快地执行一系列指令。</p>
<p>而 GPU 则专为并行和高吞吐量而设计，，但这种设计导致了中等至高程度的指令时延。当需要进行数百万甚至数十亿次这样的计算时，由于 GPU 具有强大的大规模并行能力，它将比 CPU 更快地完成这些计算任务。</p>
<figure data-type="image" tabindex="1"><img src="https://DragonFive.github.io//post-images/1706353703504.png" alt="" loading="lazy"></figure>
<p>CPU 在芯片上有大量的内存和缓存以及控制单元，但只有少量的ALU和寄存器。GPU缓存较少，但ALU较多，GPU 拥有大量线程和强大的计算能力，即使单个指令具有高延迟，GPU 也会有效地调度线程运行，以便它们在任意时间点都能利用计算能力。</p>
<h1 id="sm">SM</h1>
<p>GPU 的计算架构GPU 由一系列流式多处理器（SM）组成，其中每个 SM 又由多个流式处理器、核心或线程组成。<br>
每个 SM 都拥有一定数量的片上内存（on-chip memory），通常称为共享内存或临时存储器，这些共享内存被所有的核心所共享。同样，SM 上的控制单元资源也被所有的核心所共享。此外，每个 SM 都配备了基于硬件的线程调度器，用于执行线程。除此之外，每个 SM 还配备了几个功能单元或其他加速计算单元，例如张量核心（tensor core）。</p>
<h1 id="gpu的内存架构">GPU的内存架构</h1>
<ul>
<li>寄存器：让我们从寄存器开始。GPU 中的每个 SM 都拥有大量寄存器。这些寄存器在核心之间共享，并根据线程需求动态分配。在执行过程中，每个线程都被分配了私有寄存器，其他线程无法读取或写入这些寄存器。</li>
<li>常量缓存：接下来是芯片上的常量缓存。这些缓存用于缓存 SM 上执行的代码中使用的常量数据。为利用这些缓存，程序员需要在代码中明确将对象声明为常量，以便 GPU 可以将其缓存并保存在常量缓存中。</li>
<li>共享内存：每个 SM 还拥有一块共享内存或临时内存，它是一种小型、快速且低时延的片上可编程 SRAM 内存，供运行在 SM 上的线程块共享使用。共享内存的设计思路是，如果多个线程需要处理相同的数据，只需要其中一个线程从全局内存（global memory）加载，而其他线程将共享这一数据。合理使用共享内存可以减少从全局内存加载重复数据的操作，并提高内核执行性能。共享内存还可以用作线程块（block）内的线程之间的同步机制。</li>
<li>L1 缓存：每个 SM 还拥有一个 L1 缓存，它可以缓存从 L2 缓存中频繁访问的数据。</li>
<li>L2 缓存：所有 SM 都共享一个 L2 缓存，它用于缓存全局内存中被频繁访问的数据，以降低时延。需要注意的是，SM 并不知道它是从 L1 还是 L2 中获取数据。SM 从全局内存中获取数据，这类似于 CPU 中 L1/L2/L3 缓存的工作方式。</li>
<li>全局内存：GPU 还拥有一个片外全局内存，它是一种容量大且带宽高的动态随机存取存储器（DRAM）。由于与 SM 相距较远，全局内存的时延相当高。</li>
</ul>
<h1 id="gpu-kernel">GPU kernel</h1>
<p>GPU 上执行 kernel，我们需要启用多个线程，这些线程总体上被称为一个网格（grid），但网格还具有更多的结构。一个网格由一个或多个线程块（有时简称为块）组成，而每个线程块又由一个或多个线程组成。线程块和线程的数量取决于数据的大小和我们所需的并行度。例如，在向量相加的示例中，如果我们要对 256 维的向量进行相加运算，那么可以配置一个包含 256 个线程的单个线程块，这样每个线程就可以处理向量的一个元素。</p>
<p>由于 SM 的数量有限，而大型 kernel 可能包含大量线程块，因此并非所有线程块都可以立即分配执行。GPU 会维护一个待分配和执行的线程块列表，当有任何一个线程块执行完成时，GPU 会从该列表中选择一个线程块执行。</p>
<p>线程还会进一步划分为大小为 32 的组（称为 warp），并一起分配到一个称为处理块（processing block）的核心集合上进行执行。SM 通过获取并向所有线程发出相同的指令，以同时执行 warp 中的所有线程。然后这些线程将在数据的不同部分，同时执行该指令。</p>
<p>由于多个线程同时执行相同的指令，这种 warp 的执行模型也称为单指令多线程 （SIMT）。这类似于 CPU 中的单指令多数据（SIMD）指令。</p>
<h2 id="warp">warp</h2>
<p>即使 SM 内的所有处理块（核心组）都在处理 warp，但在任何给定时刻，只有其中少数块正在积极执行指令。因为 SM 中可用的执行单元数量是有限的。有些指令的执行时间较长，这会导致 warp 需要等待指令结果。在这种情况下，SM 会将处于等待状态的 warp 休眠，并执行另一个不需要等待任何结果的 warp。这使得 GPU 能够最大限度地利用所有可用计算资源，并提高吞吐量。</p>
<p>零计算开销调度：由于每个 warp 中的每个线程都有自己的一组寄存器，因此 SM从执行一个 warp 切换到另一个 warp 时没有额外计算开销。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[快手bagua使用教程翻译与摘抄]]></title>
        <id>https://DragonFive.github.io/post/kuai-shou-bagua-shi-yong-jiao-cheng-fan-yi-yu-zhai-chao/</id>
        <link href="https://DragonFive.github.io/post/kuai-shou-bagua-shi-yong-jiao-cheng-fan-yi-yu-zhai-chao/">
        </link>
        <updated>2021-09-01T05:32:43.000Z</updated>
        <content type="html"><![CDATA[<p>翻译自<a href="https://bagua-tutorials.kwai-seattle.com/introduction">八卦文档</a></p>
<p>八卦已经整合的原语包括</p>
<ul>
<li>集中式同步通信（AllReduce）</li>
<li>去中心化同步通讯</li>
<li>低精度通信</li>
</ul>
<p>其有效性已经在各种场景和模型中得到验证，包括ImageNet上的VGG和ResNet，Bert Large，以及快手等多个大规模工业应用：</p>
<ul>
<li>支持数十个TB参数模型训练的推荐系统，</li>
<li>超过 10 亿个图像/视频的视频/图像理解，</li>
<li>ASR 与 TB 级数据集，</li>
<li>等等。</li>
</ul>
<h1 id="使用举例">使用举例</h1>
<p>使用bagua类似于使用其他分布式训练库，如 PyTorch DDP 和 Horovod。</p>
<pre><code class="language-cpp">def main():
    args = parse_args()
    # define your model and optimizer
    model = MyNet().to(args.device)
    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr)
    # transform to Bagua wrapper
    from bagua.torch_api.algorithms import gradient_allreduce
    model = model.with_bagua(
        [optimizer], gradient_allreduce.GradientAllReduceAlgorithm()
    )

    # train the model over the dataset
    for epoch in range(args.epochs):
        for b_idx, (inputs, targets) in enumerate(train_loader):
            outputs = model(inputs)
            loss = torch.nn.CrossEntropyLoss(outputs, targets)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

</code></pre>
<p>计算使用的是pytorch, 只需要包上一个通信算法原语就可以了。</p>
<h1 id="通信算法">通信算法</h1>
<p>八卦依靠分布式学习算法的多样性而蓬勃发展。 系统的极大灵活性使得可以平滑地结合各种SOTA算法，同时在执行过程中提供性能的自动优化。 对于最终用户，八卦提供了广泛的算法选择，她可以轻松地尝试执行她的任务。 对于算法开发人员来说，八卦是一个游乐场，在那里她可以只专注于算法本身（例如，逻辑和控制），而无需在不同的算法之间重新发明轮子（例如，通信原语和系统优化）。</p>
<h2 id="梯度-allreduce">梯度 allreduce</h2>
<h3 id="总览">总览</h3>
<p>Gradient AllReduce 算法是一种流行的同步数据并行分布式算法。 它是大多数现有解决方案中实现的算法，比如  <a href="https://pytorch.org/tutorials/intermediate/ddp_tutorial.html">PyTorch DistributedDataParallel</a>, <a href="https://horovod.ai/">Horovod</a>, 以及<a href="https://www.tensorflow.org/guide/distributed_training">TensorFlow Mirrored Strategy</a>。</p>
<h3 id="算法">算法</h3>
<p>使用此算法，每个worker在每次迭代中执行以下步骤。</p>
<ul>
<li>使用小批量计算梯度。</li>
<li>使用 AllReduce 集合计算所有worker的梯度平均值。</li>
<li>用平均梯度更新模型。</li>
</ul>
<p>在八卦中，这个算法是通过 GradientAllReduce 算法类来支持的。 八卦中 GradientAllReduce 实现的默认性能应该与 PyTorch DDP 相当，并且在大多数情况下比 Horovod 更快。 八卦支持额外的优化，例如可以在实例化 GradientAllReduce 类时配置的分层通信。 在某些情况下，例如当机器间网络成为瓶颈时，它们可以使八卦比其他实现更快。</p>
<h3 id="举例">举例</h3>
<p>运行 Gradient AllReduce 的完整示例可以在带有 --algorithm gradient_allreduce 命令行参数的 <a href="https://github.com/BaguaSys/examples/blob/main/benchmark/synthetic_benchmark.py">八卦示例</a>中找到。</p>
<p>您需要初始化八卦算法（请参阅 <a href="https://bagua.readthedocs.io/en/latest/autoapi/bagua/torch_api/algorithms/gradient_allreduce/index.html">API 文档</a>了解您可以自定义哪些参数）：</p>
<pre><code class="language-python">from bagua.torch_api.algorithms import gradient_allreduce
algorithm = gradient_allreduce.GradientAllReduceAlgorithm()
</code></pre>
<p>然后用以下方法装饰您的模型：</p>
<pre><code class="language-python">model = model.with_bagua([optimizer], algorithm)
</code></pre>
<h2 id="bytegrad">bytegrad</h2>
<h3 id="总览-2">总览</h3>
<p>大规模分布式训练需要大量的通信成本，对于大型模型尤其如此。 例如，在使用 AllReduce 同步梯度的传统同步分布式设置中（许多库都是这种情况，比如  <a href="https://github.com/horovod/horovod">Horovod</a> 和 <a href="https://pytorch.org/tutorials/intermediate/ddp_tutorial.html">PyTorch DDP</a>), 在训练过程的每次迭代中，需要在每个 worker 上发送和接收大小等于模型大小的梯度。 这样的通信成本很快成为很多场景下的训练瓶颈。</p>
<p>有很多关于如何应用模型/梯度压缩来节省这种通信成本的<a href="https://awesomeopensource.com/project/chester256/Model-Compression-Papers?categoryPage=21">现有论文</a>。 八卦提供了一种内置的梯度压缩算法，称为 ByteGrad，它在通信之前将梯度浮点数压缩到 8bit 字节。 这节省了原始成本的 3/4。 它通过优化的 CUDA 内核和分层通信实现了高精度的 min-max 量化算子。 这使得它比现有框架中(比如  <a href="https://pytorch.org/docs/stable/ddp_comm_hooks.html#powersgd-communication-hook">PyTorch PowerSGD</a>)的其他压缩实现快得多（在我们的基准测试中大约快 50%）。并且在相同的时期数 ByteGrad 在大多数任务上收敛近似于全精度算法。</p>
<h3 id="算法-2">算法</h3>
<p>ByteGrad 在每次迭代中执行以下步骤。 假设我们有 m 个节点，每个节点有 n 个 GPU。</p>
<ol>
<li>为所有<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow><annotation encoding="application/x-tex">i,j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.85396em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">i</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.05724em;">j</span></span></span></span> 计算第 i 个节点的第 j 个 GPU 上的梯度 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>g</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">g_{i,j}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.716668em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span></li>
<li>每个节点上的第一个 GPU 做一个 reduce 操作来计算同一节点上所有 GPU 的梯度的平均值，定义为第 i 个节点的 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>G</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">G_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">G</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></li>
<li>第 i 个节点上的第一个 GPU 使用量化函数 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>Q</mi><mo>(</mo><mo>⋅</mo><mo>)</mo><mi mathvariant="normal">：</mi><mi>Q</mi><mo>(</mo><msub><mi>G</mi><mi>i</mi></msub><mo>)</mo></mrow><annotation encoding="application/x-tex">Q(⋅)：Q(G_i )</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">Q</span><span class="mopen">(</span><span class="mord">⋅</span><span class="mclose">)</span><span class="mord cjk_fallback">：</span><span class="mord mathdefault">Q</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">G</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> 对所有 i 量化梯度 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>G</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">G_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">G</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>。 然后每个节点在节点之间交换量化版本，使得每个节点都有所有 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>Q</mi><mo>(</mo><msub><mi>G</mi><mi>i</mi></msub><mo>)</mo></mrow><annotation encoding="application/x-tex">Q(G_i )</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">Q</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">G</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> 的平均值</li>
<li>每个节点上的第一个 GPU 将所有 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>Q</mi><mo>(</mo><msub><mi>G</mi><mi>i</mi></msub><mo>)</mo></mrow><annotation encoding="application/x-tex">Q(G_i)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">Q</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">G</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> 的平均值广播给同一节点上的所有其他 GPU，并且所有 worker 上的所有 GPU 都使用此量化平均值来更新模型</li>
</ol>
<p>量化函数 Q(⋅) 计算其输入的最小值 x 和最大值 y，并将 [x,y] 拆分为均匀间隔的 256 个区间。 然后用一个 8 位整数表示其输入的每个元素，表示原始元素在哪个区间。</p>
<h3 id="用法举例">用法举例</h3>
<p>您需要初始化八卦算法（请参阅 <a href="https://bagua.readthedocs.io/en/latest/autoapi/bagua/torch_api/algorithms/bytegrad/index.html">API 文档</a>了解您可以自定义哪些参数）：</p>
<pre><code class="language-python">from bagua.torch_api.algorithms import bytegrad
algorithm = bytegrad.ByteGradAlgorithm()
model = model.with_bagua([optimizer], algorithm)
</code></pre>
<h2 id="去中心化sgd">去中心化SGD</h2>
<h3 id="去中心化训练概述">去中心化训练概述</h3>
<p>Decentralized SGD 是一种数据并行的分布式学习算法，它消除了所有worker之间集中式全局模型的要求，这使得它在通信模式上与基于 Allreduce 或基于参数服务器的算法有很大不同。 使用去中心化 SGD，每个 worker 只需要与一个或几个特定的 worker 交换数据，而不是全局聚合数据。 因此，去中心化通信的通信连接数比 Allreduce 少得多，通信开销比 Parameter Server 更均衡。 尽管去中心化 SGD 可能会导致每个worker的模型不同，但理论上已经证明，去中心化 SGD 算法的收敛速度与其中心化对应版本相同。 您可以在我们的 <a href="https://arxiv.org/abs/1705.09056">论文</a>中找到关于去中心化 SGD 的详细分析 。</p>
<h3 id="去中心化训练算法">去中心化训练算法</h3>
<p>目前，不时有许多分散的训练算法被提出。 这些令人惊叹的作品集中在分散训练的不同方面，如<strong>peer选择、数据压缩、异步</strong>等，并提供了许多有前景的见解。 到目前为止，八卦已经整合了两种基本的去中心化算法，即去中心化 SGD 和低精度去中心化 SGD。 凭借八卦对去中心化的自动系统支持，我们预计在不久的将来会实现越来越多的去中心化算法。</p>
<h3 id="decentralized-sgd">Decentralized SGD</h3>
<p>现在我们将描述在八卦中实现的去中心化 SGD 算法。 假设worker数量为n，worker i上的模型参数为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>x</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mo separator="true">,</mo><mi>i</mi><mo>∈</mo><mrow><mn>0</mn><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><mi>n</mi><mi mathvariant="normal">−</mi><mn>1</mn></mrow></mrow><annotation encoding="application/x-tex">x^{(i)},i∈{0,...,n−1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0824399999999998em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathdefault mtight">i</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">i</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8388800000000001em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">n</span><span class="mord">−</span><span class="mord">1</span></span></span></span></span>。 每个worker 都能够直接从任何其他worker发送或接收数据。 在每次迭代 t 中，算法重复以下步骤：</p>
<ol>
<li>每个worker i 计算迭代的局部梯度 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msubsup><mi>g</mi><mi>t</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup></mrow><annotation encoding="application/x-tex">g_t^{(i)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2905559999999998em;vertical-align:-0.24575599999999992em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em;"><span style="top:-2.454244em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span><span style="top:-3.2197999999999998em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathdefault mtight">i</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.24575599999999992em;"><span></span></span></span></span></span></span></span></span></span>.</li>
<li>将本地模型与其选定的同伴的模型进行平均（表示为 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msubsup><mi>x</mi><mi>t</mi><mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow></msubsup></mrow><annotation encoding="application/x-tex">x_t^{(j)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2905559999999998em;vertical-align:-0.24575599999999992em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em;"><span style="top:-2.454244em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span><span style="top:-3.2197999999999998em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.24575599999999992em;"><span></span></span></span></span></span></span></span></span></span>), <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msubsup><mi>x</mi><mfrac><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mn>2</mn></mfrac><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup><mo>=</mo><mfrac><mrow><mi>t</mi><mo>(</mo><mi>i</mi><mo>)</mo><mi mathvariant="normal">​</mi><mo>+</mo><mi>x</mi><mi>t</mi><mo>(</mo><mi>j</mi><mo>)</mo></mrow><mn>2</mn></mfrac></mrow><annotation encoding="application/x-tex">x_{\frac{t+1}{2}}^{(i)}=\frac{t(i)​+xt(j)}{2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.6918199999999999em;vertical-align:-0.6470199999999999em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em;"><span style="top:-2.59378em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mopen nulldelimiter sizing reset-size3 size6"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8443142857142857em;"><span style="top:-2.656em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span><span style="top:-3.2255000000000003em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line mtight" style="border-bottom-width:0.049em;"></span></span><span style="top:-3.384em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.344em;"><span></span></span></span></span></span><span class="mclose nulldelimiter sizing reset-size3 size6"></span></span></span></span></span><span style="top:-3.5198em;margin-right:0.05em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathdefault mtight">i</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.6470199999999999em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.355em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.01em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.485em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mopen mtight">(</span><span class="mord mathdefault mtight">i</span><span class="mclose mtight">)</span><span class="mord mtight">​</span><span class="mbin mtight">+</span><span class="mord mathdefault mtight">x</span><span class="mord mathdefault mtight">t</span><span class="mopen mtight">(</span><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>.</li>
<li>使用局部梯度更新平均模型.<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msubsup><mi>x</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup><mo>=</mo><msubsup><mi>x</mi><mfrac><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mn>2</mn></mfrac><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup><mo>−</mo><mi>γ</mi><msubsup><mi>g</mi><mi>t</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup></mrow><annotation encoding="application/x-tex">x_{t+1}^{(i)}=x_{\frac{t+1}{2}}^{(i)}-\gamma g_t^{(i)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.3694389999999999em;vertical-align:-0.3246389999999999em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em;"><span style="top:-2.433692em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2198em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathdefault mtight">i</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3246389999999999em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.6918199999999999em;vertical-align:-0.6470199999999999em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em;"><span style="top:-2.59378em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mopen nulldelimiter sizing reset-size3 size6"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8443142857142857em;"><span style="top:-2.656em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span><span style="top:-3.2255000000000003em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line mtight" style="border-bottom-width:0.049em;"></span></span><span style="top:-3.384em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.344em;"><span></span></span></span></span></span><span class="mclose nulldelimiter sizing reset-size3 size6"></span></span></span></span></span><span style="top:-3.5198em;margin-right:0.05em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathdefault mtight">i</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.6470199999999999em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.2905559999999998em;vertical-align:-0.24575599999999992em;"></span><span class="mord mathdefault" style="margin-right:0.05556em;">γ</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em;"><span style="top:-2.454244em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span><span style="top:-3.2197999999999998em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathdefault mtight">i</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.24575599999999992em;"><span></span></span></span></span></span></span></span></span></span></li>
</ol>
<p>在步骤 2 中，我们采用一种策略为每次迭代中的每个 worker 选择一个 peer，这样所有 worker 都正确配对并且数据交换是有效的，因为每个 worker 可以在迭代之间与不同的 peer 交换数据。 简而言之，我们的策略将worker 平均分成两组，并在两组之间动态配对工worker，每次迭代都不同。</p>
<blockquote>
<p>点评，最后以谁的参数作为最终的ckpt呢？</p>
</blockquote>
<h3 id="通信开销">通信开销</h3>
<p>去中心化 SGD 的通信开销与网络程度高度相关，即一个worker与其他worker的连接数。 不同的拓扑或策略会导致不同程度的网络。 很明显，我们之前描述的Decentralized SGD算法的网络度为1。因此，在每次迭代中，一个worker只需要与一个worker建立一个连接来交换模型大小的1倍。 我们比较了不同通信模式在最繁忙节点的延迟和带宽方面的通信复杂性。</p>
<table>
<thead>
<tr>
<th style="text-align:center">算法</th>
<th style="text-align:center">时延复杂度</th>
<th style="text-align:center">带宽复杂度</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Allreduce (Ring)</td>
<td style="text-align:center">O(n)</td>
<td style="text-align:center">O(1)</td>
</tr>
<tr>
<td style="text-align:center">ps</td>
<td style="text-align:center">O(1)</td>
<td style="text-align:center">O(n)</td>
</tr>
<tr>
<td style="text-align:center">八卦去中心化SGD</td>
<td style="text-align:center">O(1)</td>
<td style="text-align:center">O(1)</td>
</tr>
</tbody>
</table>
<h3 id="用法举例-2">用法举例</h3>
<p>您需要初始化八卦算法（请参阅 <a href="https://bagua.readthedocs.io/en/latest/autoapi/bagua/torch_api/algorithms/decentralized/index.html">API 文档</a>了解您可以自定义哪些参数）：</p>
<pre><code class="language-python">from bagua.torch_api.algorithms import decentralized algorithm = decentralized.DecentralizedAlgorithm()
model = model.with_bagua([optimizer], algorithm)
</code></pre>
<h3 id="其他的算法">其他的算法</h3>
<p>八卦提供的其他的算法没什么好说的，可以参考<a href="https://bagua-tutorials.kwai-seattle.com/algorithms/">原文</a></p>
<p>八卦还支持<a href="https://baguasys.github.io/tutorials/elastic-training/index.html">弹性训练</a>, 看上去是保存中间状态实现的。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[快手的八卦：BAGUA: Scaling up Distributed Learning with System Relaxations论文翻译与赏析]]></title>
        <id>https://DragonFive.github.io/post/kuai-shou-de-ba-gua-bagua-lun-wen-fan-yi-yu-shang-xi/</id>
        <link href="https://DragonFive.github.io/post/kuai-shou-de-ba-gua-bagua-lun-wen-fan-yi-yu-shang-xi/">
        </link>
        <updated>2021-08-31T11:21:16.000Z</updated>
        <content type="html"><![CDATA[<p>《八卦-使用系统松弛（Relaxations）扩展分布式学习</p>
<blockquote>
<p>点评：这篇论文的作者中的后面发了很多论文，快手人才济济呀</p>
</blockquote>
<h1 id="摘要">摘要</h1>
<p>近年来，用于分布式数据并行训练的系统列表越来越多。 现有系统在很大程度上适合两种范式，即参数服务器和 MPI 风格的集体操作。 在算法方面，研究人员提出了广泛的技术来通过“系统松弛”降低通信：<strong>量化、去中心化和通信延迟</strong>。然而，大多数（如果不是全部）现有系统仅依赖于基于标准同步和异步随机梯度 (SG) 的优化，因此无法利用机器学习社区最近开发的所有可能的优化。鉴于当前系统和理论之间出现的这种差距，我们构建了 BAGUA，<strong>这是一个通信框架</strong>，其设计目标是提供一个既灵活又模块化的系统抽象，以支持最先进的分布式训练系统松弛技术 . 在新系统设计的支持下，BAGUA 具有强大的实现和扩展各种最先进的分布式学习算法的能力。 在拥有多达 16 台机器（128 个 GPU）的生产集群中，BAGUA 可以在端到端的训练时间内在各种任务中以显着的优势（高达 1.95 倍）优于 PyTorch-DDP、Horovod 和 BytePS。 此外，我们进行了严格的权衡探索，表明不同的算法和系统松弛在不同的网络条件下实现了最佳性能。</p>
<h1 id="1-引言">1. 引言</h1>
<p>分布式机器学习系统不断提高的可扩展性和性能一直是机器学习技术快速发展的主要推动力之一。模型质量的每一次飞跃都是通过模型大小和可以训练模型的数据量的增长以及计算量的快速增加来实现的。这种改进的背后是两个主要的促成因素：硬件加速（例如 GPU 和 TPU）和高效且可扩展的分布式训练算法的开发。可以说可扩展的分布式训练系统是现代深度学习技术的基石。</p>
<p><em>表一：不同的系统松弛技术</em></p>
<table>
<thead>
<tr>
<th style="text-align:center">论文</th>
<th style="text-align:center">同步</th>
<th style="text-align:center">精确度</th>
<th style="text-align:center">集中式</th>
<th style="text-align:center">PyTorch-DDP</th>
<th style="text-align:center">Horovod</th>
<th style="text-align:center">BytePS</th>
<th style="text-align:center">BAGUA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">limu ps</td>
<td style="text-align:center">同步</td>
<td style="text-align:center">全精度</td>
<td style="text-align:center">中心</td>
<td style="text-align:center">v</td>
<td style="text-align:center">v</td>
<td style="text-align:center">v</td>
<td style="text-align:center">v</td>
</tr>
<tr>
<td style="text-align:center"><a href="https://arxiv.org/abs/1902.00340">arXiv:1902.00340</a></td>
<td style="text-align:center">同步</td>
<td style="text-align:center">全精度</td>
<td style="text-align:center">去中心</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">v</td>
</tr>
<tr>
<td style="text-align:center"><a href="https://arxiv.org/abs/1610.02132">arXiv:1610.02132</a></td>
<td style="text-align:center">同步</td>
<td style="text-align:center">低精度</td>
<td style="text-align:center">中心</td>
<td style="text-align:center">v</td>
<td style="text-align:center">v</td>
<td style="text-align:center">v</td>
<td style="text-align:center">v</td>
</tr>
<tr>
<td style="text-align:center"><a href="https://arxiv.org/abs/1809.07599">arXiv:1809.07599</a></td>
<td style="text-align:center">同步</td>
<td style="text-align:center">低精度</td>
<td style="text-align:center">去中心</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">v</td>
</tr>
<tr>
<td style="text-align:center"><a href="https://arxiv.org/abs/1803.06443">arXiv:1803.06443</a></td>
<td style="text-align:center">异步</td>
<td style="text-align:center">全精度</td>
<td style="text-align:center">中心</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">v</td>
<td style="text-align:center">v</td>
</tr>
<tr>
<td style="text-align:center"><a href="https://arxiv.org/abs/1609.08326">arXiv:1609.08326</a></td>
<td style="text-align:center">异步</td>
<td style="text-align:center">全精度</td>
<td style="text-align:center">去中心</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">v</td>
</tr>
<tr>
<td style="text-align:center"><a href="https://arxiv.org/abs/1710.06952">arXiv:1710.06952</a></td>
<td style="text-align:center">异步</td>
<td style="text-align:center">低精度</td>
<td style="text-align:center">中心</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">v</td>
</tr>
<tr>
<td style="text-align:center">无</td>
<td style="text-align:center">异步</td>
<td style="text-align:center">低精度</td>
<td style="text-align:center">去中心</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
</tbody>
</table>
<blockquote>
<p>点评：这里对于各项技术的综述视角不错，值得学习</p>
</blockquote>
<h3 id="数据并行训练系统的现状">数据并行训练系统的现状:</h3>
<p>在本文中，我们将自己的范围定为并专注于数据并行训练，这是最流行的分布式训练范式之一，其中数据集在不同的工作人员之间进行分区，并且模型适合单个设备。 毫不奇怪，最近几年见证了越来越多的分布式数据并行训练系统。现有系统适合<strong>两个范式</strong>：ps(遵循 Li mu等人所做的开创性工作<a href="http://web.eecs.umich.edu/~mosharaf/Readings/Parameter-Server.pdf">ps</a>) 和 使用 MPI 集体操作（Sergeev等人<a href="https://arxiv.org/abs/1802.05799">horovod</a>），例如 Allreduce。两种范式都支持工业规模的分布式训练系统。基于ps 的有 Adam (Microsoft) , TensorFlow (Google) , Poseidon (Petuum) , Angel (Tencent) , and BytePS (ByteDance)。基于 MPI 风格的有：PyTorch-DDP (Facebook) , Mariana (Tencent) , MALT (NEC Labs) , NCCL (NVIDIA) , and Horovod (Uber)。这些系统通常涉及机器学习、系统和数据管理社区的共同努力，并成功地使分布式训练变得更容易和更具可扩展性。</p>
<blockquote>
<p>点评：byteps 是ps模式，还是allreduce模式在知乎上也引发正义</p>
</blockquote>
<h3 id="数据并行训练算法的现状">数据并行训练算法的现状</h3>
<p>在理论和算法方面，研究人员也一直在积极改进基于标准同步和异步随机梯度 (SG) 算法的性能。 注意到一个主要的系统瓶颈是通信，研究人员提出了一系列技术来降低通信开销，主要是通过“放松”通信的某些方面。比如包括 (1) 通信压缩（例如，量化、稀疏化和误差补偿 ），(2) 通信分散化和 (3) 通信延迟（例如 LocalSGD）和异步 。这些技术针对不同的工作负载和不同的网络条件进行了优化。 这些技术在<strong>带宽和延迟方面显着降低通信开销</strong>，或增加对落后者存在的容忍度方面有很好的保障。</p>
<h3 id="系统与理论之间新出现的鸿沟">系统与理论之间新出现的鸿沟</h3>
<p>在本文中，我们受到当前系统和理论景观之间新出现的差距的启发。尽管分布式学习理论和算法在系统松弛方面取得了最新进展，但大多数（如果不是全部）现有系统仅依赖于标准的同步和异步基于随机梯度 (SG) 的算法。 主要后果是现有系统没有利用机器学习社区一直在开发的所有可能的优化，并且许多实际应用程序可能会进一步加速。在本文中，我们问：我们能否通过通信系统松弛来进一步加速分布式学习系统？ 如果是这样，为此目的正确的系统抽象是什么？</p>
<h3 id="技术挑战">技术挑战</h3>
<p>要缩小这一差距需要的不仅仅是简单地使用这些算法来实现从现有系统中抽象出参数服务器和 Allreduce。有两个挑战。首先，在参数服务器或 Allreduce 范式中直接和自然地支持这些系统松弛是具有挑战性的。例如，使用参数服务器提供的 put/get 抽象来支持需要服务器端内存和状态的算法是具有挑战性的，这是大多数使用错误补偿的通信压缩算法所需要的。 同样，两种范式都很难支持去中心化通信。因此，必须从根本上重新审视系统抽象的设计，以支持当今许多松弛算法。其次，我们需要支持模块化系统抽象和优化来处理这些系统松弛的多样性。当 Horovod 和 BytePS 等现有系统针对性能进行优化时，它们通常会关注基于 教科书式的随机梯度算法的通信模式。当我们希望支持大量训练算法时，如表 1 所示，我们无法单独优化每个算法； 相反，我们必须了解如何在通用框架中自动优化这组不同的算法。</p>
<h3 id="bagua-系统和我们的贡献">BAGUA 系统和我们的贡献</h3>
<p>受这两个挑战的启发，我们构建了 BAGUA，这是一个通信框架，其设计目标是支持分布式训练的最先进系统松弛技术。 我们做出了两项技术贡献：</p>
<ul>
<li>我们的第一个贡献是 BAGUA 的系统设计，它为通信提供了模块化设计。 我们的抽象囊括了参数服务器和 Allreduce 范式，并提供了一系列 MPI 风格的集体操作，以促进具有不同精度和集中化策略的通信。<br>
这种抽象非常灵活和模块化，足以支持许多算法，如表 1 所示。此外，我们还开发了一个简单的自动优化框架，可以加速在 BAGUA 框架内实现的算法。 该框架背后的关键是通信的自动batching和调度。 与之前的工作，如 Horovod 和 BytePS 不同，我们的优化框架可以更广泛地应用，超出基于标准 SG 的算法</li>
<li>我们的第二个贡献是围绕两个假设进行的广泛的实证研究：(1) 通过支持不同的系统松弛技术，BAGUA 能够通过现实世界的基础设施为现实世界的应用程序和工作负载提供比现有系统显著的改进； (2) 通过支持多种系统松弛，BAGUA 能够在不同的网络条件下提供可扩展的 ML 训练，以允许用户选择不同的算法。为此，我们对两者进行了大规模的实证研究在快手公司运行的基准测试任务和实际应用程序。 在具有多达 16 台机器（总共 128 个 GPU，使用 Tensor Core 聚合 2 petaFLOPS）的集群上，我们按照 V100 GPU 机器 (p3.8xlarge, p3. 16xlarge、p3dn.24xlarge) 在 AWS 上连接：10Gbps、25Gbps 和 100Gbps，使用 TCP/IP 连接。在各种任务中，BAGUA 的性能明显优于 BytePS 、Horovod 和 PyTorch-DDP（对于 10Gbps 可达 1.9 倍，对于 100Gbps 可达 1.34 倍）。 此外，我们进行了严格的权衡探索，表明不同的算法和系统松弛可以在不同的网络条件下实现最佳性能。 这说明了向最终用户提供这种不同的算法队列的重要性。</li>
</ul>
<h3 id="局限和进步">局限和进步</h3>
<p>当前的 BAGUA 系统存在一些局限性，我们希望我们在构建 BAGUA 方面所做的努力可以帮助和启发未来在这些令人兴奋的方向上的研究。首先，BAGUA 没有提供一种原则性的方法来帮助用户自动选择最合适的系统松弛来应用。 在 BAGUA 为所有这些算法提供支持之后，一个令人兴奋的方向是了解如何构建一个有原则的自动调整系统。 其次，当前版本的 BAGUA 只关注数据并行性，集成其他技术（例如模型并行性和流水线并行性）并理解系统抽象是有趣的未来工作。</p>
<h3 id="大纲">大纲</h3>
<p>大纲 本文的其余部分组织如下。 我们首先简要回顾第 2 节中的数据并行训练和现有系统的优化框架，作为准备工作和相关工作。 我们在第 3 节中讨论了 BAGUA 的设计和优化。我们在第 4 节中描述了我们的实验研究，并在第 5 节中总结。</p>
<h1 id="2-前期工作及相关工作">2. 前期工作及相关工作</h1>
<p>BAGUA 建立在数十年关于分布式机器学习系统和算法的研究之上。 其中很多来自数据库社区。 我们现在总结相关工作并详细讨论一些以提供背景和背景。 我们建议读者参考 <a href="https://arxiv.org/abs/2104.05245">Distributed Learning Systems with First-order Methods</a> 对不同系统松弛算法的严格理论分析。</p>
<h2 id="21-基于数据并行sg的算法">2.1 基于数据并行SG的算法</h2>
<p>分布式学习系统的基石是基于数据并行随机梯度（DP-SG）的算法，这是现有系统支持和优化的主导算法。 设 D 是一个数据集，n 是worker 的数量，每个worker  i 持有它在步骤 t 的数据 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msubsup><mi>D</mi><mi>i</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msubsup></mrow><annotation encoding="application/x-tex">D^{(t)}_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.321664em;vertical-align:-0.27686399999999994em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em;"><span style="top:-2.4231360000000004em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span><span style="top:-3.2198em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathdefault mtight">t</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.27686399999999994em;"><span></span></span></span></span></span></span></span></span></span> 和模型副本的分区：<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msubsup><mi>x</mi><mi>i</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msubsup></mrow><annotation encoding="application/x-tex">x^{(t)}_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.321664em;vertical-align:-0.27686399999999994em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em;"><span style="top:-2.4231360000000004em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span><span style="top:-3.2198em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathdefault mtight">t</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.27686399999999994em;"><span></span></span></span></span></span></span></span></span></span>。 设 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msubsup><mi>g</mi><mi>i</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msubsup></mrow><annotation encoding="application/x-tex">g^{(t)}_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.321664em;vertical-align:-0.27686399999999994em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em;"><span style="top:-2.4231360000000004em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span><span style="top:-3.2198em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathdefault mtight">t</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.27686399999999994em;"><span></span></span></span></span></span></span></span></span></span> 是第 t 步worker i 的随机梯度，教科书 DP-SG 更新worker i 处的每个本地模型副本，如下所示：</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msubsup><mi>x</mi><mi>i</mi><mrow><mo>(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo>)</mo></mrow></msubsup><mo>=</mo><msubsup><mi>x</mi><mi>i</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msubsup><mi mathvariant="normal">−</mi><mi>γ</mi><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msubsup><mi>g</mi><mi>j</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msubsup></mrow><annotation encoding="application/x-tex">x^{(t+1)}_i = x^{(t)}_i − γ \sum^n_{j=1} g^{(t)}_j
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.321664em;vertical-align:-0.276864em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em;"><span style="top:-2.4231360000000004em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span><span style="top:-3.2198em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathdefault mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.276864em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:3.0651740000000007em;vertical-align:-1.4137769999999998em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em;"><span style="top:-2.4231360000000004em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span><span style="top:-3.2198em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathdefault mtight">t</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.276864em;"><span></span></span></span></span></span></span><span class="mord">−</span><span class="mord mathdefault" style="margin-right:0.05556em;">γ</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.6513970000000007em;"><span style="top:-1.872331em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.050005em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3000050000000005em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.4137769999999998em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em;"><span style="top:-2.4231360000000004em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span><span style="top:-3.2198em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathdefault mtight">t</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.412972em;"><span></span></span></span></span></span></span></span></span></span></span></p>
<p>其中γ是学习率。 为了实现这一点，所有机器都需要交换它们的局部梯度 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msubsup><mi>g</mi><mi>i</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msubsup></mrow><annotation encoding="application/x-tex">g^{(t)}_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.321664em;vertical-align:-0.27686399999999994em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em;"><span style="top:-2.4231360000000004em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span><span style="top:-3.2198em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathdefault mtight">t</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.27686399999999994em;"><span></span></span></span></span></span></span></span></span></span>，聚合并广播到所有机器。 当然，这可以通过标准的 Allreduce 通信模式来实现。</p>
<p>当有很多worker 或一些潜在的落后者时，可以将上述算法扩展到其异步对应物。 我们允许访问一些过时的版本(staled version)，而不是在迭代 t 时使用最新的梯度。</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msubsup><mi>x</mi><mi>i</mi><mrow><mo>(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo>)</mo></mrow></msubsup><mo>=</mo><msubsup><mi>x</mi><mi>i</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msubsup><mi mathvariant="normal">−</mi><mi>γ</mi><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msubsup><mi>g</mi><mi>j</mi><mrow><mo>(</mo><msubsup><mi>t</mi><mi>j</mi><mi>i</mi></msubsup><mo>)</mo></mrow></msubsup></mrow><annotation encoding="application/x-tex">x^{(t+1)}_i = x^{(t)}_i − γ \sum^n_{j=1} g^{(t^i_j)}_j
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.321664em;vertical-align:-0.276864em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em;"><span style="top:-2.4231360000000004em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span><span style="top:-3.2198em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathdefault mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.276864em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:3.0651740000000007em;vertical-align:-1.4137769999999998em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em;"><span style="top:-2.4231360000000004em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span><span style="top:-3.2198em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathdefault mtight">t</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.276864em;"><span></span></span></span></span></span></span><span class="mord">−</span><span class="mord mathdefault" style="margin-right:0.05556em;">γ</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.6513970000000007em;"><span style="top:-1.872331em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.050005em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3000050000000005em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.4137769999999998em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.2736799999999997em;"><span style="top:-2.4231360000000004em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span><span style="top:-3.3422199999999997em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9020857142857143em;"><span style="top:-2.214em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span><span style="top:-2.931em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.42488571428571426em;"><span></span></span></span></span></span></span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.412972em;"><span></span></span></span></span></span></span></span></span></span></span></p>
<p>t 是计算worker j 梯度的前一次迭代，由worker i 在迭代 t 时访问。 理论上可以通过 异步SGD 实现线性加速。</p>
<h2 id="22-现有的分布式学习系统">2.2 现有的分布式学习系统</h2>
<p>分布式学习系统在过去十年中吸引了大量研究。 大多数现有系统，包括 DistBelief [63]、Adam [25]、SparkNet [64]、GeePS [65]、Poseidon [27]、Angel [28]、BytePS [29]、PyTorch-DDP [30]、Mariana [31] ]、MALT [32]、NCCL [33]、SINGA [66, 67]、Rafiki [68] 和 Horovod [24]，都专注于 DP-SG 算法或其异步对应算法的优化。 这些系统的设计基于两个基本问题：</p>
<ul>
<li>1.（抽象通信）应该如何通信和聚合梯度和模型？</li>
<li>2.（优化）如何通过平衡通信和计算来优化端到端执行？</li>
</ul>
<h3 id="通信抽象">通信抽象</h3>
<p>在通信抽象方面，现有系统分为两种范式：参数服务器（PS）和 Allreduce ，图 1 说明了这两种范式。 在参数服务器架构中，模型可以划分为分片并分布到多个节点（我们称这些节点为“参数服务器”）。 在训练阶段，worker 定期从 PS 中获取模型，利用 GPU 等计算单元进行前向和反向传播并将梯度推送到 PS，而 PS 聚合梯度并更新参数。使用 Allreduce 范式，所有worker 与他们的邻居合作进行模型/梯度交换。 现有系统通常采用环形拓扑 <a href="https://link.springer.com/chapter/10.1007/978-3-540-24685-5_1">Optimization of Collective Reduction Operations</a> 进行两阶段通信：首先，范式将模型/梯度划分为 n 个块（其中 n 是节点数），并使用 n 个具有不同起点和终点的环 指向聚合 n 个块； 其次，通过环广播位于不同节点的每个块的聚合结果。</p>
<figure data-type="image" tabindex="1"><img src="https://DragonFive.github.io//post-images/1630754817474.png" alt="" loading="lazy"></figure>
<figure data-type="image" tabindex="2"><img src="https://DragonFive.github.io//post-images/1630754865306.png" alt="" loading="lazy"></figure>
<p><em>图 2：DP-SG 的通信模式以及 Horovod、BytePS 和 PyTorch-DDP 如何优化此通信模式的执行</em></p>
<blockquote>
<p>点评：这个图需要关注一下</p>
</blockquote>
<p>用于两阶段通信的现有系统：首先，范式将模型/梯度划分为 n 个块（其中 n 是节点数），并使用具有不同起点和终点的 n 个环来聚合 n 个块； 其次，通过环广播位于不同节点的每个块的聚合结果。</p>
<h3 id="性能优化">性能优化</h3>
<p>在决定使用哪种通信范式之后，一个关键的设计是如何在计算过程中隐藏尽可能多的通信。 这通常是以前系统的核心技术组件。Horovod、BytePS 和 PyTorch-DDP。 这些系统通过开发不同的方法来平衡通信和计算来优化 DP-SG 通信模式。 关键的复杂性源于这样一个事实，即 DP-SG 的训练过程由不同层之间的微妙依赖及其自身的 (1) 前向传递、(2) 反向传递、(3) 梯度同步和 (4) 模型更新四个阶段组成 。</p>
<p>图 2（Vanilla）说明了 DP-SG 在四层模型上的简单实现。 一旦向后传递（蓝色）完成，系统就会为每一层传达梯度（绿色），并在所有层（粉色）完成所有通信后一次性更新所有层的模型。 然后系统开始下一次向前传递（黄色）。</p>
<p>PyTorch-DDP 和 Horovod 是两个基于 Allreduce 的系统，<strong>并通过将通信 (Allreduce) 与反向传递重叠</strong>并将多个<strong>梯度分桶为一个 Allreduce 操作来专门优化此流水线</strong>。 通过重叠，Allreduce 操作可以与梯度计算并行进行。 Allreduce 操作仅在存储桶中的所有梯度都准备就绪时触发。 分桶的直觉是，像 Allreduce 这样的集体通信在大张量上更有效。 所有的 Allreduce 操作完成后，模型将通过聚合梯度进行更新。BytePS 遵循参数服务器范例，以不同的方式优化了这个管道。</p>
<p><strong>BytePS 将每个梯度分割成大小相同的小块来进行 Push/Pull</strong>。 BytePS 重叠向前和向后推/拉。 它有一个调度器来维护梯度块的通信顺序。 其原理是，阻塞下一次前向传递执行的参数将优先进行通信。 一旦从服务器中提取了参数的所有梯度块，该参数将单独更新。 因此，下一次迭代的前向传递可能会与当前迭代的通信重叠。 在异步 DP-SG 方面，BytePS 通过允许每个worker单独更新服务器状态而不等待其他worker 来支持它。 而 PyTorch-DDP 和 Horovod 不支持异步通信，因为它们依赖于 Allreduce 运算符。</p>
<h2 id="23-分布式-dp-sg-的系统放松">2.3 分布式 DP-SG 的系统放松</h2>
<p>虽然现有系统主要关注同步和异步 DP-SG 算法，但研究界已经开发出一套多样化的技术来进一步优化通信的不同方面。 这些技术通常会导致不同的训练算法，从而导致不同的通信模式，如 DP-SG。鉴于这些差异，Horovod、BytePS 和 PyTorch-DDP 都没有提供对这些算法的系统支持，如表 1 所述。BAGUA 的目标是提供灵活的抽象来支持这些具有自动性能优化框架的不同训练算法，而无需 假设特定的通信模式，例如 DP-SG 之一。 提出了不同的策略来加速 DP-SG 中昂贵的参数交换阶段。 为了减少通信量，引入了有损通信压缩方法，如量化，稀疏化，素描（sketching）和误差补偿 . 为了摆脱延迟瓶颈，人们提出了分散式通信方法。 此外，还讨论了 localSGD 以优化训练期间的通信轮数。最后，值得一提的是，有些方法结合了上面列出的多种策略。</p>
<p><strong>示例</strong><br>
为了说明这些高级训练算法与普通 DP-SG 之间的通信模式的差异，以及仅考虑使用 DP-SG 进行优化的系统在以模块化和系统的方式支持这些新算法方面面临挑战的原因，我们取 QSGD 和去中心化低精度 SGD 的示例。 图 3 说明了 DP-SG、QSGD 和分散的低精度 SGD 的执行管道和通信模式。 与DP-SG相比，流水线的执行组件及其依赖关系可以在高级算法中改变。例如，DP-SG 中甚至不存在两种算法所需的组件“量化”，而去中心化低精度 SGD 中的“模型更新”组件需要在通信之前发生。 由于这些高级算法无法适应 DP-SG 通信模式，因此为 DP-SG 而生的系这些统处理这些算法具有挑战性。</p>
<figure data-type="image" tabindex="3"><img src="https://DragonFive.github.io//post-images/1630755592348.png" alt="" loading="lazy"></figure>
<p><em>图 3：BAGUA 自动优化的系统松弛训练算法的通信模式</em></p>
<blockquote>
<p>点评：</p>
</blockquote>
<h1 id="3-系统设计">3 系统设计</h1>
<p>BAGUA 的目标是提供一个灵活的框架来支持 DP-SG 之外的高级训练算法。 为了实现这一点，我们重新审视了控制以前系统设计的两个基本问题，而不假设 DP-SG 的模式：</p>
<ol>
<li>（抽象通信）应该如何通信和聚合梯度和模型？ 在 BAGUA 中，除了参数服务器和 Allreduce 之外，我们设计了一系列 MPI 风格的集体操作，以促进具有不同精度和中心化策略的通信。</li>
<li>（优化）如何通过平衡通信和计算来优化端到端执行？ 在 BAGUA 中，我们开发了一个简单但有效的自动优化框架，可用于优化在 BAGUA 中实现的算法的执行。</li>
</ol>
<p>这两个设计决策使 BAGUA 具有灵活性和效率——通过系统松弛实现新的高级算法（例如，1-big Adam 或去中心化 SGD ）在 BAGUA 中，开发人员无需担心手动平衡通信与计算； 相反，开发人员可以在高层指定逻辑语义，BAGUA 将自动优化其执行。 在本节中，我们首先提供高级系统概述，然后描述这些原语及其实现，然后是 BAGUA 中简单但有效的优化框架。</p>
<figure data-type="image" tabindex="4"><img src="https://DragonFive.github.io//post-images/1630755614240.png" alt="" loading="lazy"></figure>
<h2 id="31-系统总览">3.1 系统总览</h2>
<p>BAGUA 的目标是促进利用系统松弛的高效且可扩展的分布式训练算法的开发。 如图 4 所示，存在三个参与者：最终用户、优化算法和 BAGUA 运行时。</p>
<figure data-type="image" tabindex="5"><img src="https://DragonFive.github.io//post-images/1630755631672.png" alt="" loading="lazy"></figure>
<p><strong>终端用户</strong> 从最终用户的角度来看，使用 BAGUA 非常类似于在单台机器上使用 PyTorch 或 TensorFlow 进行训练，只需对其现有代码进行最少的更改。 最终用户应提供：（1）需要训练的神经网络模型，在 PyTorch 中指定为图形，以及（2）数据样本流。 然后，最终用户指定要使用的训练算法，例如 QSGD （通信压缩训练）、1 位 Adam 或 DecentralizedSGD，以及训练基础设施的信息，例如 机器数量以及是否应该使用 MPI 或 NCCL 进行通信。</p>
<p>BAGUA 的核心是一个训练算法，由开发人员使用 BAGUA 提供的通信原语和抽象实现。 算法将最终用户提供的神经网络作为输入，并为其配备特定于算法的通信功能。 具体来说，算法的开发者通过在不同的执行阶段将此通信函数注册为钩子来实现这一点。 一个例子是在每一层的反向计算之后注册一个钩子。 通信函数包含训练算法的核心逻辑，其签名如下：</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi><mo>(</mo><mo>(</mo><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>g</mi><mn>1</mn></msub><mo>)</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo>(</mo><msub><mi>x</mi><mi>n</mi></msub><mo separator="true">,</mo><msub><mi>g</mi><mi>n</mi></msub><mo>)</mo><mo>)</mo><mo>−</mo><mo>&gt;</mo><mo>(</mo><msubsup><mi>x</mi><mn>1</mn><mi>l</mi></msubsup><mo separator="true">,</mo><msubsup><mi>g</mi><mn>1</mn><mi>l</mi></msubsup><mo>)</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo>(</mo><msubsup><mi>x</mi><mi>n</mi><mi>l</mi></msubsup><mo separator="true">,</mo><msubsup><mi>g</mi><mi>n</mi><mi>l</mi></msubsup><mo>)</mo></mrow><annotation encoding="application/x-tex">f((x_1, g_1)...(x_n, g_n)) -&gt; (x^l_1 , g^l_1 )...(x^l_n , g^l_n )
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mclose">)</span><span class="mord">−</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.1491079999999998em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8991079999999998em;"><span style="top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8991079999999998em;"><span style="top:-2.4530000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8991079999999998em;"><span style="top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8991079999999998em;"><span style="top:-2.4530000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></p>
<p>其中 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><msub><mi>x</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>g</mi><mi>i</mi></msub><mo>)</mo></mrow><annotation encoding="application/x-tex">(x_i, g_i)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> 是第 i 台机器上的当前模型 (<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">x_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>) 和梯度 (<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>g</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">g_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>)， (<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msubsup><mi>x</mi><mi>i</mi><mi>l</mi></msubsup><mo separator="true">,</mo><msubsup><mi>g</mi><mi>i</mi><mi>l</mi></msubsup></mrow><annotation encoding="application/x-tex">x^l_i, g^l_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.107772em;vertical-align:-0.258664em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-2.441336em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.258664em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-2.441336em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.258664em;"><span></span></span></span></span></span></span></span></span></span>) 是第 i 台机器上的更新模型和梯度。为了实现通信功能，算法的开发者假设了一个类似 MPI 的执行模型。 关键的区别在于，开发者不仅配备了 MPI 中的标准通信原语（例如 Allreduce），还配备了 BAGUA 提供的一组通信原语。 这些原语支持系统放宽，例如带有错误补偿的压缩通信或分散式通信。</p>
<p>在 BAGUA 中实现通信功能时，开发者提供了这种功能的批处理版本，将一组层作为输入。 这允许 BAGUA 稍后自动批处理通信并优化其与计算的重叠。 当BAGUA调用该函数时，它会将所有层的参数重新排列到连续的内存空间中，并传入这些层的扁平版本，将它们视为单个变量。 算法开发者可以决定她的算法是否可以使用这个扁平化的版本，通过对所有层进行一次通信来避免对每一层进行通信。</p>
<p>BAGUA Runtime 对通信功能的每次调用（由注册的钩子触发）都向 BAGUA 注册，这为 BAGUA 提供了工作负载的全局视图，<strong>以实现自动调度和批处理。 BAGUA 的关键技术贡献是自动对计算和通信应用一系列优化</strong>。 为了实现这一点，BAGUA 的核心是执行优化器，它分两个阶段运行。</p>
<ol>
<li>分析阶段。 在梯度下降计算的第一次前向/后向传递期间，BAGUA 保留所有通信函数调用的日志，在没有任何优化的情况下执行它们。然后它会自动： (1. Bucketing) 将层分组到不同的桶中，它们的通信将同时发生； (2.Flattening)将同一组内所有层的所有模型和梯度重新排列到连续的内存空间中，以达到更好的局部性； (3. Scheduling) 调度何时进行每个桶的通信，与计算重叠。</li>
<li>执行阶段。 对于梯度下降计算的其余前向/后向传递，BAGUA 将在模型的自动优化版本上执行。 默认情况下，BAGUA 每个桶进行一次通信。</li>
</ol>
<blockquote>
<p>点评：这个自动优化应该是这篇论文为数不多的亮点。<br>
除了这个自动优化，其他就像是系统工程上的大杂烩</p>
</blockquote>
<h2 id="32-通信原语">3.2 通信原语</h2>
<p>BAGUA 的一个关键组件是一组通信原语。 所有这些算子都遵循类似于 MPI 的执行模型，将 n 个张量 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>x</mi><mn>1</mn></msub><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><msub><mi>x</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">x_1...x_n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>（可以存储参数、梯度等）作为输入，每个张量在不同的 worker 上，并输出新的数据产出 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msubsup><mi>x</mi><mn>1</mn><mi>l</mi></msubsup><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><msubsup><mi>x</mi><mi>n</mi><mi>l</mi></msubsup></mrow><annotation encoding="application/x-tex">x^l_1...x^l_n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.097216em;vertical-align:-0.24810799999999997em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-2.4518920000000004em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.24810799999999997em;"><span></span></span></span></span></span></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span></span></span></span> ，每个在不同的worker：</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>o</mi><mi>p</mi><mo>(</mo><msub><mi>x</mi><mn>1</mn></msub><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><msub><mi>x</mi><mi>n</mi></msub><mo>)</mo><mo>→</mo><mo>&gt;</mo><msubsup><mi>x</mi><mn>1</mn><mi>l</mi></msubsup><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><msubsup><mi>x</mi><mi>n</mi><mi>l</mi></msubsup></mrow><annotation encoding="application/x-tex">op(x_1...x_n) →&gt; x^l_1 ...x^l_n
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">o</span><span class="mord mathdefault">p</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">→</span></span><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.146108em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8991079999999998em;"><span style="top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8991079999999998em;"><span style="top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span></span></span></span></span></p>
<h3 id="集中式全精度">集中式全精度</h3>
<p>BAGUA 提供了一个简单的原语 C FP S，它提供与标准 Allreduce 运算符相同的功能。</p>
<p>我们使用这个符号来表示，CFPS 算子的作用是计算所有本地副本的总和 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mo>∑</mo><mi>j</mi></msub><msub><mi>x</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\sum_jx_j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.185818em;vertical-align:-0.43581800000000004em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.16195399999999993em;"><span style="top:-2.40029em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.43581800000000004em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> ，并使其可供所有worker 访问。</p>
<h3 id="集中式-低精度">集中式、低精度</h3>
<p>鉴于许多深度神经网络可以容忍其梯度的激进有损压缩，最近通信压缩引起了广泛的兴趣。BAGUA 为此提供了 CLPS 原语。 具体来说：</p>
<figure data-type="image" tabindex="6"><img src="https://DragonFive.github.io//post-images/1630755670795.png" alt="" loading="lazy"></figure>
<p>其中 Q 是有损压缩函数，由开发人员指定，CLPS 支持带有错误补偿的通用通信压缩形式。 请注意，将 δi 和 i 设置为 None 将禁用误差补偿并给出。直观上，δi 和 i 保留了上次迭代压缩引起的误差。 误差补偿方法引入的收敛效率对压缩非常稳健。 当压缩功能相对激进时，此技术特别有用。</p>
<h3 id="去中心化-全精度">去中心化、全精度</h3>
<p>BAGUA 还支持去中心化通信，消除了模型同步的延迟瓶颈——而不是在集群中的所有 n 个 worker 之间同步，每个 worker <strong>只将更新发送给它的邻居</strong>。 例如，根据<strong>基于环的拓扑</strong>，工人的邻居包括环中的紧邻左worker和紧邻右worker。 形式上，BAGUA 的去中心化全精度通信原语 DFPS 可以形式化如下：</p>
<blockquote>
<p>点评：就是ring allreduce 呀</p>
</blockquote>
<figure data-type="image" tabindex="7"><img src="https://DragonFive.github.io//post-images/1630755688224.png" alt="" loading="lazy"></figure>
<h3 id="去中心化-低精度">去中心化、低精度</h3>
<p>BAGUA 还为去中心化低精度通信提供原始 DLPS</p>
<h3 id="讨论支持异步算法">讨论：支持异步算法</h3>
<p>当前版本的 BAGUA 没有提供这些原语的任何异步版本，而是支持使用这些同步原语的异步算法，如下所示。 一个算法可以实现两个并发线程，一个处理计算，另一个处理通信。 这两个线程不会互相等待。 这提供了许多异步算法的实现。总结在表 1 中。它还可以实现 LocalSGD 和模型平均。 进一步探索提供异步版本的原语的好处很有趣，我们将其留作未来的工作。</p>
<h2 id="33-原语的实现">3.3 原语的实现</h2>
<h3 id="集中原语">集中原语</h3>
<p>BAGUA 使用 ScatterReduce 通信模式运行集中原语。 特别地，目标张量被分成n个分区，其中n是worker的数量。 第 i 个工作器负责聚合第 i 个分区。 由于底层通信库 NCCL 不提供 ScatterReduce 原语，我们使用基本的 send 和 recv NCCL 操作符来实现这个原语。</p>
<blockquote>
<p>点评，在nccl 2 里面提供了 <a href="https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/api/colls.html#c.ncclReduceScatter">reducescatter</a>，不太确定底层通信库 NCCL 不提供 ScatterReduce 原语指的是什么</p>
</blockquote>
<p>每个worker 1) 对本地张量进行分区，2) 将分区发送给相应的worker，3) 从其他worker 接收负责的分区，4) 合并收到的分区，以及5) 将合并的分区发送给其他worker。</p>
<blockquote>
<p>点评：这个意思是中心化是worker 当ps用了？</p>
</blockquote>
<p>ScatterReduce 通信模式可以利用所有 worker（如 Allreduce）的聚合带宽，并支持压缩技术（与 Allreduce 不同）。 低精度原语 CLPS 利用 ScatterReduce 通信来合并两个压缩阶段。每个工作人员在发送之前只压缩本地张量的分区（阶段 1）和合并的分区（阶段 2）。 请注意，压缩和解压缩过程可以与错误补偿技术相结合可以减少信息丢失（参见第 3.2 节中的语义）。</p>
<h3 id="分散原语">分散原语</h3>
<p>与所有 worker 都参与交流的集中式培训不同，分散式培训中的每个worker只与一个或几个同伴进行交流。 BAGUA 设计了两种机制来分配对等点——环和随机。 环形策略为worker提供连续的等级并将所有worker组织成一个环。rank-i worker 只与两个相邻的 peer 通信，rank-(i − 1) 和 rank-(i + 1)。 或者，随机策略为每个worker随机选择一个对等点。 当通信对等点被分配时，每个worker将本地张量发送给对等点，从对等点接收张量，并计算它们的平均值。 低精度原语 D L P S 使用与 D FP S 相同的对等点选择和通信过程。不同之处在于 D L P S 在发送前使用压缩函数 Q 压缩张量并在接收后解压张量。</p>
<blockquote>
<p>点评：如果分散式指的是仅用两个worker 的信息进行参数更新，这需要首先证明是有效的。</p>
</blockquote>
<h2 id="34-bagua优化框架">3.4 bagua优化框架</h2>
<p>BAGUA 的核心组件是它的执行优化器。 给定一个神经网络作为输入，训练算法（例如，QSGD）将在每一层的计算过程中利用一系列通信原语。 BAGUA 执行优化器的目标是自动调度和优化这些计算和通信。 我们在 BAGUA 中探索了以下技术。</p>
<h3 id="重叠通信和计算">重叠通信和计算</h3>
<p>重叠通信和计算是加速分布式 DP-SG 的一项核心优化。不限于 DP-SG 算法，BAGUA 能够以灵活和自动的方式重叠通信原语以及其他算法的计算。 BAGUA 自动分析包含就地张量操作和张量通信原语的计算图。尽管可以通过静态分析构建此图，但 BAGUA 决定利用动态分析方法，在第一次迭代中收集张量操作和通信原语的调用依赖性。与现有系统相比，BAGUA 考虑了更复杂的调度。在vanilla DP-SG中，优化只能通过逆序将Allreduce通信隐藏在反向传播的计算中；相比之下，BAGUA 负责调度额外的元素，例如低精度压缩/解压缩和优化算法指定的模型更新计算。</p>
<blockquote>
<p>点评：但如果没有低精度需求，是不是就跟现有的方法一样了？通过scatterreduce 合并压缩解压。vanilla 这里需要关注一下。</p>
</blockquote>
<h3 id="张量分桶和内存扁平化">张量分桶和内存扁平化</h3>
<p>为了有效地将通信与计算重叠，将层划分为桶是必不可少的步骤——频繁调用通信范式来传输小片段参数对于充分利用网络带宽而言远非理想。 因此，Horovod 和 PyTorch-DDP 都采用了分桶技巧。 然而，他们的分桶模式可以简单地将 Allreduce 通信硬编码为启发式中的成本，并使用<strong>神经网络中层的相反顺序来确定桶</strong>。</p>
<p>相比之下，由于 BAGUA 支持更多由优化算法指定并由 BAGUA 的通信原语形式化的通信方式，因此根据<strong>分析阶段收集的依赖关系确定分桶</strong>。 一旦我们将计算图拆分成桶，BAGUA 就会对桶进行融合。 这使 BAGUA 可以实现更有效的流水线，尤其是在包含低精度系统松弛时</p>
<blockquote>
<p>点评：所以关键在于分桶方式，分析阶段的依赖关系确定是重点</p>
</blockquote>
<p>在第一次反向传播中确定桶的分区后，BAGUA 会仔细地将<strong>桶内的参数（例如模型参数、梯度和优化器状态）对齐到连续的内存空间中</strong>。 然后，这种扁平化的参数视图被用于所有流水线执行。 例如，低精度压缩/解压 lambda 直接应用于桶的扁平视图而不是单个参数； 用于模型更新的基于 SG 的优化器也在存储桶级别进行（来自 NVIDIA 的 Apex 也使用了类似的优化）。 请注意，这种<strong>扁平化视图可以更有效地利用计算单元提供的并行性</strong>。</p>
<blockquote>
<p>点评：看来扁平化优化的也是通信时候的操作。扁平化在取计算的时候是否会拖慢计算的速度呢，为什么可以有效利用计算的并行性呢，是说一边计算一边通信吗</p>
</blockquote>
<h3 id="分层通信">分层通信</h3>
<p>BAGUA 的通信可以分层进行。 这在处理网络连接的异构性时特别有用，例如，服务器内 GPU 之间的带宽远高于服务器之间的带宽。 因此，BAGUA <strong>以节点内和节点间</strong>两个层次进行分层通信，并基于此抽象优化通信原语的实现。</p>
<p>例如，集中式低精度原语（CLPS）可以优化为首先在没有压缩的情况下在每个节点内部的本地 worker上聚合张量，然后对从每个节点选出的leader worker进行压缩进行节点间聚合，最后 让每个leader worker在节点内广播聚合数据。</p>
<blockquote>
<p>点评：那么分层和leader worker 的好处在哪里呢</p>
</blockquote>
<p>请注意，这种优化可能会改变通信原语的语义。 对于去中心化原语，节点内的工作人员将始终更改为中心化的 Allreduce 方式。</p>
<blockquote>
<p>点评：所以去中心化就不能用这个分层优化呗，但这个算是什么优化呢</p>
</blockquote>
<h1 id="4-评估">4. 评估</h1>
<p>我们围绕三个假设进行了广泛的实验研究：</p>
<ul>
<li>与最先进的系统相比，BAGUA 能够在以下方面提供显着的性能改进<br>
端到端的培训时间和可扩展性，超过现实的工业规模基础设施。</li>
<li>BAGUA 支持的不同算法为不同网络条件下的不同模型和数据集提供了好处。 因此， BAGUA 支持所有这些算法非常重要。</li>
<li>BAGUA 的自动执行优化器有效优化了各种分布式训练算法的执行？</li>
</ul>
<h2 id="44-系统优化的消融研究">4.4 系统优化的消融研究</h2>
<p>我们现在验证 BAGUA 优化框架的有效性。 如 3.4 节所述，优化框架由三个优化组成： <strong>O：训练计算和 BAGUA 执行之间的重叠； F：张量的融合和扁平化。 H：分层通信</strong></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[华为的《ScaleFreeCTR:a MixCache-based distributed training system for CTR》]]></title>
        <id>https://DragonFive.github.io/post/hua-wei-de-scalefreectr/</id>
        <link href="https://DragonFive.github.io/post/hua-wei-de-scalefreectr/">
        </link>
        <updated>2021-06-15T12:23:03.000Z</updated>
        <content type="html"><![CDATA[<p>华为诺亚方舟提出了SFCTR <a href="https://arxiv.org/pdf/2104.08542.pdf">ScaleFreeCTR: a MixCache-based distributed training system for CTR</a>，scalefree 可能是因为数据规模对训练吞吐没有影响，后面实验部分有具体的数据。</p>
<h1 id="一-动机与主要创新">一、动机与主要创新</h1>
<p>现有的分布式CTR训练框架使用CPU内存来保存和更新参数，使用gpu 进行前向和反向计算（也有用CPU的），会有两个瓶颈</p>
<ol>
<li>
<p>CPU 和 gpu 之间的pull 和 push 操作有一定的延迟</p>
</li>
<li>
<p>cpu 进行参数的同步和更新比较慢</p>
</li>
</ol>
<p>分布式训练的关键在于</p>
<ol>
<li>关键在于减少host_gpu之间的延迟</li>
<li>减少host-gpu 及gpu之间的数据传输量也很重要</li>
</ol>
<p>推荐中的参数有两个特点：</p>
<ol>
<li>实际 working parameters 比较少，sparse 参数和 MLP参数都很少</li>
<li>sparse 特征符合幂律分布，小部分特征被高频访问</li>
</ol>
<p>根据两个特点，可以有两个方法</p>
<ol>
<li>使用缓存机制减少 host-gpu 延迟</li>
<li>通过重组batch数据来减少参数传输量(unique?)</li>
</ol>
<p>由此提出了 SFCTR:<br>
在CPU中通过 虚拟sparse id op 来减少host-gpu 和gpu-gpu 的数据传输量，使用 mixcache 实验特征预取来减少传输延迟，使用3级pipeline 来减少整体训练时长。</p>
<p>系统将会在MindSpore 上开源，现在看似乎还没有开源。</p>
<h1 id="二-相关工作">二、相关工作</h1>
<h2 id="一论文介绍的跟sfctr无关但挺有用的经验知识">（一）论文介绍的跟SFCTR无关，但挺有用的经验知识</h2>
<p>为了提高训练效率，有两种通用的做法：</p>
<ol>
<li>增量学习（batch训练的补充，用最近的数据更新模型）</li>
<li>分布式训练（使用额外的训练资源）</li>
</ol>
<p>CTR模型稀疏部分参数量太大，所以不能使用reduce 数据并行，大多数考虑用了模型并行。<br>
模型并行解决方案ps 架构的局限性：<br>
Ps server 保存并同步参数，worker执行前向和反向计算，</p>
<ul>
<li>worker pull and push from ps</li>
<li>Ps 从worker接收梯度之后进行同步<br>
分布式训练包括两个阶段：计算和参数同步。</li>
</ul>
<p>百度的综述 <a href="https://arxiv.org/abs/2003.05622">Distributed Hierarchical GPU Parameter Server for Massive Scale Deep Learning Ads Systems</a> 介绍了三种同步模式：</p>
<ul>
<li>BSP(bulk sync parallel)，严格对所有worker的更新进行同步</li>
<li>SSP(stale sync parallel)，对快worker 进行同步</li>
<li>ASP（async parallel）, 不同步gradient</li>
</ul>
<p>后两种方式虽然提升了训练效率，但是降低了模型性能。SFCTR 使用的是BSP，XDL使用的是ASP。</p>
<h1 id="三-sfctr-架构">三、SFCTR 架构</h1>
<p>SFCTR 由三部分构成</p>
<ul>
<li>Data-Loader, 提出虚拟sparse id op 来减少batch中重复的特征emb(unique?)</li>
<li>Host-Manager, 使用混合缓存策略来减少host-gpu延迟，MixCache 的管理器部分在 CPU 的内存中，MixCache 的缓冲区在 GPU 的 HBM 中</li>
<li>GPU-WORKER<br>
<img src="https://DragonFive.github.io//post-images/1625142600332.png" alt="" loading="lazy"></li>
</ul>
<p><strong>3级pipeline</strong><br>
把 Data-Loader,Host-Manager 和gpu worker 中，三阶段资源不同 Disk, CPU and GPU在三个不同的线程里完成</p>
<figure data-type="image" tabindex="1"><img src="https://DragonFive.github.io//post-images/1625142730082.png" alt="" loading="lazy"></figure>
<h2 id="1data-loader">（1）data loader</h2>
<p>sparse id op 来减少batch中重复的特征emb，就是xdl中的unique。减少host-gpu 和gpu-gpu 的数据传输量。</p>
<h2 id="2host-manager">（2）HOST-MANAGER</h2>
<p>MixCache用来减少延迟，在每个GPU的HBM 上申请一个cache buffer，使用modulo 哈希方法对 working parameters 进行分组，放在不同的GPU 上，embedding参数在data loader执行完VSI op 之后检查哪些参数 gpu 上没有就把那些参数传输到gpu 上。</p>
<p>当cache满了之后，满足两种情况的emb 会回传到host。</p>
<ol>
<li>参数完成了更新</li>
<li>下个batch不需要这个参数</li>
</ol>
<h2 id="3gpu-worker">（3）GPU-WORKER</h2>
<p>不同的GPU保存了不同的参数，所以前向和反向的时候都需要同步。<br>
前向传播，每个worekr从其它worker拿到batch所需要的参数，使用all-reduce 通信方式，一个gpu只需要跟另外两个gpu通信两次，首先通过gather_cache ，从cache buffer 中获得local common emb, 因为global_id 顺序一致，所以可以做all_reduce同步, 然后通过all_reduce 或者 global common emb，最后通过vis 算出来自己worker需要执行的batch emb<br>
<img src="https://DragonFive.github.io//post-images/1625142803471.png" alt="" loading="lazy"></p>
<p>梯度更新<br>
<img src="https://DragonFive.github.io//post-images/1625142812224.png" alt="" loading="lazy"></p>
<h1 id="四-sfctr-执行流程">四、SFCTR 执行流程</h1>
<p>执行流程<br>
<img src="https://DragonFive.github.io//post-images/1625142836322.png" alt="" loading="lazy"></p>
<ul>
<li>
<p>2-3行是 data loader 部分，有个虚拟Sparse Id OP，对batch Sparse ID 去重后形成 global_id，对于batch 中每个实例有个virtual_id，可以找到其对应的global_id ，跟XDL 的unique 操作很像。使用global_id, 各个gpu在同步的时候数据量就会少很多。</p>
</li>
<li>
<p>4-7行是 Host-Manager 部分，负责在主存中报错embedding参数（存得下吗？），使用mixcache把working parameters 放到 gpu 的cache buffer中。mixcache 还更新gpu cache buffer， 检查下一个batch需要哪些embedding ，预测哪些embedding未来一段时间不需要，在buffer满的时候进行pull, 发送数据到gpu，并对每个特征在gpu设置一个local_id (?)</p>
</li>
<li>
<p>9-15行是GPU部分，包括embedding查表，前向反向和参数更新</p>
</li>
</ul>
<p>host 和 gpu 是生产者与消费者模式</p>
<h1 id="五-一些对我们有用的实验">五、一些对我们有用的实验：</h1>
<h2 id="1环境">（1）环境</h2>
<p>GPU 集群使用 InfiniBand 连接，4台GPU服务器通过100Gb  RDMA提速</p>
<p>Intel Xeon Gold-5118 CPUs with 18 cores (36 threads), 8 Tesla V100 GPUs with 32 GB HBM，1GB内存。GPU之间PCI连接<br>
使用 Criteo-TB 数据库，使用filter构造10GB和100GB两个数据集，因为包括了优化器的信息，33×4B×80=10GB，所以实际parameter table是embedding table的3倍，所以实际上是 30GB和300GB的模型参数。</p>
<p>使用 DeepFM 模型，与 hugectr与ps mxnet对比</p>
<h2 id="2框架对比实验">（2）框架对比实验</h2>
<p>基于VSI OP，混合缓存机制，和三级pipeline，在10GB数据上SFCTR 在4机32卡上的吞吐量是psmxnet的1.6倍，hugectr的5.6倍，100GB 数据上是1.3倍和6.9倍<br>
如果GPU卡只有8个，hugectr在100GB数据上根本无法训练</p>
<figure data-type="image" tabindex="2"><img src="https://DragonFive.github.io//post-images/1625143001706.png" alt="" loading="lazy"></figure>
<h2 id="3vsi-op">（3）vsi op</h2>
<p>Host-gpu 数据传输量减少 94% ，g pu-gpu数据量减少88%</p>
<figure data-type="image" tabindex="3"><img src="https://DragonFive.github.io//post-images/1625143009606.png" alt="" loading="lazy"></figure>
<h2 id="4mixcache">（4）mixcache</h2>
<p>Cache 大小对传数据的影响<br>
2GB的cache可以把数据传输推迟到1000步之后<br>
如果cache 大小比较大，batch中要传输的数据的比例就会小，因为可以存更多高频特征<br>
12%（2GB）, 27%（0.5GB） and 29%（0.25GB）</p>
<figure data-type="image" tabindex="4"><img src="https://DragonFive.github.io//post-images/1625143020154.png" alt="" loading="lazy"></figure>
<h2 id="53级pipeline">（5）3级pipeline</h2>
<p>GPU-Worker 训练时间在pipeline中占比最高<br>
一个节点跑100GB数据，使用pipeline需要 75 s，不用pipeline就需要150s</p>
<h1 id="六-总结与思考">六、总结与思考</h1>
<p>文章写的通俗易懂，很有条理，related worker 也总结了很多训练的经验，工作很有实用性。<br>
作者提到未来的工作有两个方向</p>
<ol>
<li>提升通信效率，（使用all2all）</li>
<li>调查提升收敛速度的方法</li>
</ol>
<p>思考借鉴意义</p>
<ol>
<li>
<p>SFCTR相当于把图完全放在GPU中执行，没有进行图的分隔，所以实现起来更容易一些。CPU只是一个ps的存储和更新后落盘以及dataloader</p>
</li>
<li>
<p>CPU内存1T，而实验中的数据最大的为300GB，所以可以放在CPU内存中，其实我们的模型大小似乎也在几百GB，如果可以放在worker内存中，就没有必要单独弄一个ps server；<br>
如果模型超过1T，也可以融合AIBox的做法，使用SSD做cpu mem的缓存</p>
</li>
<li>
<p>pipeline 和 vsiop 其实 XDL 都有，只缺了缓存机制，但XDL如果不动ps这一块，参数的更新其实是在ps 上完成的，所以 ps 的 push 也会继续有延迟，参数预取只能解决pull 的问题，</p>
</li>
<li>
<p>但如果都在本地更新，那不同worker之间参数同步就会比较麻烦，所以缓存预取、更新后缓存失效再回传的机制必然依赖多机间 RDMA  单机allreduce 同步通信技术，ps存在的意义不大</p>
</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[业界CTR深度学习框架的一些新的进展 ]]></title>
        <id>https://DragonFive.github.io/post/ye-jie-ctr-shen-du-xue-xi-kuang-jia-de-yi-xie-xin-de-jin-zhan/</id>
        <link href="https://DragonFive.github.io/post/ye-jie-ctr-shen-du-xue-xi-kuang-jia-de-yi-xie-xin-de-jin-zhan/">
        </link>
        <updated>2021-03-31T13:03:49.000Z</updated>
        <content type="html"><![CDATA[<p>为了充分利用GPU的能力和高速带宽<br>
英伟达的 hugeCtr https://github.com/NVIDIA/HugeCTR 和 脸书 的 DLRM<br>
<a href="https://arxiv.org/abs/1906.00091">【CoRR2019】Deep Learning Recommendation Model for Personalization and Recommendation Systems</a><br>
把emb参数分成不同的份放在GPU HMB中，需要需要昂贵的GPU，不实用。</p>
<p>腾讯的DES<br>
<a href="https://arxiv.org/abs/1909.04823">Distributed Equivalent Substitution Training for Large-Scale Recommender Systems</a><br>
和 百度的 HierPs<br>
<a href="https://arxiv.org/abs/2003.05622">Distributed Hierarchical GPU Parameter Server for Massive Scale Deep Learning Ads System</a><br>
使用主存保存emb ，DES采用 field-aware 分片策略来reduce(减少or规约)GPU间的数据通信，但没有进行主存和GPU之间通信优化。<br>
HierPS使用大batch策略来在gpu中缓存使用大参数，以此减少传输延时。</p>
<p>Tensorflow, MxNet 和 PyTorch 并不能很好的支持大规模embedding的训练:</p>
<ul>
<li>PyTorch 中没有官方支持的ps</li>
<li>Mxnet 支持的模型大小因为实现问题受到了限制</li>
<li>tensorflow 使用它的ps后吞吐会严重下降</li>
</ul>
<p>为了提升tensorflow, mxnet, pytorch较差的分布式性能，uber 的horovod<br>
<a href="https://arxiv.org/abs/1802.05799">Horovod: fast and easy distributed deeplearninginTensorFlow</a><br>
和字节的byteps<br>
<a href="https://arxiv.org/abs/1807.00311">Product-based Neural Networks for User Response Prediction over Multi-field Categorical Data</a><br>
都支持不同的平台:</p>
<ul>
<li>horovod 使用Ring-AllReduce实现来加速dense模型的训练</li>
<li>byteps 通过调度优先来在不同的层加速同步参数，优化不同层的顺序来在反向传播和前向计算的时候同步参数</li>
</ul>
<p>之前写过一篇关于 horovod 的知识总结：<a href="https://dragonfive.github.io/post/uber-de-horovod/">uber的Horovod | dragon</a></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[byteps 论文翻译与赏析]]></title>
        <id>https://DragonFive.github.io/post/byteps-lun-wen-fan-yi-yu-shang-xi/</id>
        <link href="https://DragonFive.github.io/post/byteps-lun-wen-fan-yi-yu-shang-xi/">
        </link>
        <updated>2021-02-17T06:37:52.000Z</updated>
        <summary type="html"><![CDATA[<p>byteps 代码开源在 <a href="https://github.com/bytedance/byteps">byteps repo</a>.<br>
论文地址 <a href="https://www.usenix.org/conference/osdi20/presentation/jiang">A Unified Architecture for Accelerating Distributed DNN Training in Heterogeneous GPU/CPU Clusters | USENIX</a></p>
<p>《用于在异构 GPU/CPU 集群中加速分布式 DNN 训练的统一架构》</p>
<h1 id="摘要">摘要</h1>
<p>运行 DNN 训练作业的数据中心集群本质上是异构的。 他们拥有用于计算的 GPU 和 CPU 以及用于分布式训练的网络带宽。 然而，现有的分布式 DNN 训练架构，all-reduce 和参数服务器 (PS)，无法充分利用这种异构资源。 在本文中，我们提出了一种新的分布式 DNN 训练架构，称为 BytePS。 BytePS 可以利用集群中的空闲 CPU 和带宽资源来加速在 GPU 上运行的分布式 DNN 训练任务。它提供了一个被证明是最优和统一的通信框架——现有的 all-reduce 和 PS 成为 BytePS 的两个特例。 为了在实践中实现已证明的最优性，BytePS 进一步拆分了参数优化器的功能。 它引入了用于聚合梯度的<strong>summation服务</strong>抽象，这对于所有优化器来说都是通用的。</p>
<blockquote>
<p>点评：主要的优化在于通信上。</p>
</blockquote>
<p><strong>summation服务</strong>可以通过AVX指令加速，可以在CPU上高效运行，而DNN模型相关的优化器算法在GPU上运行，进行计算加速。BytePS 可以加速主要框架的 DNN 训练，包括 TensorFlow、PyTorch 和 MXNet。 对于具有多达 256 个 GPU 的代表性 DNN 训练任务，BytePS 的性能分别比最先进的开源 all-reduce 和 PS 高出 84% 和 245%。</p>
<h1 id="1-介绍">1. 介绍</h1>
<p>近年来，深度神经网络 (DNN) 的研究经历了复兴。 DNN 为计算机视觉 、语音识别和合成 、自然语言处理 (NLP) 和许多其他领域带来了突破。 训练这些 DNN 模型通常需要大量的算术计算资源。 因此，首选 GPU。 为了运行许多此类任务并实现高资源利用率，引入了具有数千个或更多 GPU 的大型 GPU 集群。</p>
<p>这样的GPU集群不仅有GPU，还有CPU和高速网络。 GPU 机器通常也有高端 CPU。 也可能有仅 CPU 的机器用于训练数据的预处理和生成，比如在强化学习里面。这些 GPU/CPU 机器通过高速以太网或 Infiniband 网络连接，以方便分布式训练。 根据我们操作生产 GPU 集群的经验（第 3.1 节）和其他人的最新文献，通常可以更好地利用 GPU，而通常有空闲的 CPU 和带宽资源。</p>
<p>有两个主要的分布式训练架构系列，all-reduce 和参数服务器 (PS) 。 它们都基于数据并行性 。 在使用 all-reduce 的任务中，只涉及 GPU 机器。 在一次迭代中，GPU 独立计算模型参数的梯度，然后使用 all-reduce 原语聚合梯度。 在 PS 任务中，GPU 机器和 CPU 机器都可以使用。 与 all-reduce 不同的是，梯度被发送到 PS，它通常在 CPU 机器上运行并聚合接收到的梯度。然后  PS 运行某些 DNN 训练优化器，例如 SGD  或 Adam 并发回更新后的模型。 对于 all-reduce 和 PS，每次迭代都会发生上述情况，直到训练结束。</p>
<p>All-reduce 和 PS 在理论和实践上都大不相同。 给定一组没有额外 CPU 机器的 GPU 机器，经证明 all-reduce 是带宽最优的。 然而，随着 CPU 和带宽资源的增加，all-reduce 的最优性不再成立——我们发现，理论上，PS 可以通过利用额外的 CPU 机器来帮助 GPU 机器提供更好的性能。 这似乎是加速 DNN 训练的好机会，因为 GPU 集群确实有空闲的 CPU 和带宽资源。 不幸的是，在实践中，由于多种设计原因，所有现有 PS 的性能都很差，我们将在本文中很快看到。 因此看到分布式 DNN 训练速度记录以 all-reduce 为主也就不足为奇了</p>
<blockquote>
<p>点评：所有byteps 是对ps进行优化，利用好 cpu 资源</p>
</blockquote>
<p>因此，我们有动力设计 BytePS，这是一种在理论上和实践中都具有最佳通信性能的架构。 从根本上说，all-reduce 和 PS 理论上仅在非常特定的 GPU/CPU 设置中是最佳的，而对于更通用的设置则不是最佳的，例如，<strong>有一些有限的额外 CPU 资源。 通过仔细分配流量负载</strong>，BytePS 统一了 PS 或 all-reduce 理论上最优的情况，并将最优性推广到任何给定数量的具有<strong>不同 PCIe/NVLink 配置的 GPU/CPU 机器</strong>，并提供分析证明。</p>
<p>最重要的是，BytePS 通过消除现有 PS 设计中的瓶颈，将其实际性能推向接近理论极限。 对于快速的高速网络，我们发现 CPU 对于成熟的 DNN 优化器来说不够快。 我们引入了一个新的抽象，Summation Service，来解决这个问题。 我们将<strong>优化器拆分为梯度聚合和参数更新</strong>。 我们在 CPU 上运行的 <strong>Summation Service 中保持梯度聚合</strong>，并将计算密集度更高的参数更新移动到 GPU。 此外，在实施中，我们结合了先前工作中的<strong>流水线和优先级调度</strong>的想法，并解决了多个与 RDMA 相关的性能问题。</p>
<blockquote>
<p>点评：summation service 跑在cpu进行梯度聚合，因为这个是网络IO</p>
</blockquote>
<p>作为 all-reduce 和 PS 的直接替代品，BytePS 旨在在不改变 DNN 算法或其准确性的情况下加速分布式训练。 之前在 allreduce 和 PS 之上的工作，如张量压缩 ，可以直接应用于 BytePS。 我们的 BytePS 实现支持流行的 DNN 训练框架，包括 TensorFlow 、PyTorch  和 MXNet，以及类似 Horovod 的  API 和原生 API。</p>
<blockquote>
<p>点评：张量压缩的两篇论文也可以看一看[21, 45]</p>
</blockquote>
<p>本文做出以下贡献：</p>
<ul>
<li>我们为异构 GPU/CPU 集群设计了一种新的分布式 DNN 训练架构 BytePS。 借助集群中的备用 CPU 内核和网络带宽，BytePS 可以<strong>实现 DNN 训练加速的通信优化</strong>。 BytePS 提供了一个统一的框架，其中包括 all-reduce 和 PS 作为两种特殊情况。</li>
<li>我们进一步<strong>优化了机内通信</strong>。 我们解释了 GPU 机器中多样化和复杂的拓扑结构，并提出了最佳策略和原则。</li>
<li>我们提出了<strong>求和服务</strong>，它通过保持 CPU 中运行的梯度求和来加速 DNN 优化器，并将计算密集度更高的参数更新移动到 GPU。 这消除了原始 PS 设计中的 CPU 瓶颈</li>
</ul>
<p>作为主要的在线服务提供商，我们在内部部署了 BytePS，并将其广泛用于 DNN 训练。 我们在生产数据中心使用六个 DNN 模型和三个训练框架来评估 BytePS。 结果表明，使用 256 个 GPU，BytePS 始终优于现有的 allreduce 和 PS 解决方案，分别高达 84% 和 245%。 我们还发布了一个开源版本，吸引了成千上万的开源社区、几家顶级公司和多个研究小组的兴趣。</p>
<blockquote>
<p>点评：所以byteps 的主要发力点是在通信优化上</p>
</blockquote>
<h1 id="2-背景">2. 背景</h1>
<h2 id="21-分布式-dnn-训练">2.1 分布式 DNN 训练</h2>
<p>DNN 模型由许多参数组成。 DNN 训练包括三个主要步骤：（1）前向传播（FP），它接收一批训练数据，通过 DNN 模型进行传播，并计算损失函数； （2）反向传播（BP），它使用损失值来计算每个参数的梯度； (3) 参数更新，它使用聚合梯度通过某个优化器（例如，SGD、Adam 等）更新参数。 训练一个 DNN 用以上三个步骤迭代地细化模型参数，直到损失函数达到其最小值。</p>
<p>最重要的是，用户可以选择运行分布式训练。 最流行的分布式 DNN 训练方法是数据并行性，它将数据集划分到多个分布式计算设备（通常是 GPU），而每个 GPU 拥有完整的 DNN 模型。 由于每个GPU输入的数据不同，BP生成的梯度也会不同。 因此，数据并行要求所有 GPU 在每次训练迭代期间同步。</p>
<p>在大型企业或公共云中，用户经常在共享的 GPU 集群中运行这些 DNN 训练任务。 这样的集群是由成百上千的 GPU 机器构建的，这些机器通过高速 RDMA 网络连接。 这些 GPU 机器通常具有多个 GPU、数十个 CPU 内核、数百 GB 的 DRAM 和一到几个 100Gb/s NIC。 这些集群同时运行许多训练作业，其中许多作业集中使用 GPU 而不是大量使用 CPU。 DNN 集群上的公共数据集表明 50% 的主机 CPU 利用率低于 30%。</p>
<p>对于分布式训练，有两大类数据并行方法，即 all-reduce 和 Parameter Server (PS)。 下面介绍all-reduce和PS，分析它们的通信开销。 我们假设我们有 n 个 GPU 机器用于数据并行训练工作。 DNN 模型大小为 M 字节。 网络带宽为 B。</p>
<h2 id="22-all-reduce">2.2 All-reduce</h2>
<p>起源于 HPC 社区，all-reduce 在 GPU 本地更新自己的参数之前以集体的方式聚合每个 GPU 的梯度。 在 all-reduce 中，不涉及额外的 CPU 机器。 Ring 是最流行的 all-reduce 算法。 All-reduce经过多年优化，大部分最先进的训练速度记录都是使用all-reduce实现的，包括经典的基于CNN的ImageNet任务，基于RNN的语言建模任务，以及 BERT 的预训练。</p>
<p>图 1 显示了三个节点的基于环的 all-reduce 的示例。 我们可以将 all-reduce 操作分解为 <strong>reduce-scatter 和 all-gather</strong>。 Reduce-scatter（图1（a））将整个M个字节分成n个部分，并使用n个具有不同起点和终点的环分别reduce n个部分。 每个节点将发送 (n-1)M/n 个流量，因为每个节点充当仅 1 个环的最后一个节点，因此发送 0，而对于其他 n1 个环中的每一个，它必须发送 M/n 个字节。</p>
<figure data-type="image" tabindex="1"><img src="https://tva1.sinaimg.cn/large/008i3skNgy1gv5klx1p25j60w00a8gmy02.jpg" alt="Pasted image 20210905173924" loading="lazy"></figure>
<p>接下来，all-gather 要求每个节点使用环向所有其他 (n-1) 节点广播其减少的部分。 最后，所有节点都具有完全减少的相同数据（图 1（c））。 与reduce-scatter 类似，每个节点也在此操作期间发送(n-1)M/n 个出口流量。</p>
<p>将这两个步骤加在一起，在 all-reduce 操作中，每个节点向（和从）网络发送（和接收）2(n-1)M/n 个流量。 对于 B 网络带宽，所需时间为 2(n-1)M/nB，这在具有统一链路带宽的拓扑中被证明是最佳的，假设没有额外的资源。</p>
<p>在具有非均匀链路带宽的分层拓扑中，最佳分层策略至少需要<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>2</mn><mo>(</mo><msup><mi>n</mi><mi>l</mi></msup><mo>−</mo><mn>1</mn><mo>)</mo><mi>M</mi><mi mathvariant="normal">/</mi><msup><mi>n</mi><mi>l</mi></msup><msup><mi>B</mi><mi>l</mi></msup></mrow><annotation encoding="application/x-tex">2(n^l-1)M/n^lB^l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.099108em;vertical-align:-0.25em;"></span><span class="mord">2</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">n</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.099108em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mclose">)</span><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="mord">/</span><span class="mord"><span class="mord mathdefault">n</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span></span></span></span></span></span></span></span> 通信时间。其中<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>B</mi><mi>l</mi></msup></mrow><annotation encoding="application/x-tex">B^l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.849108em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span></span></span></span></span></span></span></span> 是最慢的链路带宽，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>n</mi><mi>l</mi></msup></mrow><annotation encoding="application/x-tex">n^l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.849108em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault">n</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span></span></span></span></span></span></span></span> 是具有最慢链路的节点数。 在分布式 DNN 训练中，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>n</mi><mi>l</mi></msup></mrow><annotation encoding="application/x-tex">n^l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.849108em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault">n</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span></span></span></span></span></span></span></span>通常是 GPU 机器的数量，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>B</mi><mi>l</mi></msup></mrow><annotation encoding="application/x-tex">B^l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.849108em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span></span></span></span></span></span></span></span> 通常是每台机器的网络带宽。 为简单起见且不影响我们的分析，下面我们假设每台机器只有一个 GPU，并通过相同的网络带宽连接，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>n</mi><mo>=</mo><msup><mi>n</mi><mi>l</mi></msup><mo separator="true">,</mo><mi>B</mi><mo>=</mo><msup><mi>B</mi><mi>l</mi></msup></mrow><annotation encoding="application/x-tex">n=n^l, B=B^l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">n</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.043548em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault">n</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.849108em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span></span></span></span></span></span></span></span>。</p>
<p>All-reduce 无法利用额外的非工作节点，因为它是为同构设置而设计的。 接下来，我们将展示 2(n-1)M/nB 通信时间对于额外的 CPU 机器不再是最佳的。</p>
<h2 id="23-parameter-server-ps">2.3 Parameter Server (PS)</h2>
<p>PS 架构包含两个角色：worker 和 PS。 Worker 通常在 GPU 机器上运行，执行 FP 和 BP，并将梯度推送到 PS。 PS聚合来自不同worker的梯度并更新参数。 最后，worker 从 PS 中拉取最新的参数并开始下一次迭代。 根据我们的行业经验，PS 流程通常在 CPU 上运行，因为它们具有成本效益。 由于 GPU（和 GPU 内存）比 CPU 贵得多，我们希望 GPU 专注于计算密集度最高的任务，而不是存储模型参数。</p>
<p>PS有两种放置策略。 一种是非共置模式（图 2（a）），其中 PS 进程部署在专用 CPU 机器上，与 GPU 机器分开。 假设我们有 k 个 CPU 机器，DNN 模型将被分成 k 个部分并分别存储在 k 个机器上。 在每次迭代中，每个 GPU 工作人员必须发送 M 字节梯度并接收回 M 字节参数。 每台 CPU 机器必须从 GPU 工作人员处接收总共 nM/k 梯度并发送回 nM/k 参数。</p>
<p>假设 k = n，理论上 PS 将比 all-reduce 更快，如表 1 所述。实际上，PS 在这种设置下是最佳通信，因为 M 是每个 GPU 机器必须发送和接收的绝对下限。 但是，CPU 机器越少（k 越小），CPU 机器上的通信时间 nM/kB 就会增加，如果 k &lt;n/2，则变得比 all-reduce 慢。 GPU 机器的网络带宽将被利用不足，因为 CPU 机器将成为通信瓶颈。</p>
<blockquote>
<p>点评：k &lt; n/2, 通信时间就慢了</p>
</blockquote>
<p><em>表 1：每次训练迭代所需的理论通信时间。 n 是 GPU 机器的数量。 k 是附加 CPU 机器的数量。 M 是模型尺寸。 B 是网络带宽。 我们将重新审视最优</em>。<br>
<img src="https://tva1.sinaimg.cn/large/008i3skNgy1gv5kn3z2jcj60xg06agmo02.jpg" alt="Pasted image 20210905184807" style="zoom:80%;" /></p>
<p>另一种策略是共置模式（图 2（b）），它不使用任何 CPU 机器。 相反，它会在每个 GPU 工作线程上启动一个 PS 进程并重用其备用 CPU 资源。 同一台机器上的 PS 和 GPU 工作器将通过环回流量（loopback traffic）进行通信。 在这种情况下，很容易计算出通信时间与all-reduce相同（表1）。</p>
<blockquote>
<p>点评：这个共置模式的提法，之前没看到过</p>
</blockquote>
<h2 id="all-reduce-vs-ps">All-reduce vs. PS.</h2>
<p>他们有不同的通信模式。 PS使用二部图。 非共置模式的 PS 可以利用额外的 CPU 和带宽资源来帮助 GPU 机器，同时可能未充分利用 GPU 机器的资源。 Colocated PS 和 all-reduce 更好地利用了 GPU Worker 资源，同时不能使用额外的 CPU 机器。</p>
<p>另一个区别是 PS 支持异步训练，它允许 GPU worker 以不同的速度运行并减轻掉队者的影响，而 all-reduce 不支持它。 但是，异步训练不太受欢迎，因为它会减慢模型收敛速度。 我们将在本文中主要关注同步训练，同时在 §5 中简要介绍异步训练。</p>
<h1 id="3-动机和-byteps-架构">3. 动机和 BytePS 架构</h1>
<h2 id="31-动机">3.1 动机</h2>
<p>在我们内部 GPU 集群中部署 BytePS 之前，我们的用户大多使用 all-reduce 作为分布式训练架构，因为它比现有的 PS 设计具有更高的性能。 其余用户选择 PS 用于异步训练可接受或更可取的任务。 凭借多年在加速 DNN 任务和提高资源利用率方面的经验和努力，我们有以下观察。</p>
<h3 id="机会">机会：</h3>
<p><strong>生产 GPU 集群中有空闲的 CPU 和带宽</strong>。</p>
<p>大规模 GPU 集群同时运行大量作业，其中许多作业不会大量使用 CPU 或网络带宽。 图 3 显示了从我们拥有数千个 GPU 的 GPU 集群之一收集的 3 个月跟踪记录。 GPU在那个时期的利用率很高（高峰期接近96%的分配率）。 我们发现，至少有 55%-80% 的 GPU 机器被分配为 GPU worker，用于至少一项分布式训练任务。 这使得 20%-45% 的 GPU 机器的网络带宽未使用，因为它们正在运行非分布式作业。集群范围内的平均 CPU 利用率仅为 20%-35% 左右。 这与 Microsoft 先前工作中的发现一致。</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNgy1gv5knmkdpcj60xe0cs0uk02.jpg" alt="Pasted image 20210905192021" style="zoom:80%;" />
<p>这一观察结果与 §2.1 中的 all-reduce vs. noncolocated PS 分析相结合，启发了我们——如果我们能更好地利用这些备用 CPU 和带宽，就有可能加速在给定 GPU 上运行的分布式训练作业。</p>
<h3 id="现有的-all-reduce-和-ps-架构存在不足">现有的 all-reduce 和 PS 架构存在不足</h3>
<p>不幸的是，§2.1 中的分析也表明 all-reduce 和 PS 有一个共同的问题：它们没有很好地利用额外的 CPU 和带宽资源。 All-reduce 和 colocated PS 仅使用 GPU worker 上的资源，而非 colocated PS 可能无法充分利用 GPU worker 上的 CPU core 和 NIC 带宽。 前者仅在 k = 0 时通信最优，而后者仅在 k = n 时最优。 当 CPU 机器 k 的数量为 0 &lt; k &lt; n 时，两者都不是最优的。 我们将进一步分析放到§4.1。 在这里，我们通过一个实验来展示现有的 all-reduce 和 PS 的端到端性能。</p>
<blockquote>
<p>点评：allreduce用不了 cpu， ps 在cpu机器少的时候存在热点</p>
</blockquote>
<figure data-type="image" tabindex="2"><img src="https://tva1.sinaimg.cn/large/008i3skNgy1gv5kok1hwcj60vs0gywhm02.jpg" alt="Pasted image 20210905221007" loading="lazy"></figure>
<p>图 4 显示了使用 32 个 V100 GPU（4 个 GPU 机器）和 100GbE RDMA 网络的 VGG-16 的训练速度。 每个 GPU 的批量大小为 32 张图像。 我们运行最新的 MXNet 原生 PS RDMA 实现 和最流行的 all-reduce 库之一的 NCCL-2.5.7 。 我们还测试了 TensorFlow 的原生 PS，并得到了类似的结果。 我们为每个设置改变了额外 CPU 机器的数量。 All-reduce 图是平坦的，因为额外的 CPU 机器没有用，而 PS 即使有额外的 CPU 机器也有最差的性能。 两者都远非最佳。 即使使用 ByteScheduler [55]，这是一种可以提高通信性能的最先进技术，但 all-reduce 和 PS 都离线性缩放还很远，即单 GPU 训练速度的 32倍。</p>
<blockquote>
<p>点评：看上去byte scheduler 调度方式，是对通信过程的流程调度。</p>
</blockquote>
<p>这是因为 ByteScheduler 工作在 PS 或 all-reduce 之上，因此具有相同的限制。 BytePS 在任何给定数量的 CPU 机器上都优于以上所有内容（更多见第 7 节）。</p>
<h3 id="我们的方案-byteps">我们的方案: BytePS.</h3>
<p>它是分布式 DNN 训练的统一架构，可以<strong>利用空闲 CPU 和带宽资源</strong>。 它实现了以下目标。</p>
<p>首先，BytePS 始终是与集群调度程序<strong>分配的任何额外 CPU 和带宽资源</strong>（即 0 &lt;= k &lt;= n）的最佳通信。 在实践中，空闲资源的数量可以是动态的（图 3），因此 BytePS 必须很好地适应。 此外，GPU 机器的硬件设置可以多种多样，尤其是内部 PCIe 或 NVLink 拓扑。 BytePS 也被证明是机器内通信的最佳选择。 All-reduce 和 PS，当它们是最佳通信时，是 BytePS（第 4 节）的两种特殊情况。</p>
<p>其次，BytePS 可以实现非常接近理论最优的通信时间。 这很重要，如现有 PS 案例所示——PS 性能远未达到其理论极限。 我们发现原始 PS 设计有几个实现瓶颈（我们将在第 6 节中讨论）。 但即使去除了所有瓶颈，PS 性能仍然不如最优。 这导致了 BytePS 的第二个设计贡献：<strong>求和服务。 我们发现在 CPU 上运行完整的优化器可能是一个瓶颈。 我们对优化器的计算进行划分，只对 CPU 进行求和</strong>。 我们将在 §5 中阐述这种设计的基本原理。</p>
<p>所有 BytePS 设计都适用于 DNN 训练。 因此，BytePS 可以加速各种 DNN 训练框架，包括 TensorFlow、PyTorch 和 MXNet。 我们从介绍 BytePS 的架构开始。</p>
<h2 id="32-架构概述">3.2 架构概述</h2>
<figure data-type="image" tabindex="3"><img src="https://tva1.sinaimg.cn/large/008i3skNgy1gv5kp2m9xzj60x40dcq4z02.jpg" alt="Pasted image 20210905223306" loading="lazy"></figure>
<p><em>图 5：BytePS 架构。 实线：CPU 机器和 GPU 机器之间的连接。 虚线：GPU 机器内部的数据流。</em></p>
<p>图 5 显示了 BytePS 的架构。 BytePS 有两个主要模块——通信服务（CS）和求和服务（SS）。 在 BytePS 中，我们旨在利用任何 CPU 资源，无论是在 GPU 机器上还是 CPU 机器上，以实现最佳通信效率。 这是由 SS 实现的，它运行在每台机器的 CPU 上，包括 CPU 机器和 GPU 机器。 CPU 机器不一定是实际的仅使用 CPU 的机器。 例如，我们的内部集群调度程序可以在运行非分布式作业并具有备用 CPU 内核和网络带宽的 GPU 机器上分配 CPU。 这提高了整体集群资源利用率。</p>
<p>SS 的另一个重要特性是它比运行成熟 DNN 算法优化器的常见 PS 服务器进程简单得多。 相比之下，SS 只负责接收 CS 发送的张量，将张量汇总并发送回 CS。 <strong>另一个模块 CS 负责在多个（如果有）本地 GPU 之间内部同步张量，并与 SS 进行外部通信</strong>。 每次训练迭代，每个 CS 必须向 SS 发送总共 M 个字节（DNN 模型大小）并从 SS 接收 M 个字节。 在同步分布式训练中，张量是模型梯度。</p>
<blockquote>
<p>点评：cs 负责gpu间同步</p>
</blockquote>
<p>CS 包含了 BytePS 的几个设计点。 首先，它决定每个 SS（内部和外部）的流量。 负载分配策略基于我们对最佳通信策略的分析（第 4.1 节）。 其次，它根据 GPU 机器的不同内部 GPU 和 NIC 拓扑（第 4.2 节）选择最佳的局部张量聚合策略。 最后，CS 和 SS 都应针对现代高速数据中心中的 RDMA 进行优化（第 6.2 节）。</p>
<blockquote>
<p>点评：所以有几个关键点：负载均衡通信策略，张量聚合策略，RDMA优化，cs 和 ss 和交互</p>
</blockquote>
<p>这种架构使 BytePS 能够灵活地利用任意数量的额外 CPU 资源和网络带宽。 当 CPU 机器数为 0 时，即 k = 0，通信将回退到仅在 GPU 机器上使用 SS。 当 CPU 机器数与 GPU 机器数相同时，BytePS 与非 colocated PS 的通信最佳。 在其他情况下，BytePS 可以同时利用所有机器上的 SS。 事实上，我们的分析结果将揭示与任意数量的 CPU 机器的最佳通信策略，而 PS 和 all-reduce 只是整个问题空间中的两个特定点。</p>
<blockquote>
<p>点评：其实就是借用gpu机器上的cpu，来达到最优ps数量</p>
</blockquote>
<h1 id="4-byteps-通信设计">4. BytePS 通信设计</h1>
<h2 id="41-机器间通信">4.1 机器间通信</h2>
<p>在 BytePS 中，所有的网络通信都是在 CS 和 SS 之间进行的。 为了防止瓶颈节点拖慢整个系统，我们必须平衡所有机器的通信时间。 在下文中，我们假设网络具有完整的二分带宽，这是深度学习集群中的常见做法。 我们还假设由于新引入的 RDMA 拥塞控制算法，例如 DCQCN，可以充分利用完整的二分带宽。</p>
<blockquote>
<p>点评：所以 byteps 依赖 rdma 的gpu 集群</p>
</blockquote>
<p>在每台 CPU 机器上，其 SS 的工作负载总和决定了网络流量。 例如，如果一个 SS 负责汇总 x% 的 DNN 模型，则 CPU 机器将在每次训练迭代期间向每台 GPU 机器发送和接收 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi><mi mathvariant="normal">%</mi><mi>M</mi></mrow><annotation encoding="application/x-tex">x \% M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.80556em;vertical-align:-0.05556em;"></span><span class="mord mathdefault">x</span><span class="mord">%</span><span class="mord mathdefault" style="margin-right:0.10903em;">M</span></span></span></span> 字节的流量。 但是，GPU 机器的网络流量是由其上运行的 CS 和 SS 的组合决定的。 由于这种差异，BytePS根据是运行在CPU机器还是GPU机器上将SS分为SSCPU和SSGPU。</p>
<p>为了最小化通信时间，BytePS 将 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>M</mi><mrow><mi>S</mi><msub><mi>S</mi><mrow><mi>C</mi><mi>P</mi><mi>U</mi></mrow></msub></mrow></msub></mrow><annotation encoding="application/x-tex">M_{SS_{CPU}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.933635em;vertical-align:-0.250305em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05764em;">S</span><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3567071428571427em;margin-left:-0.05764em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.07153em;">C</span><span class="mord mathdefault mtight" style="margin-right:0.13889em;">P</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">U</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.14329285714285717em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.250305em;"><span></span></span></span></span></span></span></span></span></span> 字节总和工作负载分配给每个 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>S</mi><msub><mi>S</mi><mrow><mi>C</mi><mi>P</mi><mi>U</mi></mrow></msub></mrow><annotation encoding="application/x-tex">SS_{CPU}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.07153em;">C</span><span class="mord mathdefault mtight" style="margin-right:0.13889em;">P</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">U</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 。  <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>M</mi><mrow><mi>S</mi><msub><mi>S</mi><mrow><mi>C</mi><mi>P</mi><mi>U</mi></mrow></msub></mrow></msub></mrow><annotation encoding="application/x-tex">M_{SS_{CPU}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.933635em;vertical-align:-0.250305em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05764em;">S</span><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3567071428571427em;margin-left:-0.05764em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.07153em;">C</span><span class="mord mathdefault mtight" style="margin-right:0.13889em;">P</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">U</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.14329285714285717em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.250305em;"><span></span></span></span></span></span></span></span></span></span> 在方程式1中给出。 其中 k &lt;= 1 是 CPU 机器的数量，n &lt;= 2 是 GPU 机器的数量，k &lt;= n。 在这些限制之外，BytePS 的通信时间回退到像 PS（当 k &gt; n）和 all-reduce（当 k = 0）这样的简单解决方案，如 §4.1.1 所示。</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNgy1gv5kpjgf9lj60o203yq2z02.jpg" alt="Pasted image 20210906105230" style="zoom:80%;" />
<p>类似地，BytePS 将 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>M</mi><mrow><mi>S</mi><msub><mi>S</mi><mrow><mi>G</mi><mi>P</mi><mi>U</mi></mrow></msub></mrow></msub></mrow><annotation encoding="application/x-tex">M_{SS_{GPU}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.933635em;vertical-align:-0.250305em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05764em;">S</span><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3567071428571427em;margin-left:-0.05764em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">G</span><span class="mord mathdefault mtight" style="margin-right:0.13889em;">P</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">U</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.14329285714285717em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.250305em;"><span></span></span></span></span></span></span></span></span></span> 字节分配给每个 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>S</mi><msub><mi>S</mi><mrow><mi>G</mi><mi>P</mi><mi>U</mi></mrow></msub></mrow><annotation encoding="application/x-tex">SS_{GPU}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">G</span><span class="mord mathdefault mtight" style="margin-right:0.13889em;">P</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">U</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 。</p>
<figure data-type="image" tabindex="4"><img src="https://tva1.sinaimg.cn/large/008i3skNgy1gv5kq12up8j60oq04kmx602.jpg" alt="Pasted image 20210906105725" loading="lazy"></figure>
<p>等式 1 和等式 2 显示了最适合最小化通信时间的工作负载分配策略。 分析在 §4.1.1 中。 在实践中，DNN 模型由大小可变的张量组成，可能无法让我们完美地分配工作负载。 BytePS 使用近似方法。 它将张量分成不大于 4MB 的小部分。然后，所有 CS 一致地索引每个部分并将索引散列到 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>[</mo><mn>0</mn><mo separator="true">,</mo><msup><mi>n</mi><mn>2</mn></msup><mo>+</mo><mi>k</mi><mi>n</mi><mo>−</mo><mn>2</mn><mi>k</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">[0,n^2+kn-2k)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.064108em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">n</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.77777em;vertical-align:-0.08333em;"></span><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="mord mathdefault">n</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">2</span><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="mclose">)</span></span></span></span> 的范围内。 CS 将根据散列值向 SS 发送和接收张量，并根据等式 1 和等式 2 估算概率。一致的索引和散列保证来自所有 GPU 的相同部分将被发送到同一个 SS 并由其处理。</p>
<blockquote>
<p>我们发现 4MB 分区大小在我们的环境中运行良好，但 BytePS 允许用户调整分区大小值。</p>
</blockquote>
<blockquote>
<p>点评：tensor切分小块，索引按什么顺序索引呢，如果是稀疏的场景该怎么办呢</p>
</blockquote>
<h3 id="411-通信效率分析">4.1.1 通信效率分析</h3>
<p>接下来，我们介绍BytePS的通信时间分析。 为了简化分析，我们假设模型大小 M 远大于分区大小（在我们的例子中为 4MB）。 分区使 BytePS 不仅可以更好地平衡求和工作负载，还可以通过流水线发送和接收来很好地利用双向网络带宽，如 [34, 55] 所示。 因此，我们进一步假设发送和接收整个 M 字节可以完全重叠，开销可以忽略不计。 我们有以下结果。</p>
<p><strong>推理 1</strong>. 等式 1 和等式 2 给出的 SS 工作负载分配对于最小化通信时间是最佳的。</p>
<p>证明。 我们首先考虑 GPU 机器的网络流量。 它运行一个 CS 模块和一个 SS 模块。 CS 总共应该发送和接收 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.10903em;">M</span></span></span></span> 个字节。 但是，当它与同一 GPU 机器上的 SS 通信时，流量不会通过网络。 因此，CS 模块将发送和接收 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>M</mi><mo>−</mo><msub><mi>M</mi><mrow><mi>S</mi><msub><mi>S</mi><mrow><mi>G</mi><mi>P</mi><mi>U</mi></mrow></msub></mrow></msub></mrow><annotation encoding="application/x-tex">M-M_{SS_{GPU}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.933635em;vertical-align:-0.250305em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05764em;">S</span><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3567071428571427em;margin-left:-0.05764em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">G</span><span class="mord mathdefault mtight" style="margin-right:0.13889em;">P</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">U</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.14329285714285717em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.250305em;"><span></span></span></span></span></span></span></span></span></span> 个字节。 GPU机器上的SS模块必须从其他<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">n-1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="mord mathdefault">n</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span>个GPU机器接收和发送<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>M</mi><mrow><mi>S</mi><msub><mi>S</mi><mrow><mi>G</mi><mi>P</mi><mi>U</mi></mrow></msub></mrow></msub></mrow><annotation encoding="application/x-tex">M_{SS_{GPU}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.933635em;vertical-align:-0.250305em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05764em;">S</span><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3567071428571427em;margin-left:-0.05764em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">G</span><span class="mord mathdefault mtight" style="margin-right:0.13889em;">P</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">U</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.14329285714285717em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.250305em;"><span></span></span></span></span></span></span></span></span></span>, 总共 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><mi>n</mi><mo>−</mo><mn>1</mn><mo>)</mo><msub><mi>M</mi><mrow><mi>S</mi><msub><mi>S</mi><mrow><mi>G</mi><mi>P</mi><mi>U</mi></mrow></msub></mrow></msub></mrow><annotation encoding="application/x-tex">(n-1)M_{SS_{GPU}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathdefault">n</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.000305em;vertical-align:-0.250305em;"></span><span class="mord">1</span><span class="mclose">)</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05764em;">S</span><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3567071428571427em;margin-left:-0.05764em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">G</span><span class="mord mathdefault mtight" style="margin-right:0.13889em;">P</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">U</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.14329285714285717em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.250305em;"><span></span></span></span></span></span></span></span></span></span> 。将它们加在一起，网络带宽为 B 的 GPU 机器需要通信时间 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>t</mi><mi>g</mi></msub></mrow><annotation encoding="application/x-tex">t_g</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9011879999999999em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">g</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span>：</p>
<figure data-type="image" tabindex="5"><img src="https://tva1.sinaimg.cn/large/008i3skNgy1gv5kzkcgx3j60oo04s74b02.jpg" alt="Pasted image 20210906121509" loading="lazy"></figure>
<p>同样，如果 k &gt; 0，我们可以得到网络带宽为 B 的 CPU 机器需要通信时间 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>t</mi><mi>c</mi></msub></mrow><annotation encoding="application/x-tex">t_c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.76508em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>：</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>t</mi><mi>c</mi></msub><mo>=</mo><msub><mi>M</mi><mrow><mi>S</mi><msub><mi>S</mi><mrow><mi>C</mi><mi>P</mi><mi>U</mi></mrow></msub></mrow></msub><mi mathvariant="normal">/</mi><mi>B</mi></mrow><annotation encoding="application/x-tex">t_c = M_{SS_{CPU}}/B
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.76508em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.000305em;vertical-align:-0.250305em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05764em;">S</span><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3567071428571427em;margin-left:-0.05764em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.07153em;">C</span><span class="mord mathdefault mtight" style="margin-right:0.13889em;">P</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">U</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.14329285714285717em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.250305em;"><span></span></span></span></span></span></span><span class="mord">/</span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span></span></span></span></span></p>
<p>此外，所有 SS 工作量的总和应等于总模型大小。</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>M</mi><mo>=</mo><mi>k</mi><msub><mi>M</mi><mrow><mi>S</mi><mi>S</mi><mrow><mi>C</mi><mi>P</mi><mi>U</mi></mrow></mrow></msub><mo>+</mo><mi>n</mi><msub><mi>M</mi><mrow><mi>S</mi><mi>S</mi><mrow><mi>G</mi><mi>P</mi><mi>U</mi></mrow></mrow></msub></mrow><annotation encoding="application/x-tex">M = kM_{SS{CPU}} +nM_{SS{GPU}}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05764em;">S</span><span class="mord mathdefault mtight" style="margin-right:0.05764em;">S</span><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.07153em;">C</span><span class="mord mathdefault mtight" style="margin-right:0.13889em;">P</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">U</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord mathdefault">n</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05764em;">S</span><span class="mord mathdefault mtight" style="margin-right:0.05764em;">S</span><span class="mord mtight"><span class="mord mathdefault mtight">G</span><span class="mord mathdefault mtight" style="margin-right:0.13889em;">P</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">U</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span></p>
<p>从方程 5 可以看出，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>M</mi><mrow><mi>S</mi><msub><mi>S</mi><mrow><mi>C</mi><mi>P</mi><mi>U</mi></mrow></msub></mrow></msub></mrow><annotation encoding="application/x-tex">M_{SS_{CPU}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.933635em;vertical-align:-0.250305em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05764em;">S</span><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3567071428571427em;margin-left:-0.05764em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.07153em;">C</span><span class="mord mathdefault mtight" style="margin-right:0.13889em;">P</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">U</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.14329285714285717em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.250305em;"><span></span></span></span></span></span></span></span></span></span> 越大，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>M</mi><mrow><mi>S</mi><msub><mi>S</mi><mrow><mi>G</mi><mi>P</mi><mi>U</mi></mrow></msub></mrow></msub></mrow><annotation encoding="application/x-tex">M_{SS_{GPU}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.933635em;vertical-align:-0.250305em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05764em;">S</span><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3567071428571427em;margin-left:-0.05764em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">G</span><span class="mord mathdefault mtight" style="margin-right:0.13889em;">P</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">U</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.14329285714285717em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.250305em;"><span></span></span></span></span></span></span></span></span></span> 越小。 因此，当 n 2 时，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>t</mi><mi>c</mi></msub></mrow><annotation encoding="application/x-tex">t_c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.76508em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 越大，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>t</mi><mi>g</mi></msub></mrow><annotation encoding="application/x-tex">t_g</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9011879999999999em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">g</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span>  越小（或者如果 n = 2，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>t</mi><mi>g</mi></msub></mrow><annotation encoding="application/x-tex">t_g</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9011879999999999em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">g</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span>  不变）。 另外，我们知道最终的通信时间是<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>m</mi><mi>a</mi><mi>x</mi><mo>(</mo><msub><mi>t</mi><mi>c</mi></msub><mo separator="true">,</mo><msub><mi>t</mi><mi>g</mi></msub><mo>)</mo></mrow><annotation encoding="application/x-tex">max(t_c ,t_g)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.036108em;vertical-align:-0.286108em;"></span><span class="mord mathdefault">m</span><span class="mord mathdefault">a</span><span class="mord mathdefault">x</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">g</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>。</p>
<p>为了最小化通信时间，tc 和 tg 需要相等。 如果它们不相等，例如 tc &gt; tg，则意味着可以通过减少 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>M</mi><mrow><mi>S</mi><msub><mi>S</mi><mrow><mi>C</mi><mi>P</mi><mi>U</mi></mrow></msub></mrow></msub></mrow><annotation encoding="application/x-tex">M_{SS_{CPU}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.933635em;vertical-align:-0.250305em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05764em;">S</span><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3567071428571427em;margin-left:-0.05764em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.07153em;">C</span><span class="mord mathdefault mtight" style="margin-right:0.13889em;">P</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">U</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.14329285714285717em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.250305em;"><span></span></span></span></span></span></span></span></span></span> 进一步减少通信时间，从而降低 tc。</p>
<p>当 CPU 机器和 GPU 机器的数量相同时， <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>M</mi><mrow><mi>S</mi><msub><mi>S</mi><mrow><mi>G</mi><mi>P</mi><mi>U</mi></mrow></msub></mrow></msub></mrow><annotation encoding="application/x-tex">M_{SS_{GPU}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.933635em;vertical-align:-0.250305em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05764em;">S</span><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3567071428571427em;margin-left:-0.05764em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">G</span><span class="mord mathdefault mtight" style="margin-right:0.13889em;">P</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">U</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.14329285714285717em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.250305em;"><span></span></span></span></span></span></span></span></span></span> = 0，这意味着我们不需要任何 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>S</mi><msub><mi>S</mi><mrow><mi>G</mi><mi>P</mi><mi>U</mi></mrow></msub></mrow><annotation encoding="application/x-tex">{SS_{GPU}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">G</span><span class="mord mathdefault mtight" style="margin-right:0.13889em;">P</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">U</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span> 。 这是因为 CPU 机器已经提供了足够的聚合带宽。 BytePS 回退到非共放置PS。 同样，当CPU机器数为0时，BytePS回退到all-reduce和colocated PS。</p>
<p>当然，更有趣的情况是当 0 &lt; k &lt; n 时的一般情况。 我们使用普通的 all-reduce 和 non-colocated PS 的通信时间作为两个基线。 我们将加速比 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>r</mi><mi>a</mi></msub></mrow><annotation encoding="application/x-tex">r_a</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">a</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 定义为普通 all-reduce 的通信时间除以一般情况的通信时间。 类似地，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>r</mi><mi>p</mi></msub></mrow><annotation encoding="application/x-tex">r_p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.716668em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> 被定义为与非同位 PS 情况相比的加速比。 我们有以下结论</p>
<figure data-type="image" tabindex="6"><img src="https://tva1.sinaimg.cn/large/008i3skNgy1gv5kqkapkaj60ra05u74g02.jpg" alt="Pasted image 20210906153023" loading="lazy"></figure>
<p>当 k = n 且 n趋向无穷, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>r</mi><mi>a</mi></msub><mo>=</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">r_a=2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">a</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span></span></span></span>。当k 很小时，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>r</mi><mi>p</mi></msub></mrow><annotation encoding="application/x-tex">r_p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.716668em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> 可能会很大，<strong>因为通信带宽在非colocated PS 中被CPU 机器严重瓶颈</strong>。 例如，当 n = 32 和 k = 16 时，我们分别有 ga = 1.46 和 gp = 1.52。 这意味着 BytePS 理论上可以分别超过 all-reduce 和 PS 46% 和 52%。 我们注意到在 k = n 之外添加更多 CPU 机器无济于事，因为通信瓶颈将成为 GPU 机器的 NIC 带宽。</p>
<h2 id="42-机内通信">4.2 机内通信</h2>
<p>在 §4.1 中，我们设计了最优的机器间通信策略。 在实践中，我们发现机内通信同样重要。 一台机器中通常有多个 GPU。 CS 必须在与 SS 通信之前/之后聚合/广播张量。 这会在 PCIe 链路上造成拥塞，并阻止 NIC 充分利用其带宽 B。此外，GPU 机器的内部拓扑在数据中心可能会有所不同。 下面，我们分享我们环境中最常见的两种机器设置以及我们相应的解决方案。 我们在第 4.2.3 节中介绍了一些适用于其他机器设置的原则。</p>
<h3 id="421-pcie-only-拓扑">4.2.1 PCIe-only 拓扑</h3>
<figure data-type="image" tabindex="7"><img src="https://tva1.sinaimg.cn/large/008i3skNgy1gv5kr5ii2jj60vi0ckta702.jpg" alt="Pasted image 20210906154354" loading="lazy"></figure>
<p><em>图 6：仅 PCIe 的机器拓扑和 BytePS 数据流。 灰色框是 GPU。 数据流图中仅显示了传出方向（从 GPU 到网络）。 传入则相反。</em></p>
<p>图 6(a) 显示了我们生产环境中的设置。 一台 GPU 机器有两个通过 QPI 连接的 NUMA CPU。 八个 GPU 分为两组，分别连接到两个 PCIe 交换机。 网卡是 100Gbps 并连接到其中一个 CPU 的 PCIe。 图中所有PCIe链路均为3.0 x16（128Gbps理论带宽）。 CPU 内存和 QPI 具有 &gt; 300Gbps 带宽，通信瓶颈的可能性较小。 我们称之为仅 PCIe 拓扑。对于这个机器模型，我们测量到 GPU 到 GPU 内存复制的吞吐量在同一 PCIe 交换机内为 105Gbps。 然而，跨 PCIe 交换机的 GPU 到 GPU 内存复制的吞吐量仅为 80Gbps。</p>
<p>不幸的是，许多现有的训练框架忽略了内部拓扑的这些细节。 例如，TensorFlow PS、MXNet PS 甚至 Horovod 的“分层 all-reduce”模式在同一台机器上的所有 GPU 上使用直接的 reduce 或 reduce-scatter。 这将导致跨 PCIe 交换机内存复制，不幸的是速度较慢。</p>
<p>相比之下，BytePS让同一PCIe交换机下的GPU先对张量求和，然后复制到CPU让CPU做全局求和，最后广播回全局求和。 我们称之为 CPU 辅助聚合。 具体来说，它包括以下步骤。</p>
<blockquote>
<p>点评：这种做法有点针对特定拓扑吧</p>
</blockquote>
<ol>
<li>Reduce-Scatter：假设每个 PCIe 交换机有L 个 GPU。 这 L 个GPU 执行reduce-scatter，仅在 PCIe 交换机内部产生 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><mi>l</mi><mo>−</mo><mn>1</mn><mo>)</mo><mi>M</mi><mi mathvariant="normal">/</mi><mi>l</mi></mrow><annotation encoding="application/x-tex">(l-1)M/l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mclose">)</span><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="mord">/</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span></span></span></span> 流量。 当它完成时，每个 GPU 应该保存 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>M</mi><mi mathvariant="normal">/</mi><mi>l</mi></mrow><annotation encoding="application/x-tex">M/l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="mord">/</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span></span></span></span> 聚合数据。</li>
<li>GPU-CPU 复制：每个 GPU 将其 M/l 数据复制到 CPU 内存，这会导致沿途的 M/l 流量。 每个 PCIe 交换机都会生成 M 个聚合数据。</li>
<li>CPU-Reduce：CPU 规约来自所有 PCIe 交换机的数据并生成跨所有 GPU 的聚合数据。 这种规约不会产生任何 PCIe 流量。</li>
<li>networking：CS 将数据发送到 SS，并从 SS 接收<strong>全局聚合数据</strong>。</li>
<li>CPU-GPU 复制：每个 GPU 将其 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>M</mi><mi mathvariant="normal">/</mi><mi>l</mi></mrow><annotation encoding="application/x-tex">M/l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="mord">/</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span></span></span></span> 分区从 CPU 内存复制回自身。 这会导致从 CPU 到每个 GPU 的 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>M</mi><mi mathvariant="normal">/</mi><mi>l</mi></mrow><annotation encoding="application/x-tex">M/l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="mord">/</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span></span></span></span> 流量。</li>
<li>All-Gather：每个 GPU 对同一 PCIe 交换机下的 GPU 执行all-gather操作。 这会导致交换机内部产生 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><mi>l</mi><mo>−</mo><mn>1</mn><mo>)</mo><mi>M</mi><mi mathvariant="normal">/</mi><mi>l</mi></mrow><annotation encoding="application/x-tex">(l-1)M/l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mclose">)</span><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="mord">/</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span></span></span></span>流量。</li>
</ol>
<blockquote>
<p>点评：这个做法比快手使用 0号gpu 的做法要合理一点</p>
</blockquote>
<p>图 6(b) 显示了步骤 1 到 3 的流量。步骤 4 到 6 使用相同的链路但方向相反。 通过 CPU 辅助聚合，从 PCIe 切换到 CPU 链路将在每个方向仅承载 M 个流量，远低于直接在 8 个 GPU 上进行集体操作（7M/4 个流量）。 同时，每个 PCIe 交换机到 GPU 链路的流量将为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><mn>2</mn><mi>l</mi><mo>−</mo><mn>1</mn><mo>)</mo><mi>M</mi><mi mathvariant="normal">/</mi><mi>L</mi></mrow><annotation encoding="application/x-tex">(2l-1)M/L</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord">2</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mclose">)</span><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="mord">/</span><span class="mord mathdefault">L</span></span></span></span>, 设 l = 4（每个 PCIe 有四个 GPU），即 7M/4，与现有方法保持一致。 从根本上说，BytePS 利用 GPU 机器上的备用 CPU 来避免缓慢的 <strong>GPU 到 GPU 跨 PCIe 交换机内存复制</strong>。</p>
<p><strong>优化分析</strong>。 我们现在分析上述策略的通信最优性。 图 7 显示了一种更通用的仅 PCIe 拓扑，具有可变数量的 GPU 和 PCIe 交换机。 我们没有像图 6(a) 那样绘制 NIC，因为在这种拓扑结构下，NIC 具有专用的 PCIe 通道并且不会与 GPU 竞争 PCIe 带宽。</p>
<figure data-type="image" tabindex="8"><img src="https://tva1.sinaimg.cn/large/008i3skNgy1gv5krr2hlmj60ny0e4mxx02.jpg" alt="Pasted image 20210906164341" loading="lazy"></figure>
<p>系统架构被建模为层次图 G = (V,E)。 将 N 表示为叶节点 (GPU) 的集合，将 S 表示为中间节点（交换机）的集合，将 C 表示为 CPU 节点的集合。 V为 N、S和C的并集. E 中的每条边 e(vx, vy) 表示从顶点 vx 到 vy 的带宽，我们将 t(vx, vy) 表示为从 vx 发送到 vy 的流量。 我们进一步定义 p 为交换机的数量 (p&gt;=2)，n 作为每个交换机连接的叶节点 (n &gt;=2)。</p>
<p>我们假设 G 的以下特征： (1) E 中的每条边都是双工的，并且两个方向的带宽相等。 将b(vx, vy)表示为e(vx, vy)的带宽，则b(vx, vy) = b(vy, vx)； (2) 我们假设 G 是对称的。 树的同一层的带宽是等效的。 例如，对于任何 j, k 属于 [0, p1], x, y 属于 [ jn, (j+1)n-1]; (3)<strong>内存和QPI</strong>带宽远高于PCIe链路，不太可能成为瓶颈。 下面，我们只关注 PCIe 链路。</p>
<blockquote>
<p>补充：<code>QPI &lt;Quick Path Interconnect&gt;</code>，又名CSI，Common System Interface公共系统接口，是一种可以实现芯片间直接互联的架构。</p>
</blockquote>
<p>从 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>N</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">N_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 到 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>N</mi><msub><mi>p</mi><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow></msub></msub></mrow><annotation encoding="application/x-tex">N_{p_{n-1}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.975095em;vertical-align:-0.291765em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3173142857142857em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">n</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.20252142857142857em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.291765em;"><span></span></span></span></span></span></span></span></span></span> 的 GPU 需要对它们的数据求和。 我们可以使用前面提到的 CPU 辅助聚合，也可以使用需要每个 GPU 将其整个数据直接复制到 C 的蛮力复制。 在实践中，最优解应该是这两种策略的组合，取决于 b(Sj,Cj) 和 b(Ni,Sj) 的值。 直觉是我们对 x 的数据应用蛮力复制，并对数据的 y (x+y = 1) 应用 CPU 辅助聚合。 在一定的 x 和 y 下，作业完成时间 J 可以最小化。 我们分别计算两条链路的流量。 在 e(Sj,Cj) 上，流量由 n 次蛮力复制加上 CPU 辅助聚合的流量组成。 在 e(Ni,Cj) 上，流量由一个蛮力副本和 CPU 辅助聚合的完整流量组成。</p>
<blockquote>
<p>这里其实没明白为什么</p>
</blockquote>
<figure data-type="image" tabindex="9"><img src="https://tva1.sinaimg.cn/large/008i3skNgy1gv5ksbqn8lj60vw09cgma02.jpg" alt="Pasted image 20210906172651" loading="lazy"></figure>
<p>（一通计算后）这意味着最佳解决方案是这样工作的：每个 GPU 对其 1/5 数据应用蛮力复制，并对其余 4/5 数据使用 CPU 辅助聚合。 因此，我们有以下主要结论：</p>
<p><strong>CPU 辅助聚合接近最佳</strong><br>
当 x = 0 时，解决方案是我们的 CPU 辅助聚合，作业完成时间为 J(0) = 0.141s。 根据计算，最佳时间为 0.129 秒。 因此，我们的策略非常接近最佳解决方案，性能差异为 9%。 然而，在实践中，强力复制对 CPU 内存有很大压力——与 CPU 辅助聚合相比，任何使用强力复制的张量都将消耗 4倍 CPU 内存带宽。 CPU内存真的没有4倍带宽的PCIe链路，尤其是FP16求和。因此，我们选择根本不使用蛮力复制并坚持使用 CPU 辅助聚合。</p>
<p><strong>CPU 辅助聚合优于基于环的 allreduce</strong></p>
<figure data-type="image" tabindex="10"><img src="https://tva1.sinaimg.cn/large/008i3skNgy1gv5kssysyxj60w8076gn102.jpg" alt="Pasted image 20210906173540" loading="lazy"></figure>
<p>所以很容易证明 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>J</mi><mrow><mi>c</mi><mi>a</mi></mrow></msub><mo>&lt;</mo><msub><mi>J</mi><mrow><mi>a</mi><mi>r</mi></mrow></msub></mrow><annotation encoding="application/x-tex">J_{ca} &lt; J_{ar}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.09618em;">J</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.09618em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">c</span><span class="mord mathdefault mtight">a</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.09618em;">J</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.09618em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">a</span><span class="mord mathdefault mtight" style="margin-right:0.02778em;">r</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 对于任何 n, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi><mo>&gt;</mo><mo>=</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">p&gt;=2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7335400000000001em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">p</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&gt;</span></span><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span></span></span></span> 总是成立。例如，使用我们的 PCIe 机器的值，让 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi><mo>=</mo><mn>2</mn><mo separator="true">,</mo><mi>n</mi><mo>=</mo><mn>4</mn><mo separator="true">,</mo><msub><mi>b</mi><mrow><mi>b</mi><mi>o</mi><mi>t</mi><mi>t</mi><mi>l</mi><mi>e</mi><mi>n</mi><mi>e</mi><mi>c</mi><mi>k</mi></mrow></msub><mo>=</mo><mn>80</mn><mi>G</mi><mi>b</mi><mi>p</mi><mi>s</mi></mrow><annotation encoding="application/x-tex">p = 2, n = 4, b_{bottleneck} = 80Gbps</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">p</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8388800000000001em;vertical-align:-0.19444em;"></span><span class="mord">2</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">n</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord">4</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">b</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">t</span><span class="mord mathdefault mtight">t</span><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span><span class="mord mathdefault mtight">e</span><span class="mord mathdefault mtight">n</span><span class="mord mathdefault mtight">e</span><span class="mord mathdefault mtight">c</span><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord">8</span><span class="mord">0</span><span class="mord mathdefault">G</span><span class="mord mathdefault">b</span><span class="mord mathdefault">p</span><span class="mord mathdefault">s</span></span></span></span>（跨越 PCIe 交换机的内存副本的带宽 ) 和 b(Sj,Cj) = 105Gbps 我们得到 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>J</mi><mrow><mi>c</mi><mi>a</mi></mrow></msub></mrow><annotation encoding="application/x-tex">J_{ca}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.09618em;">J</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.09618em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">c</span><span class="mord mathdefault mtight">a</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 比 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>J</mi><mrow><mi>a</mi><mi>r</mi></mrow></msub></mrow><annotation encoding="application/x-tex">J_{ar}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.09618em;">J</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.09618em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">a</span><span class="mord mathdefault mtight" style="margin-right:0.02778em;">r</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 小 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>23.7</mn></mrow><annotation encoding="application/x-tex">23.7%</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span><span class="mord">3</span><span class="mord">.</span><span class="mord">7</span></span></span></span>。</p>
<h3 id="422-基于-nvlink-的拓扑">4.2.2 基于 NVLink 的拓扑</h3>
<p>图 8(a) 显示了我们数据中心的另一个机器模型——带有 NVLinks 的 GPU 机器。 有四个 PCIe 交换机，每个交换机连接两个 GPU 卡。 GPU 也通过 NVLink 连接。 NVLinks 为每个 GPU 提供总计 1.2Tbps GPU-GPU 带宽，远高于 PCIe 链路。 NIC 连接到其中一个 PCIe 交换机。</p>
<figure data-type="image" tabindex="11"><img src="https://tva1.sinaimg.cn/large/008i3skNgy1gv5ktg0ciuj60w80es76302.jpg" alt="Pasted image 20210906193948" loading="lazy"></figure>
<p><em>图 8：基于 NVLink 的机器拓扑和 BytePS 数据流。 数据流图中只显示出方向</em></p>
<p>有了NVLink，GPU-to-GPU通信可以完全避免占用PCIe带宽。 因此，我们不再需要 CPU 辅助聚合。 然而，我们发现现有框架，包括最流行的 GPU all-reduce 实现 NCCL（由 TensorFlow、PyTorch、MXNet 和 Horovod 使用），再次不是最佳的。</p>
<p>问题在于，考虑到 NIC，拓扑不对称，它仅连接到一个（四分之一）PCIe 交换机。 同一个PCIe交换机下的网卡和两个GPU要争夺 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>P</mi><mn>0</mn></msub><mo>−</mo><mi>C</mi><mi>P</mi><msub><mi>U</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">P_0-CPU_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">U</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 的PCIe带宽。 请记住，不仅 CS 使用此 PCIe 带宽，而且在同一台 GPU 机器上运行的 SS 也使用它！ <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>P</mi><mn>0</mn></msub><mo>−</mo><mi>C</mi><mi>P</mi><msub><mi>U</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">P_0-CPU_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">U</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 再次成为整个通信的瓶颈。</p>
<p>根据分析，我们应该在本地聚合时将尽可能多的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>P</mi><mn>0</mn></msub><mo>−</mo><mi>C</mi><mi>P</mi><msub><mi>U</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">P_0-CPU_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">U</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> PCIe带宽留给网卡。 对于这种拓扑，BytePS 使用reduce 和broadcast 而不是reduce-scatter 和all-gather——来自所有GPU 的张量首先 reduce 到GPU2，然后将结果从 GPU2 复制到CPU0 内存。 图 8(b) 显示了这些步骤。 之后，当 CS 从 SS 得到聚合结果时，GPU2 会将数据复制到 GPU 内存中，并将它们广播到其他 GPU。 这样，我们完全阻止了 GPU 使用 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>P</mi><mn>0</mn></msub><mo>−</mo><mi>C</mi><mi>P</mi><msub><mi>U</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">P_0-CPU_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">U</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 带宽进行通信，因此 NIC 可以运行到 100Gbps 带宽。</p>
<blockquote>
<p>点评：这个拓扑的bug 在于gpu 连接是一个环，建立在特殊的拓扑基础上。<br>
从内存拷贝到网卡用PCIE， 网线会是另外的瓶颈</p>
</blockquote>
<p>这种方法似乎会在 GPU2 上创建流量热点。 但是，NVLinks 的带宽比 PCIe 链路大得多，因此即使在热点上，GPU 间通信也永远不是瓶颈。 同时，用于GPU-CPU复制的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>P</mi><mn>1</mn></msub><mo>−</mo><mi>C</mi><mi>P</mi><msub><mi>U</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">P_1-CPU_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">U</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>  PCIe链路与网卡100Gbps带宽大致相同，因此也不是瓶颈。</p>
<p>BytePS 达到了最优结果——没有机内带宽瓶颈。 不幸的是，现有的解决方案，如 NCCL，由于 GPU0 和 NIC 之间的距离很近，往往让 GPU 使用 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>P</mi><mn>0</mn></msub><mo>−</mo><mi>C</mi><mi>P</mi><msub><mi>U</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">P_0-CPU_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">U</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 瓶颈链路。</p>
<p>因此，它的通信性能低于我们在基于 NVLink 的机器中的解决方案。</p>
<blockquote>
<p>点评：这个看起来应该只是一个微小的工作吧</p>
</blockquote>
<h3 id="423-讨论">4.2.3 讨论</h3>
<p>PCIe-only 和基于 NVLink 拓扑的解决方案大不相同。 这表明不存在万能的最优解。 <strong>匹配内部通信必须适应不同的内部拓扑</strong>。 诚然，在我们的环境中使用的拓扑肯定比上述两种更多。 但是，我们认为以上两个是具有代表性的，因为它们分别类似于服务器供应商和 NVIDIA推荐的参考设计。</p>
<p>尽管存在差异，但我们总结了两个原则 - 1）当两个 GPU 不在同一个 PCIe 交换机下时，始终避免直接 GPU 到 GPU 内存复制，因为它在实践中很慢。 2) 始终尽量减少 PCIe 交换机到 GPU 和 NIC 共享的 CPU 链路上的流量。 我们提出以下最佳实践程序。 设 Sn 是带有 GPU 和 NIC 的 PCIe 交换机的数量，Sg 是只有 GPU 的 PCIe 交换机的数量。</p>
<ol>
<li>如果 Sn &gt; 0 且 Sg &gt; 0，则拓扑是非对称的，就像我们基于 NVLink 的拓扑一样。 CS 应该使用reduce 和broadcast，使用不与NIC 竞争的GPU 作为reduce 或broadcast 根。</li>
<li>如果 Sn = 0 或 Sg = 0，拓扑是对称的，就像我们的 PCIe-only 情况一样。 CS 应该使用 reduce-scatter 和 allgather 来平衡所有 PCIe 交换机上的流量。 如果没有 NVLink，则应使用 CPU 辅助聚合（第 4.2.1 节）。</li>
</ol>
<p><strong>多网卡拓扑</strong>。 虽然我们讨论的两种具体拓扑只有一个网卡，但上述原理可以直接扩展到多网卡拓扑——它只是改变了 Sn 和 Sg 的值。</p>
<p><strong>GPU 直接 RDMA (GDR)</strong>。 GDR 可以潜在地减少 PCIe 流量。 但是，GDR 要求 GPU 和 RDMA 网卡在同一个 PCIe 交换机上，否则即使使用 100GbE 网卡，吞吐量也可能低于 50Gbps [12]，我们自己的测量也证实了这一点。 因此，GDR 对我们的设置没有好处——仅 PCIe 拓扑不能满足要求，我们已经避免了基于 NVLink 拓扑的任何 PCIe 瓶颈。 此外，像 AWS 这样的大多数云都不支持 GDR。 因此，BytePS 目前不使用 GDR。</p>
<blockquote>
<p>点评：byteps 不支持 GPU 直接RDMA</p>
</blockquote>
<p>我们可以看到最优的<strong>机内通信策略与内部拓扑紧密耦合</strong>。 构建一个分析器来自动检测拓扑、探测带宽并生成最佳策略是未来有趣的工作。</p>
<h1 id="5-求和服务">5 求和服务</h1>
<p>为了获得最佳的机器间通信时间（第 4.1 节），BytePS 需要一个可以在每台机器的 CPU 上运行并与 CS 通信的模块。 问题是，它在训练算法中的作用是什么？ 我们最初的尝试是遵循之前的 PS 设计，其中 PS 进程负责运行优化器。 优化器聚合来自所有 GPU 的梯度，并使用各种优化器更新 DNN 模型参数。</p>
<h2 id="cpu-瓶颈">CPU 瓶颈</h2>
<p>不幸的是，很快我们发现 CPU 成为系统中的瓶颈。 我们用一个实验来证明这一点。 我们使用典型的非协同 PS 设置训练 VGG16 DNN ：使用一台 Tesla V100 GPU 机器和一台 CPU 机器（Intel Xeon Platinum CPU，具有超线程的 32 核和 Intel MKL ），通过 100GbE 以太网连接 . GPU 机器运行前向和反向传播，CPU 机器使用全部 32 个 CPU 内核运行优化器。</p>
<figure data-type="image" tabindex="12"><img src="https://tva1.sinaimg.cn/large/008i3skNgy1gv5ktx6e3rj60ww0e876a02.jpg" alt="Pasted image 20210906204431" loading="lazy"></figure>
<p><em>图 9：优化器的 CPU 很慢，但求和的 CPU 不慢</em></p>
<p>图 9(a) 显示，即使有 32 个内核并启用了 MKL，在 CPU 机器上运行优化器也会减慢端到端的训练速度。 这意味着 CPU 无法匹配网络带宽并成为瓶颈（第 6 节）。 随着优化器算法变得越来越复杂（从更简单的 SGD 到更复杂的 RMSProp），瓶颈效应变得更加严重。</p>
<p>根本原因。 CPU 瓶颈是由有限的内存带宽引起的。 Adam 等流行的优化器很容易耗尽现代 CPU 的内存带宽。 例如，6 通道 DDR4-2666 内存设置的峰值传输速率高达 1024 Gbps，结合读取和写入 [8]。 很容易估计，例如，Adam 优化器 [42] 需要超过 10 倍的内存访问（读+写）来应用每个梯度更新。 加上 100Gbps 网卡消耗 200 Gbps 内存带宽（读+写），1024 Gbps 内存带宽根本不足以让 Adam 处理 100 Gbps 梯度流。</p>
<blockquote>
<p>点评：这里很硬核</p>
</blockquote>
<h2 id="cpu擅长求和">CPU擅长求和</h2>
<p>上面的实验让我们重新思考 CPU 上的任务。优化器的计算可以分为两个步骤，梯度求和和参数更新，如图 10 所示。</p>
<figure data-type="image" tabindex="13"><img src="https://tva1.sinaimg.cn/large/008i3skNgy1gv5kufwhl2j60v20ay0u502.jpg" alt="Pasted image 20210906205041" loading="lazy"></figure>
<p><em>图 10：all-reduce、PS 和 BytePS 之间的组件放置比较</em></p>
<p>幸运的是，由于高度优化的<strong>AVX 指令 [47]</strong>，现代 x86 CPU 擅长求和。 在图 9(b) 中，我们使用合成浮点张量显示了与上述相同 CPU 上的求和吞吐量。 <strong>FP16和FP32精度的吞吐量都超过200Gbps，高于100Gbps的网卡带宽</strong>。 因此，CPU 上的求和不会成为瓶颈。</p>
<p>BytePS 的解决方案。 基于这些观察，BytePS 将优化器的两个步骤解耦。 我们将计算密集型参数更新移至 GPU，并仅在 CPU 上进行求和——这就是我们将 CPU 模块命名为求和服务 (SS) 的原因。 SS 不仅可以避免 CPU 成为瓶颈，还可以大大降低 CPU 开销。 通过使用 AVX 和 OpenMP 精心实施，SS 在以 100Gbps 吞吐量运行时仅消耗少于 3 个 CPU 内核。 图 10 对 PS、all-reduce 和 BytePS 进行了high-level 比较，了解它们如何将 DNN 训练中的不同组件放置到 GPU 和 CPU 资源上。</p>
<p>由于 Summation Service 将参数更新移动到 GPU 机器上，因此所有 GPU 机器都需要执行相同的参数更新计算，而在传统 PS 中，参数更新只需进行一次。 因此，BytePS 比 PS 使用更多的计算周期来更新参数。 这是我们自愿做出的权衡，以加快端到端的训练速度。 我们将 SS 开销比定义为 参数更新的 FLOP 除以FP 和 BP FLOPS 总和。</p>
<p>VGG-16、ResNet-50、BERTlarge使用SGD作为优化器的比率为138 MFLOPs / 32 GFLOPs、26 MFLOPs / 7.8 GFLOPs、387 MFLOPs / 494 GFLOPs，<strong>均小于0.5%</strong>。 与训练加速相比，引入的开销可以忽略不计（图 9（a））。 上述比率定义假设批量大小为 1。DNN 训练通常使用数十或数百个批量大小。 每批参数更新一次，因此额外的开销在实践中更小。</p>
<p>我们注意到 Horovod [60] 可以选择通过首先将张量复制到 CPU 内存然后执行 CPU-only all-reduce 来将梯度聚合移动到 CPU。 由于它仍然只依赖 GPU 机器上的 CPU 和带宽，因此与 GPU 上的直接 all-reduce 相比，它没有提供通信方面的优势。 BytePS 不同：它利用额外的 CPU 机器进行梯度求和，同时在 GPU 上保持参数更新。</p>
<blockquote>
<p>点评：byteps 用的额外的cpu 和带宽</p>
</blockquote>
<h2 id="支持异步训练">支持异步训练</h2>
<p>虽然分离求和和更新给我们带来了性能上的好处，但它打破了原始 PS 的一个重要特性：像 Asynchronous Parallel [25] 这样的异步训练的支持。 异步并行依赖于保持最新模型参数的 PS 进程，这与 SS 的设计不直接兼容。</p>
<blockquote>
<p>点评：是啊，worker 上做参数更新，那肯定得同步更新</p>
</blockquote>
<p>为了弥补这一差距，我们重新设计了一个新的工作流程，可以启用与 SS 的异步训练，如图 11（b）所示。 简而言之，GPU 更新参数并首先计算 delta 参数。 CS 发送它们并接收最新的参数。 SS 不断地在最新的参数中添加 delta 参数。 接下来，我们证明这个新的训练工作流程在算法收敛方面等同于异步并行。</p>
<figure data-type="image" tabindex="14"><img src="https://tva1.sinaimg.cn/large/008i3skNgy1gv5kv3ivedj60qo0aodgz02.jpg" alt="Pasted image 20210906215412" loading="lazy"></figure>
<p>图 11：PS 和 BytePS 之间的异步训练工作流比较。 g 是梯度。 w 是参数。</p>
<p><strong>定理2</strong> BytePS的异步算法等价于异步并行<br>
<strong>证明</strong>  考虑一个与 n 个 CS 相连的 SS。 我们说 CS 存储本地模型参数，SS 保存最新版本的参数。 我们证明的high level 想法是表明，在给定相同的通信顺序（推和拉顺序）的情况下，我们的算法与异步并行生成相同的状态（即，SS 模块和 n 个 CS 模块的相同参数）。 我们使用 f 作为优化器的一般表示。因此，优化可以表示为 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>w</mi><mo>=</mo><mi>w</mi><mo>+</mo><mi>f</mi><mo>(</mo><mi>g</mi><mi>i</mi><mo separator="true">,</mo><mi>t</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">w=w+ f(gi,t)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mord mathdefault">i</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">t</span><span class="mclose">)</span></span></span></span>，其中 gi,t 表示迭代 t (t  in [1,T]) 时 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>C</mi><msub><mi>S</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">CS_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> (i  in [0,n1]) 的梯度。 分别将 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>w</mi><mrow><mi>p</mi><mi>s</mi></mrow></msub></mrow><annotation encoding="application/x-tex">w_{ps}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.716668em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">p</span><span class="mord mathdefault mtight">s</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>w</mi><mrow><mi>b</mi><mi>y</mi><mi>t</mi><mi>e</mi><mi>p</mi><mi>s</mi></mrow></msub></mrow><annotation encoding="application/x-tex">w_{byteps}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.716668em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">b</span><span class="mord mathdefault mtight" style="margin-right:0.03588em;">y</span><span class="mord mathdefault mtight">t</span><span class="mord mathdefault mtight">e</span><span class="mord mathdefault mtight">p</span><span class="mord mathdefault mtight">s</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> 表示为 PS 和 BytePS 中的参数。 并且将 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>w</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>t</mi></mrow></msub></mrow><annotation encoding="application/x-tex">w_{i,t}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.716668em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathdefault mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> 表示为迭代 t 时每个 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>w</mi><mi>o</mi><mi>r</mi><mi>k</mi><mi>e</mi><msub><mi>r</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">worker_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="mord mathdefault">e</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>（对于 PS）或 CS（对于 BytePS）上的参数。 对于所有 CS 和 SS，该参数被初始化为 w0。 经过 T 次迭代，我们可以得到更新后的参数为：</p>
<figure data-type="image" tabindex="15"><img src="https://tva1.sinaimg.cn/large/008i3skNgy1gv5kxlas4dj614q09gmxm02.jpg" alt="Pasted image 20210906220857" loading="lazy"></figure>
<blockquote>
<p>点评：这个证明其实没有谈到异步时间的问题</p>
</blockquote>
<h1 id="6-实现">6 实现</h1>
<p>虽然 BytePS 的核心对于任何训练框架都是通用的，但 BytePS 还为 TensorFlow、PyTorch 和 MXNet 实现了插件，以方便用户使用。 核心是用 C++ 实现的，而框架插件包含 C++ 和 Python。 BytePS 总共包含大约 7.8K 行 Python 代码和 10K 行 C++ 代码。 作为主要的在线服务提供商，我们在内部部署了 BytePS。 BytePS 也已经开源 [4] 并吸引了成千上万的用户。</p>
<h2 id="61-多阶段流水线">6.1 多阶段流水线</h2>
<p>加速多步骤过程的一种常见方法是构建一个多级流水线，该流水线与每个步骤的处理时间重叠。 我们结合了先前工作中的张量分区和流水线的想法 [34, 55]。 例如，对于 PCIe-only 拓扑，CS 有六个步骤。 它映射到 <strong>BytePS 运行时</strong>中的 6 级管道。 我们实现 BytePS 以灵活构建管道而无需重新编译。 管道中的每个阶段都被实现为一个具有张量优先级队列的独立线程。 优先级的分配类似于 [34,55]。 正如 §4.1.1 中分析的那样，大张量被分割成多个不超过 4MB 的小张量。 接下来，每个小张量被排入第一个队列，并在一个阶段完成处理后移向下一个队列，直到它从最后一个队列中出列。</p>
<h2 id="62-解决-rdma-性能问题">6.2 解决 RDMA 性能问题</h2>
<p>对于机器间通信，我们使用 RDMA RoCEv2。 每台机器有一个 100GbE 网卡，RDMA 网络提供完整的双向带宽。 为了充分发挥 RDMA 的优势，我们经历了完整的设计和调试之旅，我们分享如下。</p>
<p><strong>RDMA 内存管理</strong></p>
<p>为了提高性能，我们的目标是避免不必要的内存复制 [72] 并在 CPU 内存上实现零复制。 BytePS 基于 RDMA WRITE，因为它是常见 RDMA 用法中性能最高的 [39]。</p>
<blockquote>
<p>点评：RDMA这些论文也看看</p>
</blockquote>
<p>传统的单边 RDMA 操作（WRITE 和 READ）至少需要两次往返：获取远程地址，并将值写入（读取）到（从）该地址 [39, 40, 50, 70]。 我们通过利用 DNN 训练在每次迭代中始终发送相同的张量集这一事实来优化过程。传统的单边 RDMA 操作（WRITE 和 READ）至少需要两次往返：获取远程地址，并将值写入（读取）到（从）该地址 [39, 40, 50, 70]。 我们通过利用 <strong>DNN 训练在每次迭代中始终发送相同的张量集这一事实</strong>来优化过程。</p>
<blockquote>
<p>点评：也就是寻址只需要一次，那特征准入没发解决吧</p>
</blockquote>
<p>只有在第一次迭代时，BytePS 才会初始化所有需要的张量，向 RDMA NIC 注册缓冲区并交换所有远程地址。 然后 BytePS 存储远程缓冲区信息并在其余迭代中直接重用。</p>
<blockquote>
<p>点评：也就是用空间换时间，多占M的空间</p>
</blockquote>
<p><strong>解决慢速接收器症状</strong><br>
我们还遇到了 [30] 中报告的缓慢接收器症状——NIC 向网络发送了许多 PFC。 那些过多的 PFC 会减慢张量传输速度，这可能会对其他流量造成附带损害。 在这里，我们报告了这种症状的几个其他原因以及我们如何解决这些问题</p>
<p>我们的第一个发现是内部 RDMA 环回流量会导致内部 incast，并推动 NIC 生成 PFC。 BytePS 在每台 GPU 机器上运行 CS 和 SS。 它们之间的流量，我们称之为环回流量，不消耗 NIC 的外部以太网带宽，但会消耗内部 CPU-NIC PCIe 带宽。 最初，我们没有添加任何特殊设计——我们坚持使用 RDMA  [9] 来处理环回流量，并认为 NIC DMA 可以处理它。 但是，我们意识到它在 NIC 上创建了 2:1 的 incast，将 RX 和环回作为两个入口端口，将 DMA 到内存引擎作为一个出口端口！</p>
<blockquote>
<p>点评：不太理解这段</p>
</blockquote>
<p>为了解决这个问题，我们实现了一个共享内存 (shm) 数据路径。 当 CS 检测到 SS 与自己在同一台机器上时，CS 会简单地通知 SS 数据在共享内存中。 SS完成求和后，SS将结果从自己的缓冲区复制回CS的共享内存。 因此，消除了环回 RDMA 流量。</p>
<blockquote>
<p>点评：使用共享内存代替socket，正常做法，local 不用rdma</p>
</blockquote>
<p>我们的第二个发现是我们需要为 RDMA 使用页面对齐的内存。 否则可能会触发 PFC。 我们的假设是硬件 DMA 将传输单元与页面大小（例如 4096 字节）对齐。 因此，<strong>使用页对齐地址对 DMA 引擎更友好，因为它减少了需要写入的页数</strong>。</p>
<p>我们的第三个发现是，RDMA NIC RX 性能会受到<strong>并发</strong>发送的实现方式的影响！ 最后，我们不仅使用页面对齐的内存，而且在发送方的每个 RDMA WRITE 只强制执行一个scater-gather entry (sge)。</p>
<p>在整个过程中，我们联系了网卡供应商，并与他们的软硬件专家进行了长时间的讨论。 在撰写本文时，我们还没有得到最后两个问题的官方根本原因。</p>
<p>在所有优化之后，BytePS 实现可以按预期运行。 表 2 显示了应用上述三种优化中的每一种后的性能改进。 NIC 产生的 PFC 可忽略不计。</p>
<blockquote>
<p>点评：相当于byteps硬核修改了 rdma的实现？</p>
</blockquote>
<figure data-type="image" tabindex="16"><img src="https://tva1.sinaimg.cn/large/008i3skNgy1gv5kyusj8dj612009w40202.jpg" alt="Pasted image 20210907094705" loading="lazy"></figure>
<p>正如我们在 §4.1 中所讨论的，BytePS 在网络中创建了许多多对一的通信模式。 多对一以在 TCP/IP 网络中创建 incast 和丢包而闻名 [66]。 但是 BytePS 使用 RDMA/RoCEv2，它依赖于无损结构和 DCQCN [75] 进行拥塞控制。 我们在 BytePS 中没有观察到 incast 问题。</p>
<blockquote>
<p>点评：<a href="https://www.cnblogs.com/liusikun/p/5663193.html">tcp incast 问题</a></p>
</blockquote>
<h2 id="63-byteps-的使用">6.3 BytePS 的使用</h2>
<p>BytePS易于使用。 我们提供几乎与 Horovod、PyTorch 原生 API 和 TensorFlow 原生 API 相同的 Python 接口。 用户可以选择其中任何一个，并以最小的努力迁移到 BytePS。 例如，对于 Horovod-MNIST 示例，我们只需要更改一行 Python 代码，从“import horovod”到“import byteps”。 事实上，我们能够将大部分基于 Horovod 的内部训练任务自动转换为 BytePS。</p>
<h1 id="7-evaluation">7. Evaluation</h1>
<p>在本节中，我们展示了 BytePS 不仅在微基准测试中实现了最佳通信性能，而且还显着加速了生产环境中的训练工作。 我们列出了一些关于结果高保真度的亮点.</p>
<ul>
<li>所有使用的资源都由生产集群的调度器分配。调度器使用非抢占式资源调度——一旦调度了一个训练作业，它将拥有固定数量的 CPU 机器，不会改变。即使是我们展示的最大规模的任务，也使用运行许多生产任务的集群的不到 5% 的 GPU。</li>
<li>我们使用大的训练批次大小。<strong>较小的批大小意味着更少的 GPU 内存消耗但更多的通信</strong>，因此端到端的改进将更加明显。然而，我们所有的任务几乎都使用了 GPU 内存，因此针对 all-reduce 和 PS 的加速数字是 BytePS 的下限。</li>
<li>虽然我们不能透露任何内部使用的具体模型，但显示的任务和 DNN 模型结构高度代表了生产工作负载。该代码也可公开获取以实现可重复性。</li>
<li>我们将BytePS 与最先进的PS 和allreduce 实现进行比较，无需修改。例如，我们不会将 §6.2 中提到的 RDMA 优化应用于 native-PS 和 all-reduce</li>
</ul>
<p>我们使用的集群有一个全二分带宽的 RoCEv2 网络。 所有机器都有一个 100GbE 网卡。 我们注意到 TensorFlow、PyTorch 和 MXNet 可以重叠 DNN 计算和通信 [34, 55]，因此即使端到端性能的微小改进也可以表明通信的巨大改进。</p>
<blockquote>
<p>点评：这里说的应该是非稀疏场景的情况</p>
</blockquote>
<blockquote>
<p>补充：<strong>Ro****CE</strong>是在InfiniBand Trade Association（IBTA）标准中定义的<strong>网络</strong>协议，允许通过以太网络使用RDMA。<br>
大致有三类RDMA网络，分别是Infiniband、RoCE、iWARP。其中，Infiniband是一种专为RDMA设计的网络，从硬件级别保证可靠传输 ，而RoCE 和 iWARP都是基于以太网的RDMA技术，支持相应的verbs接口。<br>
<a href="https://cloud.tencent.com/developer/article/1771431">浅析RoCE网络技术 - 云+社区 - 腾讯云</a></p>
</blockquote>
<h1 id="8-观察与讨论">8. 观察与讨论</h1>
<p>在本节中，我们将分享我们的一些观察和讨论，旨在激发未来的研究。</p>
<p>即使没有额外的 CPU 机器，BytePS 的性能也优于 all-reduce。 理论上，当没有额外的 CPU 机器可用时，all-reduce 和 BytePS 的通信时间是相同的（第 4.1 节）。 在实践中，我们观察到 BytePS 在这种情况下仍然明显优于 all-reduce。 一个原因是 BytePS 有比 all-reduce 更好的机内通信策略。 然而，即使没有机器内优化，BytePS 仍然优于 all-reduce。</p>
<p>我们假设 BytePS 比 all-reduce 具有允许更多“异步性”的优势。 All-reduce 通常需要额外的带外同步来保证跨节点的顺序一致，而 BytePS 没有这个开销。 然而，为了分析它，我们需要一个分布式分析器，它可以构建分布式训练中跨所有节点的执行和通信的完整时间线</p>
<blockquote>
<p>点评：确实没有看到在cpu上进行同步的情况</p>
</blockquote>
<p>GPU 集群调度器应该考虑动态 CPU 资源。 通过利用额外的 CPU 机器，BytePS 可以加速 DNN 训练。 由于 BytePS 可以适应任意数量的 CPU 机器，因此它具有弹性——集群调度程序可以根据实时条件为现有作业扩展或缩减 CPU 机器。 由于收敛问题[16, 74]，大多数现有的调度器将作业的 GPU 数量保持不变。 幸运的是，BytePS 中的 CPU 机器数量只影响系统性能，而不影响模型收敛。 我们计划为 BytePS 添加弹性支持，这将使 BytePS 在训练过程中动态调度 CPU 资源。</p>
<blockquote>
<p>点评：弹性调度，但是hash 索引要变，可能这里利用一致性hash</p>
</blockquote>
<p>模型并行支持。 当减少跨 GPU 的张量时，BytePS 可以加速通信。 一些模型并行方法，例如 Megatron-LM  和 MeshTensorFlow ，也依赖于 all-reduce 原语进行通信。 因此，BytePS 也可以通过替换 all-reduce 操作来加速它们。</p>
<h1 id="9-related-work">9 Related Work</h1>
<p><strong>计算加速</strong>：<br>
为了加速前向传播和反向传播，社区已经制定了许多先进的编译器和库，包括 cuDNN [10]、MKL [7]、TVM [23]、XLA [17]、Astra [64] 和其他计算图优化，例如张量融合 [14] 和图替换 [37]。 他们专注于加速 DNN 计算。 它们是 BytePS 的补充并可与 BytePS 一起使用。</p>
<blockquote>
<p>点评：与社区兼容是很重要的一点</p>
</blockquote>
<p><strong>通信加速</strong>：<br>
加速通信有几个方向：（1）梯度压缩[21, 45]被提出来减少通信流量，即使用半精度进行梯度传输，代价是潜在的精度下降。 (2) 通信调度和流水线：最近的工作探索通过基于优先级的调度和张量分区来更好地重叠计算和通信 [31, 34, 55]。 想法是张量分区可以同时进行双向通信，并且在通信期间，前几层具有更高的优先级，因为下一次迭代的 FP 需要更快地使用它们。 这些想法是对 BytePS 的补充，并且它们已集成到我们的实现中。 Pipedream [51] 增加了多个批次之间的并行性。 BytePS 还可以潜在地加速其数据并行阶段。</p>
<blockquote>
<p>点评：快手也用了梯度压缩，介绍说精度不下降</p>
</blockquote>
<p><strong>分层 all-reduce</strong>：<br>
一些工作建议在 all-reduce 期间利用分层拓扑 [24, 49]，以最小化瓶颈链接处的流量。 然而，他们仍然依赖于资源同质的假设，而忽略了 CPU 资源。 BytePS 可以通过利用异构资源来超越它们。 其实最新的NCCL包括了分层的、基于树的all-reduce，和结果差别不大。</p>
<p><strong>机内优化</strong>：Blink [68] 还通过利用 NVLinks 和 PCIe 链路上的混合传输优化了单台机器内的多个 GPU 通信。 但是，Blink 没有优化分布式训练案例，其中主要的通信瓶颈是 NIC 及其 PCIe 连接，而不是速度更快的 NVLink。 BytePS 仔细调度机器内流量以更好地利用瓶颈带宽——网卡带宽。 我们的机内设计也考虑了网卡消耗的 PCIe 带宽，而 Blink 只关注 GPU 的 PCIe 连接。</p>
<blockquote>
<p>补充：利用 GPU 间所有异构数据传输通道，实现数据聚合的最优解决方案 Blink。相比 NCCL/Horovod，Blink 提高 GPU 间数据聚合的通信效率高达 8 倍，最多可以缩短分布式机器学习模型总训练时间的 40%<br>
<a href="https://blog.csdn.net/danteLiujie/article/details/102901189">Blink:网络自适配的GPU集群通信库 - 深度学习集群_danteLiujie的专栏-CSDN博客</a></p>
</blockquote>
<p><strong>用于加速 DNN 训练的新硬件芯片或架构</strong>：<br>
最近有许多新芯片，如 TPU [38] 和 Habana [6]，专门用于 DNN 训练。 事实上，BytePS 的设计并不是针对 GPU 的，只要它们也是 PCIe 设备就应该适用于它们。 有些人还建议使用 InfiniBand 交换机 ASIC [28] 来加速 all-reduce，或使用 P4 交换机 [58, 59] 来加速 PS。 E3 [46] 利用 SmartNIC 来加速网络应用程序，并且可以通过将梯度求和从 CPU 卸载到 SmartNIC 来潜在地使 BytePS 受益。 PHub [48] 提出了一种具有定制网络配置的机架级硬件架构，例如，一台服务器上有 10 个 NIC。 BytePS 专注于在商品数据中心使用普遍可用的 CPU 和 GPU 服务器。</p>
<h1 id="10-结论">10 结论</h1>
<p>BytePS 是一个统一的分布式 DNN 训练加速系统，可在异构 GPU/CPU 集群中实现最佳通信效率。 BytePS 处理不同数量的 CPU 机器的情况，并将传统的 all-reduce 和 PS 作为其框架的两个特殊情况。 为了进一步加速 DNN 训练，<strong>BytePS 提出了 Summation Service，并将 DNN 优化器拆分为两部分：梯度求和和参数更新。</strong> 它将 CPU 友好的部分，梯度求和保留在 CPU 中，并将计算量更大的参数更新移动到 GPU。 我们已经实施了 BytePS 并解决了许多实施问题，包括<strong>影响 RDMA 性能的问题</strong>。 BytePS 已被部署、广泛使用和开源 [4]。 已经基于它开发了多个外部项目。 重现评估的工件附录位于 [3]。</p>
]]></summary>
        <content type="html"><![CDATA[<p>byteps 代码开源在 <a href="https://github.com/bytedance/byteps">byteps repo</a>.<br>
论文地址 <a href="https://www.usenix.org/conference/osdi20/presentation/jiang">A Unified Architecture for Accelerating Distributed DNN Training in Heterogeneous GPU/CPU Clusters | USENIX</a></p>
<p>《用于在异构 GPU/CPU 集群中加速分布式 DNN 训练的统一架构》</p>
<h1 id="摘要">摘要</h1>
<p>运行 DNN 训练作业的数据中心集群本质上是异构的。 他们拥有用于计算的 GPU 和 CPU 以及用于分布式训练的网络带宽。 然而，现有的分布式 DNN 训练架构，all-reduce 和参数服务器 (PS)，无法充分利用这种异构资源。 在本文中，我们提出了一种新的分布式 DNN 训练架构，称为 BytePS。 BytePS 可以利用集群中的空闲 CPU 和带宽资源来加速在 GPU 上运行的分布式 DNN 训练任务。它提供了一个被证明是最优和统一的通信框架——现有的 all-reduce 和 PS 成为 BytePS 的两个特例。 为了在实践中实现已证明的最优性，BytePS 进一步拆分了参数优化器的功能。 它引入了用于聚合梯度的<strong>summation服务</strong>抽象，这对于所有优化器来说都是通用的。</p>
<blockquote>
<p>点评：主要的优化在于通信上。</p>
</blockquote>
<p><strong>summation服务</strong>可以通过AVX指令加速，可以在CPU上高效运行，而DNN模型相关的优化器算法在GPU上运行，进行计算加速。BytePS 可以加速主要框架的 DNN 训练，包括 TensorFlow、PyTorch 和 MXNet。 对于具有多达 256 个 GPU 的代表性 DNN 训练任务，BytePS 的性能分别比最先进的开源 all-reduce 和 PS 高出 84% 和 245%。</p>
<h1 id="1-介绍">1. 介绍</h1>
<p>近年来，深度神经网络 (DNN) 的研究经历了复兴。 DNN 为计算机视觉 、语音识别和合成 、自然语言处理 (NLP) 和许多其他领域带来了突破。 训练这些 DNN 模型通常需要大量的算术计算资源。 因此，首选 GPU。 为了运行许多此类任务并实现高资源利用率，引入了具有数千个或更多 GPU 的大型 GPU 集群。</p>
<p>这样的GPU集群不仅有GPU，还有CPU和高速网络。 GPU 机器通常也有高端 CPU。 也可能有仅 CPU 的机器用于训练数据的预处理和生成，比如在强化学习里面。这些 GPU/CPU 机器通过高速以太网或 Infiniband 网络连接，以方便分布式训练。 根据我们操作生产 GPU 集群的经验（第 3.1 节）和其他人的最新文献，通常可以更好地利用 GPU，而通常有空闲的 CPU 和带宽资源。</p>
<p>有两个主要的分布式训练架构系列，all-reduce 和参数服务器 (PS) 。 它们都基于数据并行性 。 在使用 all-reduce 的任务中，只涉及 GPU 机器。 在一次迭代中，GPU 独立计算模型参数的梯度，然后使用 all-reduce 原语聚合梯度。 在 PS 任务中，GPU 机器和 CPU 机器都可以使用。 与 all-reduce 不同的是，梯度被发送到 PS，它通常在 CPU 机器上运行并聚合接收到的梯度。然后  PS 运行某些 DNN 训练优化器，例如 SGD  或 Adam 并发回更新后的模型。 对于 all-reduce 和 PS，每次迭代都会发生上述情况，直到训练结束。</p>
<p>All-reduce 和 PS 在理论和实践上都大不相同。 给定一组没有额外 CPU 机器的 GPU 机器，经证明 all-reduce 是带宽最优的。 然而，随着 CPU 和带宽资源的增加，all-reduce 的最优性不再成立——我们发现，理论上，PS 可以通过利用额外的 CPU 机器来帮助 GPU 机器提供更好的性能。 这似乎是加速 DNN 训练的好机会，因为 GPU 集群确实有空闲的 CPU 和带宽资源。 不幸的是，在实践中，由于多种设计原因，所有现有 PS 的性能都很差，我们将在本文中很快看到。 因此看到分布式 DNN 训练速度记录以 all-reduce 为主也就不足为奇了</p>
<blockquote>
<p>点评：所有byteps 是对ps进行优化，利用好 cpu 资源</p>
</blockquote>
<p>因此，我们有动力设计 BytePS，这是一种在理论上和实践中都具有最佳通信性能的架构。 从根本上说，all-reduce 和 PS 理论上仅在非常特定的 GPU/CPU 设置中是最佳的，而对于更通用的设置则不是最佳的，例如，<strong>有一些有限的额外 CPU 资源。 通过仔细分配流量负载</strong>，BytePS 统一了 PS 或 all-reduce 理论上最优的情况，并将最优性推广到任何给定数量的具有<strong>不同 PCIe/NVLink 配置的 GPU/CPU 机器</strong>，并提供分析证明。</p>
<p>最重要的是，BytePS 通过消除现有 PS 设计中的瓶颈，将其实际性能推向接近理论极限。 对于快速的高速网络，我们发现 CPU 对于成熟的 DNN 优化器来说不够快。 我们引入了一个新的抽象，Summation Service，来解决这个问题。 我们将<strong>优化器拆分为梯度聚合和参数更新</strong>。 我们在 CPU 上运行的 <strong>Summation Service 中保持梯度聚合</strong>，并将计算密集度更高的参数更新移动到 GPU。 此外，在实施中，我们结合了先前工作中的<strong>流水线和优先级调度</strong>的想法，并解决了多个与 RDMA 相关的性能问题。</p>
<blockquote>
<p>点评：summation service 跑在cpu进行梯度聚合，因为这个是网络IO</p>
</blockquote>
<p>作为 all-reduce 和 PS 的直接替代品，BytePS 旨在在不改变 DNN 算法或其准确性的情况下加速分布式训练。 之前在 allreduce 和 PS 之上的工作，如张量压缩 ，可以直接应用于 BytePS。 我们的 BytePS 实现支持流行的 DNN 训练框架，包括 TensorFlow 、PyTorch  和 MXNet，以及类似 Horovod 的  API 和原生 API。</p>
<blockquote>
<p>点评：张量压缩的两篇论文也可以看一看[21, 45]</p>
</blockquote>
<p>本文做出以下贡献：</p>
<ul>
<li>我们为异构 GPU/CPU 集群设计了一种新的分布式 DNN 训练架构 BytePS。 借助集群中的备用 CPU 内核和网络带宽，BytePS 可以<strong>实现 DNN 训练加速的通信优化</strong>。 BytePS 提供了一个统一的框架，其中包括 all-reduce 和 PS 作为两种特殊情况。</li>
<li>我们进一步<strong>优化了机内通信</strong>。 我们解释了 GPU 机器中多样化和复杂的拓扑结构，并提出了最佳策略和原则。</li>
<li>我们提出了<strong>求和服务</strong>，它通过保持 CPU 中运行的梯度求和来加速 DNN 优化器，并将计算密集度更高的参数更新移动到 GPU。 这消除了原始 PS 设计中的 CPU 瓶颈</li>
</ul>
<p>作为主要的在线服务提供商，我们在内部部署了 BytePS，并将其广泛用于 DNN 训练。 我们在生产数据中心使用六个 DNN 模型和三个训练框架来评估 BytePS。 结果表明，使用 256 个 GPU，BytePS 始终优于现有的 allreduce 和 PS 解决方案，分别高达 84% 和 245%。 我们还发布了一个开源版本，吸引了成千上万的开源社区、几家顶级公司和多个研究小组的兴趣。</p>
<blockquote>
<p>点评：所以byteps 的主要发力点是在通信优化上</p>
</blockquote>
<h1 id="2-背景">2. 背景</h1>
<h2 id="21-分布式-dnn-训练">2.1 分布式 DNN 训练</h2>
<p>DNN 模型由许多参数组成。 DNN 训练包括三个主要步骤：（1）前向传播（FP），它接收一批训练数据，通过 DNN 模型进行传播，并计算损失函数； （2）反向传播（BP），它使用损失值来计算每个参数的梯度； (3) 参数更新，它使用聚合梯度通过某个优化器（例如，SGD、Adam 等）更新参数。 训练一个 DNN 用以上三个步骤迭代地细化模型参数，直到损失函数达到其最小值。</p>
<p>最重要的是，用户可以选择运行分布式训练。 最流行的分布式 DNN 训练方法是数据并行性，它将数据集划分到多个分布式计算设备（通常是 GPU），而每个 GPU 拥有完整的 DNN 模型。 由于每个GPU输入的数据不同，BP生成的梯度也会不同。 因此，数据并行要求所有 GPU 在每次训练迭代期间同步。</p>
<p>在大型企业或公共云中，用户经常在共享的 GPU 集群中运行这些 DNN 训练任务。 这样的集群是由成百上千的 GPU 机器构建的，这些机器通过高速 RDMA 网络连接。 这些 GPU 机器通常具有多个 GPU、数十个 CPU 内核、数百 GB 的 DRAM 和一到几个 100Gb/s NIC。 这些集群同时运行许多训练作业，其中许多作业集中使用 GPU 而不是大量使用 CPU。 DNN 集群上的公共数据集表明 50% 的主机 CPU 利用率低于 30%。</p>
<p>对于分布式训练，有两大类数据并行方法，即 all-reduce 和 Parameter Server (PS)。 下面介绍all-reduce和PS，分析它们的通信开销。 我们假设我们有 n 个 GPU 机器用于数据并行训练工作。 DNN 模型大小为 M 字节。 网络带宽为 B。</p>
<h2 id="22-all-reduce">2.2 All-reduce</h2>
<p>起源于 HPC 社区，all-reduce 在 GPU 本地更新自己的参数之前以集体的方式聚合每个 GPU 的梯度。 在 all-reduce 中，不涉及额外的 CPU 机器。 Ring 是最流行的 all-reduce 算法。 All-reduce经过多年优化，大部分最先进的训练速度记录都是使用all-reduce实现的，包括经典的基于CNN的ImageNet任务，基于RNN的语言建模任务，以及 BERT 的预训练。</p>
<p>图 1 显示了三个节点的基于环的 all-reduce 的示例。 我们可以将 all-reduce 操作分解为 <strong>reduce-scatter 和 all-gather</strong>。 Reduce-scatter（图1（a））将整个M个字节分成n个部分，并使用n个具有不同起点和终点的环分别reduce n个部分。 每个节点将发送 (n-1)M/n 个流量，因为每个节点充当仅 1 个环的最后一个节点，因此发送 0，而对于其他 n1 个环中的每一个，它必须发送 M/n 个字节。</p>
<figure data-type="image" tabindex="1"><img src="https://tva1.sinaimg.cn/large/008i3skNgy1gv5klx1p25j60w00a8gmy02.jpg" alt="Pasted image 20210905173924" loading="lazy"></figure>
<p>接下来，all-gather 要求每个节点使用环向所有其他 (n-1) 节点广播其减少的部分。 最后，所有节点都具有完全减少的相同数据（图 1（c））。 与reduce-scatter 类似，每个节点也在此操作期间发送(n-1)M/n 个出口流量。</p>
<p>将这两个步骤加在一起，在 all-reduce 操作中，每个节点向（和从）网络发送（和接收）2(n-1)M/n 个流量。 对于 B 网络带宽，所需时间为 2(n-1)M/nB，这在具有统一链路带宽的拓扑中被证明是最佳的，假设没有额外的资源。</p>
<p>在具有非均匀链路带宽的分层拓扑中，最佳分层策略至少需要<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>2</mn><mo>(</mo><msup><mi>n</mi><mi>l</mi></msup><mo>−</mo><mn>1</mn><mo>)</mo><mi>M</mi><mi mathvariant="normal">/</mi><msup><mi>n</mi><mi>l</mi></msup><msup><mi>B</mi><mi>l</mi></msup></mrow><annotation encoding="application/x-tex">2(n^l-1)M/n^lB^l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.099108em;vertical-align:-0.25em;"></span><span class="mord">2</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">n</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.099108em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mclose">)</span><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="mord">/</span><span class="mord"><span class="mord mathdefault">n</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span></span></span></span></span></span></span></span> 通信时间。其中<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>B</mi><mi>l</mi></msup></mrow><annotation encoding="application/x-tex">B^l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.849108em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span></span></span></span></span></span></span></span> 是最慢的链路带宽，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>n</mi><mi>l</mi></msup></mrow><annotation encoding="application/x-tex">n^l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.849108em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault">n</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span></span></span></span></span></span></span></span> 是具有最慢链路的节点数。 在分布式 DNN 训练中，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>n</mi><mi>l</mi></msup></mrow><annotation encoding="application/x-tex">n^l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.849108em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault">n</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span></span></span></span></span></span></span></span>通常是 GPU 机器的数量，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>B</mi><mi>l</mi></msup></mrow><annotation encoding="application/x-tex">B^l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.849108em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span></span></span></span></span></span></span></span> 通常是每台机器的网络带宽。 为简单起见且不影响我们的分析，下面我们假设每台机器只有一个 GPU，并通过相同的网络带宽连接，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>n</mi><mo>=</mo><msup><mi>n</mi><mi>l</mi></msup><mo separator="true">,</mo><mi>B</mi><mo>=</mo><msup><mi>B</mi><mi>l</mi></msup></mrow><annotation encoding="application/x-tex">n=n^l, B=B^l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">n</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.043548em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault">n</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.849108em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span></span></span></span></span></span></span></span>。</p>
<p>All-reduce 无法利用额外的非工作节点，因为它是为同构设置而设计的。 接下来，我们将展示 2(n-1)M/nB 通信时间对于额外的 CPU 机器不再是最佳的。</p>
<h2 id="23-parameter-server-ps">2.3 Parameter Server (PS)</h2>
<p>PS 架构包含两个角色：worker 和 PS。 Worker 通常在 GPU 机器上运行，执行 FP 和 BP，并将梯度推送到 PS。 PS聚合来自不同worker的梯度并更新参数。 最后，worker 从 PS 中拉取最新的参数并开始下一次迭代。 根据我们的行业经验，PS 流程通常在 CPU 上运行，因为它们具有成本效益。 由于 GPU（和 GPU 内存）比 CPU 贵得多，我们希望 GPU 专注于计算密集度最高的任务，而不是存储模型参数。</p>
<p>PS有两种放置策略。 一种是非共置模式（图 2（a）），其中 PS 进程部署在专用 CPU 机器上，与 GPU 机器分开。 假设我们有 k 个 CPU 机器，DNN 模型将被分成 k 个部分并分别存储在 k 个机器上。 在每次迭代中，每个 GPU 工作人员必须发送 M 字节梯度并接收回 M 字节参数。 每台 CPU 机器必须从 GPU 工作人员处接收总共 nM/k 梯度并发送回 nM/k 参数。</p>
<p>假设 k = n，理论上 PS 将比 all-reduce 更快，如表 1 所述。实际上，PS 在这种设置下是最佳通信，因为 M 是每个 GPU 机器必须发送和接收的绝对下限。 但是，CPU 机器越少（k 越小），CPU 机器上的通信时间 nM/kB 就会增加，如果 k &lt;n/2，则变得比 all-reduce 慢。 GPU 机器的网络带宽将被利用不足，因为 CPU 机器将成为通信瓶颈。</p>
<blockquote>
<p>点评：k &lt; n/2, 通信时间就慢了</p>
</blockquote>
<p><em>表 1：每次训练迭代所需的理论通信时间。 n 是 GPU 机器的数量。 k 是附加 CPU 机器的数量。 M 是模型尺寸。 B 是网络带宽。 我们将重新审视最优</em>。<br>
<img src="https://tva1.sinaimg.cn/large/008i3skNgy1gv5kn3z2jcj60xg06agmo02.jpg" alt="Pasted image 20210905184807" style="zoom:80%;" /></p>
<p>另一种策略是共置模式（图 2（b）），它不使用任何 CPU 机器。 相反，它会在每个 GPU 工作线程上启动一个 PS 进程并重用其备用 CPU 资源。 同一台机器上的 PS 和 GPU 工作器将通过环回流量（loopback traffic）进行通信。 在这种情况下，很容易计算出通信时间与all-reduce相同（表1）。</p>
<blockquote>
<p>点评：这个共置模式的提法，之前没看到过</p>
</blockquote>
<h2 id="all-reduce-vs-ps">All-reduce vs. PS.</h2>
<p>他们有不同的通信模式。 PS使用二部图。 非共置模式的 PS 可以利用额外的 CPU 和带宽资源来帮助 GPU 机器，同时可能未充分利用 GPU 机器的资源。 Colocated PS 和 all-reduce 更好地利用了 GPU Worker 资源，同时不能使用额外的 CPU 机器。</p>
<p>另一个区别是 PS 支持异步训练，它允许 GPU worker 以不同的速度运行并减轻掉队者的影响，而 all-reduce 不支持它。 但是，异步训练不太受欢迎，因为它会减慢模型收敛速度。 我们将在本文中主要关注同步训练，同时在 §5 中简要介绍异步训练。</p>
<h1 id="3-动机和-byteps-架构">3. 动机和 BytePS 架构</h1>
<h2 id="31-动机">3.1 动机</h2>
<p>在我们内部 GPU 集群中部署 BytePS 之前，我们的用户大多使用 all-reduce 作为分布式训练架构，因为它比现有的 PS 设计具有更高的性能。 其余用户选择 PS 用于异步训练可接受或更可取的任务。 凭借多年在加速 DNN 任务和提高资源利用率方面的经验和努力，我们有以下观察。</p>
<h3 id="机会">机会：</h3>
<p><strong>生产 GPU 集群中有空闲的 CPU 和带宽</strong>。</p>
<p>大规模 GPU 集群同时运行大量作业，其中许多作业不会大量使用 CPU 或网络带宽。 图 3 显示了从我们拥有数千个 GPU 的 GPU 集群之一收集的 3 个月跟踪记录。 GPU在那个时期的利用率很高（高峰期接近96%的分配率）。 我们发现，至少有 55%-80% 的 GPU 机器被分配为 GPU worker，用于至少一项分布式训练任务。 这使得 20%-45% 的 GPU 机器的网络带宽未使用，因为它们正在运行非分布式作业。集群范围内的平均 CPU 利用率仅为 20%-35% 左右。 这与 Microsoft 先前工作中的发现一致。</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNgy1gv5knmkdpcj60xe0cs0uk02.jpg" alt="Pasted image 20210905192021" style="zoom:80%;" />
<p>这一观察结果与 §2.1 中的 all-reduce vs. noncolocated PS 分析相结合，启发了我们——如果我们能更好地利用这些备用 CPU 和带宽，就有可能加速在给定 GPU 上运行的分布式训练作业。</p>
<h3 id="现有的-all-reduce-和-ps-架构存在不足">现有的 all-reduce 和 PS 架构存在不足</h3>
<p>不幸的是，§2.1 中的分析也表明 all-reduce 和 PS 有一个共同的问题：它们没有很好地利用额外的 CPU 和带宽资源。 All-reduce 和 colocated PS 仅使用 GPU worker 上的资源，而非 colocated PS 可能无法充分利用 GPU worker 上的 CPU core 和 NIC 带宽。 前者仅在 k = 0 时通信最优，而后者仅在 k = n 时最优。 当 CPU 机器 k 的数量为 0 &lt; k &lt; n 时，两者都不是最优的。 我们将进一步分析放到§4.1。 在这里，我们通过一个实验来展示现有的 all-reduce 和 PS 的端到端性能。</p>
<blockquote>
<p>点评：allreduce用不了 cpu， ps 在cpu机器少的时候存在热点</p>
</blockquote>
<figure data-type="image" tabindex="2"><img src="https://tva1.sinaimg.cn/large/008i3skNgy1gv5kok1hwcj60vs0gywhm02.jpg" alt="Pasted image 20210905221007" loading="lazy"></figure>
<p>图 4 显示了使用 32 个 V100 GPU（4 个 GPU 机器）和 100GbE RDMA 网络的 VGG-16 的训练速度。 每个 GPU 的批量大小为 32 张图像。 我们运行最新的 MXNet 原生 PS RDMA 实现 和最流行的 all-reduce 库之一的 NCCL-2.5.7 。 我们还测试了 TensorFlow 的原生 PS，并得到了类似的结果。 我们为每个设置改变了额外 CPU 机器的数量。 All-reduce 图是平坦的，因为额外的 CPU 机器没有用，而 PS 即使有额外的 CPU 机器也有最差的性能。 两者都远非最佳。 即使使用 ByteScheduler [55]，这是一种可以提高通信性能的最先进技术，但 all-reduce 和 PS 都离线性缩放还很远，即单 GPU 训练速度的 32倍。</p>
<blockquote>
<p>点评：看上去byte scheduler 调度方式，是对通信过程的流程调度。</p>
</blockquote>
<p>这是因为 ByteScheduler 工作在 PS 或 all-reduce 之上，因此具有相同的限制。 BytePS 在任何给定数量的 CPU 机器上都优于以上所有内容（更多见第 7 节）。</p>
<h3 id="我们的方案-byteps">我们的方案: BytePS.</h3>
<p>它是分布式 DNN 训练的统一架构，可以<strong>利用空闲 CPU 和带宽资源</strong>。 它实现了以下目标。</p>
<p>首先，BytePS 始终是与集群调度程序<strong>分配的任何额外 CPU 和带宽资源</strong>（即 0 &lt;= k &lt;= n）的最佳通信。 在实践中，空闲资源的数量可以是动态的（图 3），因此 BytePS 必须很好地适应。 此外，GPU 机器的硬件设置可以多种多样，尤其是内部 PCIe 或 NVLink 拓扑。 BytePS 也被证明是机器内通信的最佳选择。 All-reduce 和 PS，当它们是最佳通信时，是 BytePS（第 4 节）的两种特殊情况。</p>
<p>其次，BytePS 可以实现非常接近理论最优的通信时间。 这很重要，如现有 PS 案例所示——PS 性能远未达到其理论极限。 我们发现原始 PS 设计有几个实现瓶颈（我们将在第 6 节中讨论）。 但即使去除了所有瓶颈，PS 性能仍然不如最优。 这导致了 BytePS 的第二个设计贡献：<strong>求和服务。 我们发现在 CPU 上运行完整的优化器可能是一个瓶颈。 我们对优化器的计算进行划分，只对 CPU 进行求和</strong>。 我们将在 §5 中阐述这种设计的基本原理。</p>
<p>所有 BytePS 设计都适用于 DNN 训练。 因此，BytePS 可以加速各种 DNN 训练框架，包括 TensorFlow、PyTorch 和 MXNet。 我们从介绍 BytePS 的架构开始。</p>
<h2 id="32-架构概述">3.2 架构概述</h2>
<figure data-type="image" tabindex="3"><img src="https://tva1.sinaimg.cn/large/008i3skNgy1gv5kp2m9xzj60x40dcq4z02.jpg" alt="Pasted image 20210905223306" loading="lazy"></figure>
<p><em>图 5：BytePS 架构。 实线：CPU 机器和 GPU 机器之间的连接。 虚线：GPU 机器内部的数据流。</em></p>
<p>图 5 显示了 BytePS 的架构。 BytePS 有两个主要模块——通信服务（CS）和求和服务（SS）。 在 BytePS 中，我们旨在利用任何 CPU 资源，无论是在 GPU 机器上还是 CPU 机器上，以实现最佳通信效率。 这是由 SS 实现的，它运行在每台机器的 CPU 上，包括 CPU 机器和 GPU 机器。 CPU 机器不一定是实际的仅使用 CPU 的机器。 例如，我们的内部集群调度程序可以在运行非分布式作业并具有备用 CPU 内核和网络带宽的 GPU 机器上分配 CPU。 这提高了整体集群资源利用率。</p>
<p>SS 的另一个重要特性是它比运行成熟 DNN 算法优化器的常见 PS 服务器进程简单得多。 相比之下，SS 只负责接收 CS 发送的张量，将张量汇总并发送回 CS。 <strong>另一个模块 CS 负责在多个（如果有）本地 GPU 之间内部同步张量，并与 SS 进行外部通信</strong>。 每次训练迭代，每个 CS 必须向 SS 发送总共 M 个字节（DNN 模型大小）并从 SS 接收 M 个字节。 在同步分布式训练中，张量是模型梯度。</p>
<blockquote>
<p>点评：cs 负责gpu间同步</p>
</blockquote>
<p>CS 包含了 BytePS 的几个设计点。 首先，它决定每个 SS（内部和外部）的流量。 负载分配策略基于我们对最佳通信策略的分析（第 4.1 节）。 其次，它根据 GPU 机器的不同内部 GPU 和 NIC 拓扑（第 4.2 节）选择最佳的局部张量聚合策略。 最后，CS 和 SS 都应针对现代高速数据中心中的 RDMA 进行优化（第 6.2 节）。</p>
<blockquote>
<p>点评：所以有几个关键点：负载均衡通信策略，张量聚合策略，RDMA优化，cs 和 ss 和交互</p>
</blockquote>
<p>这种架构使 BytePS 能够灵活地利用任意数量的额外 CPU 资源和网络带宽。 当 CPU 机器数为 0 时，即 k = 0，通信将回退到仅在 GPU 机器上使用 SS。 当 CPU 机器数与 GPU 机器数相同时，BytePS 与非 colocated PS 的通信最佳。 在其他情况下，BytePS 可以同时利用所有机器上的 SS。 事实上，我们的分析结果将揭示与任意数量的 CPU 机器的最佳通信策略，而 PS 和 all-reduce 只是整个问题空间中的两个特定点。</p>
<blockquote>
<p>点评：其实就是借用gpu机器上的cpu，来达到最优ps数量</p>
</blockquote>
<h1 id="4-byteps-通信设计">4. BytePS 通信设计</h1>
<h2 id="41-机器间通信">4.1 机器间通信</h2>
<p>在 BytePS 中，所有的网络通信都是在 CS 和 SS 之间进行的。 为了防止瓶颈节点拖慢整个系统，我们必须平衡所有机器的通信时间。 在下文中，我们假设网络具有完整的二分带宽，这是深度学习集群中的常见做法。 我们还假设由于新引入的 RDMA 拥塞控制算法，例如 DCQCN，可以充分利用完整的二分带宽。</p>
<blockquote>
<p>点评：所以 byteps 依赖 rdma 的gpu 集群</p>
</blockquote>
<p>在每台 CPU 机器上，其 SS 的工作负载总和决定了网络流量。 例如，如果一个 SS 负责汇总 x% 的 DNN 模型，则 CPU 机器将在每次训练迭代期间向每台 GPU 机器发送和接收 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi><mi mathvariant="normal">%</mi><mi>M</mi></mrow><annotation encoding="application/x-tex">x \% M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.80556em;vertical-align:-0.05556em;"></span><span class="mord mathdefault">x</span><span class="mord">%</span><span class="mord mathdefault" style="margin-right:0.10903em;">M</span></span></span></span> 字节的流量。 但是，GPU 机器的网络流量是由其上运行的 CS 和 SS 的组合决定的。 由于这种差异，BytePS根据是运行在CPU机器还是GPU机器上将SS分为SSCPU和SSGPU。</p>
<p>为了最小化通信时间，BytePS 将 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>M</mi><mrow><mi>S</mi><msub><mi>S</mi><mrow><mi>C</mi><mi>P</mi><mi>U</mi></mrow></msub></mrow></msub></mrow><annotation encoding="application/x-tex">M_{SS_{CPU}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.933635em;vertical-align:-0.250305em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05764em;">S</span><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3567071428571427em;margin-left:-0.05764em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.07153em;">C</span><span class="mord mathdefault mtight" style="margin-right:0.13889em;">P</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">U</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.14329285714285717em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.250305em;"><span></span></span></span></span></span></span></span></span></span> 字节总和工作负载分配给每个 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>S</mi><msub><mi>S</mi><mrow><mi>C</mi><mi>P</mi><mi>U</mi></mrow></msub></mrow><annotation encoding="application/x-tex">SS_{CPU}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.07153em;">C</span><span class="mord mathdefault mtight" style="margin-right:0.13889em;">P</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">U</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 。  <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>M</mi><mrow><mi>S</mi><msub><mi>S</mi><mrow><mi>C</mi><mi>P</mi><mi>U</mi></mrow></msub></mrow></msub></mrow><annotation encoding="application/x-tex">M_{SS_{CPU}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.933635em;vertical-align:-0.250305em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05764em;">S</span><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3567071428571427em;margin-left:-0.05764em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.07153em;">C</span><span class="mord mathdefault mtight" style="margin-right:0.13889em;">P</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">U</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.14329285714285717em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.250305em;"><span></span></span></span></span></span></span></span></span></span> 在方程式1中给出。 其中 k &lt;= 1 是 CPU 机器的数量，n &lt;= 2 是 GPU 机器的数量，k &lt;= n。 在这些限制之外，BytePS 的通信时间回退到像 PS（当 k &gt; n）和 all-reduce（当 k = 0）这样的简单解决方案，如 §4.1.1 所示。</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNgy1gv5kpjgf9lj60o203yq2z02.jpg" alt="Pasted image 20210906105230" style="zoom:80%;" />
<p>类似地，BytePS 将 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>M</mi><mrow><mi>S</mi><msub><mi>S</mi><mrow><mi>G</mi><mi>P</mi><mi>U</mi></mrow></msub></mrow></msub></mrow><annotation encoding="application/x-tex">M_{SS_{GPU}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.933635em;vertical-align:-0.250305em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05764em;">S</span><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3567071428571427em;margin-left:-0.05764em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">G</span><span class="mord mathdefault mtight" style="margin-right:0.13889em;">P</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">U</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.14329285714285717em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.250305em;"><span></span></span></span></span></span></span></span></span></span> 字节分配给每个 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>S</mi><msub><mi>S</mi><mrow><mi>G</mi><mi>P</mi><mi>U</mi></mrow></msub></mrow><annotation encoding="application/x-tex">SS_{GPU}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">G</span><span class="mord mathdefault mtight" style="margin-right:0.13889em;">P</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">U</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 。</p>
<figure data-type="image" tabindex="4"><img src="https://tva1.sinaimg.cn/large/008i3skNgy1gv5kq12up8j60oq04kmx602.jpg" alt="Pasted image 20210906105725" loading="lazy"></figure>
<p>等式 1 和等式 2 显示了最适合最小化通信时间的工作负载分配策略。 分析在 §4.1.1 中。 在实践中，DNN 模型由大小可变的张量组成，可能无法让我们完美地分配工作负载。 BytePS 使用近似方法。 它将张量分成不大于 4MB 的小部分。然后，所有 CS 一致地索引每个部分并将索引散列到 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>[</mo><mn>0</mn><mo separator="true">,</mo><msup><mi>n</mi><mn>2</mn></msup><mo>+</mo><mi>k</mi><mi>n</mi><mo>−</mo><mn>2</mn><mi>k</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">[0,n^2+kn-2k)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.064108em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">n</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.77777em;vertical-align:-0.08333em;"></span><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="mord mathdefault">n</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">2</span><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="mclose">)</span></span></span></span> 的范围内。 CS 将根据散列值向 SS 发送和接收张量，并根据等式 1 和等式 2 估算概率。一致的索引和散列保证来自所有 GPU 的相同部分将被发送到同一个 SS 并由其处理。</p>
<blockquote>
<p>我们发现 4MB 分区大小在我们的环境中运行良好，但 BytePS 允许用户调整分区大小值。</p>
</blockquote>
<blockquote>
<p>点评：tensor切分小块，索引按什么顺序索引呢，如果是稀疏的场景该怎么办呢</p>
</blockquote>
<h3 id="411-通信效率分析">4.1.1 通信效率分析</h3>
<p>接下来，我们介绍BytePS的通信时间分析。 为了简化分析，我们假设模型大小 M 远大于分区大小（在我们的例子中为 4MB）。 分区使 BytePS 不仅可以更好地平衡求和工作负载，还可以通过流水线发送和接收来很好地利用双向网络带宽，如 [34, 55] 所示。 因此，我们进一步假设发送和接收整个 M 字节可以完全重叠，开销可以忽略不计。 我们有以下结果。</p>
<p><strong>推理 1</strong>. 等式 1 和等式 2 给出的 SS 工作负载分配对于最小化通信时间是最佳的。</p>
<p>证明。 我们首先考虑 GPU 机器的网络流量。 它运行一个 CS 模块和一个 SS 模块。 CS 总共应该发送和接收 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.10903em;">M</span></span></span></span> 个字节。 但是，当它与同一 GPU 机器上的 SS 通信时，流量不会通过网络。 因此，CS 模块将发送和接收 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>M</mi><mo>−</mo><msub><mi>M</mi><mrow><mi>S</mi><msub><mi>S</mi><mrow><mi>G</mi><mi>P</mi><mi>U</mi></mrow></msub></mrow></msub></mrow><annotation encoding="application/x-tex">M-M_{SS_{GPU}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.933635em;vertical-align:-0.250305em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05764em;">S</span><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3567071428571427em;margin-left:-0.05764em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">G</span><span class="mord mathdefault mtight" style="margin-right:0.13889em;">P</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">U</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.14329285714285717em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.250305em;"><span></span></span></span></span></span></span></span></span></span> 个字节。 GPU机器上的SS模块必须从其他<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">n-1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="mord mathdefault">n</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span>个GPU机器接收和发送<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>M</mi><mrow><mi>S</mi><msub><mi>S</mi><mrow><mi>G</mi><mi>P</mi><mi>U</mi></mrow></msub></mrow></msub></mrow><annotation encoding="application/x-tex">M_{SS_{GPU}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.933635em;vertical-align:-0.250305em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05764em;">S</span><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3567071428571427em;margin-left:-0.05764em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">G</span><span class="mord mathdefault mtight" style="margin-right:0.13889em;">P</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">U</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.14329285714285717em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.250305em;"><span></span></span></span></span></span></span></span></span></span>, 总共 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><mi>n</mi><mo>−</mo><mn>1</mn><mo>)</mo><msub><mi>M</mi><mrow><mi>S</mi><msub><mi>S</mi><mrow><mi>G</mi><mi>P</mi><mi>U</mi></mrow></msub></mrow></msub></mrow><annotation encoding="application/x-tex">(n-1)M_{SS_{GPU}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathdefault">n</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.000305em;vertical-align:-0.250305em;"></span><span class="mord">1</span><span class="mclose">)</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05764em;">S</span><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3567071428571427em;margin-left:-0.05764em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">G</span><span class="mord mathdefault mtight" style="margin-right:0.13889em;">P</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">U</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.14329285714285717em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.250305em;"><span></span></span></span></span></span></span></span></span></span> 。将它们加在一起，网络带宽为 B 的 GPU 机器需要通信时间 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>t</mi><mi>g</mi></msub></mrow><annotation encoding="application/x-tex">t_g</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9011879999999999em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">g</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span>：</p>
<figure data-type="image" tabindex="5"><img src="https://tva1.sinaimg.cn/large/008i3skNgy1gv5kzkcgx3j60oo04s74b02.jpg" alt="Pasted image 20210906121509" loading="lazy"></figure>
<p>同样，如果 k &gt; 0，我们可以得到网络带宽为 B 的 CPU 机器需要通信时间 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>t</mi><mi>c</mi></msub></mrow><annotation encoding="application/x-tex">t_c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.76508em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>：</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>t</mi><mi>c</mi></msub><mo>=</mo><msub><mi>M</mi><mrow><mi>S</mi><msub><mi>S</mi><mrow><mi>C</mi><mi>P</mi><mi>U</mi></mrow></msub></mrow></msub><mi mathvariant="normal">/</mi><mi>B</mi></mrow><annotation encoding="application/x-tex">t_c = M_{SS_{CPU}}/B
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.76508em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.000305em;vertical-align:-0.250305em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05764em;">S</span><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3567071428571427em;margin-left:-0.05764em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.07153em;">C</span><span class="mord mathdefault mtight" style="margin-right:0.13889em;">P</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">U</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.14329285714285717em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.250305em;"><span></span></span></span></span></span></span><span class="mord">/</span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span></span></span></span></span></p>
<p>此外，所有 SS 工作量的总和应等于总模型大小。</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>M</mi><mo>=</mo><mi>k</mi><msub><mi>M</mi><mrow><mi>S</mi><mi>S</mi><mrow><mi>C</mi><mi>P</mi><mi>U</mi></mrow></mrow></msub><mo>+</mo><mi>n</mi><msub><mi>M</mi><mrow><mi>S</mi><mi>S</mi><mrow><mi>G</mi><mi>P</mi><mi>U</mi></mrow></mrow></msub></mrow><annotation encoding="application/x-tex">M = kM_{SS{CPU}} +nM_{SS{GPU}}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05764em;">S</span><span class="mord mathdefault mtight" style="margin-right:0.05764em;">S</span><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.07153em;">C</span><span class="mord mathdefault mtight" style="margin-right:0.13889em;">P</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">U</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord mathdefault">n</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05764em;">S</span><span class="mord mathdefault mtight" style="margin-right:0.05764em;">S</span><span class="mord mtight"><span class="mord mathdefault mtight">G</span><span class="mord mathdefault mtight" style="margin-right:0.13889em;">P</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">U</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span></p>
<p>从方程 5 可以看出，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>M</mi><mrow><mi>S</mi><msub><mi>S</mi><mrow><mi>C</mi><mi>P</mi><mi>U</mi></mrow></msub></mrow></msub></mrow><annotation encoding="application/x-tex">M_{SS_{CPU}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.933635em;vertical-align:-0.250305em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05764em;">S</span><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3567071428571427em;margin-left:-0.05764em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.07153em;">C</span><span class="mord mathdefault mtight" style="margin-right:0.13889em;">P</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">U</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.14329285714285717em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.250305em;"><span></span></span></span></span></span></span></span></span></span> 越大，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>M</mi><mrow><mi>S</mi><msub><mi>S</mi><mrow><mi>G</mi><mi>P</mi><mi>U</mi></mrow></msub></mrow></msub></mrow><annotation encoding="application/x-tex">M_{SS_{GPU}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.933635em;vertical-align:-0.250305em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05764em;">S</span><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3567071428571427em;margin-left:-0.05764em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">G</span><span class="mord mathdefault mtight" style="margin-right:0.13889em;">P</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">U</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.14329285714285717em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.250305em;"><span></span></span></span></span></span></span></span></span></span> 越小。 因此，当 n 2 时，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>t</mi><mi>c</mi></msub></mrow><annotation encoding="application/x-tex">t_c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.76508em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 越大，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>t</mi><mi>g</mi></msub></mrow><annotation encoding="application/x-tex">t_g</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9011879999999999em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">g</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span>  越小（或者如果 n = 2，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>t</mi><mi>g</mi></msub></mrow><annotation encoding="application/x-tex">t_g</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9011879999999999em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">g</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span>  不变）。 另外，我们知道最终的通信时间是<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>m</mi><mi>a</mi><mi>x</mi><mo>(</mo><msub><mi>t</mi><mi>c</mi></msub><mo separator="true">,</mo><msub><mi>t</mi><mi>g</mi></msub><mo>)</mo></mrow><annotation encoding="application/x-tex">max(t_c ,t_g)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.036108em;vertical-align:-0.286108em;"></span><span class="mord mathdefault">m</span><span class="mord mathdefault">a</span><span class="mord mathdefault">x</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">g</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>。</p>
<p>为了最小化通信时间，tc 和 tg 需要相等。 如果它们不相等，例如 tc &gt; tg，则意味着可以通过减少 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>M</mi><mrow><mi>S</mi><msub><mi>S</mi><mrow><mi>C</mi><mi>P</mi><mi>U</mi></mrow></msub></mrow></msub></mrow><annotation encoding="application/x-tex">M_{SS_{CPU}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.933635em;vertical-align:-0.250305em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05764em;">S</span><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3567071428571427em;margin-left:-0.05764em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.07153em;">C</span><span class="mord mathdefault mtight" style="margin-right:0.13889em;">P</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">U</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.14329285714285717em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.250305em;"><span></span></span></span></span></span></span></span></span></span> 进一步减少通信时间，从而降低 tc。</p>
<p>当 CPU 机器和 GPU 机器的数量相同时， <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>M</mi><mrow><mi>S</mi><msub><mi>S</mi><mrow><mi>G</mi><mi>P</mi><mi>U</mi></mrow></msub></mrow></msub></mrow><annotation encoding="application/x-tex">M_{SS_{GPU}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.933635em;vertical-align:-0.250305em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05764em;">S</span><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3567071428571427em;margin-left:-0.05764em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">G</span><span class="mord mathdefault mtight" style="margin-right:0.13889em;">P</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">U</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.14329285714285717em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.250305em;"><span></span></span></span></span></span></span></span></span></span> = 0，这意味着我们不需要任何 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>S</mi><msub><mi>S</mi><mrow><mi>G</mi><mi>P</mi><mi>U</mi></mrow></msub></mrow><annotation encoding="application/x-tex">{SS_{GPU}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">G</span><span class="mord mathdefault mtight" style="margin-right:0.13889em;">P</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">U</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span> 。 这是因为 CPU 机器已经提供了足够的聚合带宽。 BytePS 回退到非共放置PS。 同样，当CPU机器数为0时，BytePS回退到all-reduce和colocated PS。</p>
<p>当然，更有趣的情况是当 0 &lt; k &lt; n 时的一般情况。 我们使用普通的 all-reduce 和 non-colocated PS 的通信时间作为两个基线。 我们将加速比 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>r</mi><mi>a</mi></msub></mrow><annotation encoding="application/x-tex">r_a</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">a</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 定义为普通 all-reduce 的通信时间除以一般情况的通信时间。 类似地，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>r</mi><mi>p</mi></msub></mrow><annotation encoding="application/x-tex">r_p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.716668em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> 被定义为与非同位 PS 情况相比的加速比。 我们有以下结论</p>
<figure data-type="image" tabindex="6"><img src="https://tva1.sinaimg.cn/large/008i3skNgy1gv5kqkapkaj60ra05u74g02.jpg" alt="Pasted image 20210906153023" loading="lazy"></figure>
<p>当 k = n 且 n趋向无穷, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>r</mi><mi>a</mi></msub><mo>=</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">r_a=2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">a</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span></span></span></span>。当k 很小时，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>r</mi><mi>p</mi></msub></mrow><annotation encoding="application/x-tex">r_p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.716668em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> 可能会很大，<strong>因为通信带宽在非colocated PS 中被CPU 机器严重瓶颈</strong>。 例如，当 n = 32 和 k = 16 时，我们分别有 ga = 1.46 和 gp = 1.52。 这意味着 BytePS 理论上可以分别超过 all-reduce 和 PS 46% 和 52%。 我们注意到在 k = n 之外添加更多 CPU 机器无济于事，因为通信瓶颈将成为 GPU 机器的 NIC 带宽。</p>
<h2 id="42-机内通信">4.2 机内通信</h2>
<p>在 §4.1 中，我们设计了最优的机器间通信策略。 在实践中，我们发现机内通信同样重要。 一台机器中通常有多个 GPU。 CS 必须在与 SS 通信之前/之后聚合/广播张量。 这会在 PCIe 链路上造成拥塞，并阻止 NIC 充分利用其带宽 B。此外，GPU 机器的内部拓扑在数据中心可能会有所不同。 下面，我们分享我们环境中最常见的两种机器设置以及我们相应的解决方案。 我们在第 4.2.3 节中介绍了一些适用于其他机器设置的原则。</p>
<h3 id="421-pcie-only-拓扑">4.2.1 PCIe-only 拓扑</h3>
<figure data-type="image" tabindex="7"><img src="https://tva1.sinaimg.cn/large/008i3skNgy1gv5kr5ii2jj60vi0ckta702.jpg" alt="Pasted image 20210906154354" loading="lazy"></figure>
<p><em>图 6：仅 PCIe 的机器拓扑和 BytePS 数据流。 灰色框是 GPU。 数据流图中仅显示了传出方向（从 GPU 到网络）。 传入则相反。</em></p>
<p>图 6(a) 显示了我们生产环境中的设置。 一台 GPU 机器有两个通过 QPI 连接的 NUMA CPU。 八个 GPU 分为两组，分别连接到两个 PCIe 交换机。 网卡是 100Gbps 并连接到其中一个 CPU 的 PCIe。 图中所有PCIe链路均为3.0 x16（128Gbps理论带宽）。 CPU 内存和 QPI 具有 &gt; 300Gbps 带宽，通信瓶颈的可能性较小。 我们称之为仅 PCIe 拓扑。对于这个机器模型，我们测量到 GPU 到 GPU 内存复制的吞吐量在同一 PCIe 交换机内为 105Gbps。 然而，跨 PCIe 交换机的 GPU 到 GPU 内存复制的吞吐量仅为 80Gbps。</p>
<p>不幸的是，许多现有的训练框架忽略了内部拓扑的这些细节。 例如，TensorFlow PS、MXNet PS 甚至 Horovod 的“分层 all-reduce”模式在同一台机器上的所有 GPU 上使用直接的 reduce 或 reduce-scatter。 这将导致跨 PCIe 交换机内存复制，不幸的是速度较慢。</p>
<p>相比之下，BytePS让同一PCIe交换机下的GPU先对张量求和，然后复制到CPU让CPU做全局求和，最后广播回全局求和。 我们称之为 CPU 辅助聚合。 具体来说，它包括以下步骤。</p>
<blockquote>
<p>点评：这种做法有点针对特定拓扑吧</p>
</blockquote>
<ol>
<li>Reduce-Scatter：假设每个 PCIe 交换机有L 个 GPU。 这 L 个GPU 执行reduce-scatter，仅在 PCIe 交换机内部产生 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><mi>l</mi><mo>−</mo><mn>1</mn><mo>)</mo><mi>M</mi><mi mathvariant="normal">/</mi><mi>l</mi></mrow><annotation encoding="application/x-tex">(l-1)M/l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mclose">)</span><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="mord">/</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span></span></span></span> 流量。 当它完成时，每个 GPU 应该保存 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>M</mi><mi mathvariant="normal">/</mi><mi>l</mi></mrow><annotation encoding="application/x-tex">M/l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="mord">/</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span></span></span></span> 聚合数据。</li>
<li>GPU-CPU 复制：每个 GPU 将其 M/l 数据复制到 CPU 内存，这会导致沿途的 M/l 流量。 每个 PCIe 交换机都会生成 M 个聚合数据。</li>
<li>CPU-Reduce：CPU 规约来自所有 PCIe 交换机的数据并生成跨所有 GPU 的聚合数据。 这种规约不会产生任何 PCIe 流量。</li>
<li>networking：CS 将数据发送到 SS，并从 SS 接收<strong>全局聚合数据</strong>。</li>
<li>CPU-GPU 复制：每个 GPU 将其 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>M</mi><mi mathvariant="normal">/</mi><mi>l</mi></mrow><annotation encoding="application/x-tex">M/l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="mord">/</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span></span></span></span> 分区从 CPU 内存复制回自身。 这会导致从 CPU 到每个 GPU 的 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>M</mi><mi mathvariant="normal">/</mi><mi>l</mi></mrow><annotation encoding="application/x-tex">M/l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="mord">/</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span></span></span></span> 流量。</li>
<li>All-Gather：每个 GPU 对同一 PCIe 交换机下的 GPU 执行all-gather操作。 这会导致交换机内部产生 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><mi>l</mi><mo>−</mo><mn>1</mn><mo>)</mo><mi>M</mi><mi mathvariant="normal">/</mi><mi>l</mi></mrow><annotation encoding="application/x-tex">(l-1)M/l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mclose">)</span><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="mord">/</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span></span></span></span>流量。</li>
</ol>
<blockquote>
<p>点评：这个做法比快手使用 0号gpu 的做法要合理一点</p>
</blockquote>
<p>图 6(b) 显示了步骤 1 到 3 的流量。步骤 4 到 6 使用相同的链路但方向相反。 通过 CPU 辅助聚合，从 PCIe 切换到 CPU 链路将在每个方向仅承载 M 个流量，远低于直接在 8 个 GPU 上进行集体操作（7M/4 个流量）。 同时，每个 PCIe 交换机到 GPU 链路的流量将为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><mn>2</mn><mi>l</mi><mo>−</mo><mn>1</mn><mo>)</mo><mi>M</mi><mi mathvariant="normal">/</mi><mi>L</mi></mrow><annotation encoding="application/x-tex">(2l-1)M/L</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord">2</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mclose">)</span><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="mord">/</span><span class="mord mathdefault">L</span></span></span></span>, 设 l = 4（每个 PCIe 有四个 GPU），即 7M/4，与现有方法保持一致。 从根本上说，BytePS 利用 GPU 机器上的备用 CPU 来避免缓慢的 <strong>GPU 到 GPU 跨 PCIe 交换机内存复制</strong>。</p>
<p><strong>优化分析</strong>。 我们现在分析上述策略的通信最优性。 图 7 显示了一种更通用的仅 PCIe 拓扑，具有可变数量的 GPU 和 PCIe 交换机。 我们没有像图 6(a) 那样绘制 NIC，因为在这种拓扑结构下，NIC 具有专用的 PCIe 通道并且不会与 GPU 竞争 PCIe 带宽。</p>
<figure data-type="image" tabindex="8"><img src="https://tva1.sinaimg.cn/large/008i3skNgy1gv5krr2hlmj60ny0e4mxx02.jpg" alt="Pasted image 20210906164341" loading="lazy"></figure>
<p>系统架构被建模为层次图 G = (V,E)。 将 N 表示为叶节点 (GPU) 的集合，将 S 表示为中间节点（交换机）的集合，将 C 表示为 CPU 节点的集合。 V为 N、S和C的并集. E 中的每条边 e(vx, vy) 表示从顶点 vx 到 vy 的带宽，我们将 t(vx, vy) 表示为从 vx 发送到 vy 的流量。 我们进一步定义 p 为交换机的数量 (p&gt;=2)，n 作为每个交换机连接的叶节点 (n &gt;=2)。</p>
<p>我们假设 G 的以下特征： (1) E 中的每条边都是双工的，并且两个方向的带宽相等。 将b(vx, vy)表示为e(vx, vy)的带宽，则b(vx, vy) = b(vy, vx)； (2) 我们假设 G 是对称的。 树的同一层的带宽是等效的。 例如，对于任何 j, k 属于 [0, p1], x, y 属于 [ jn, (j+1)n-1]; (3)<strong>内存和QPI</strong>带宽远高于PCIe链路，不太可能成为瓶颈。 下面，我们只关注 PCIe 链路。</p>
<blockquote>
<p>补充：<code>QPI &lt;Quick Path Interconnect&gt;</code>，又名CSI，Common System Interface公共系统接口，是一种可以实现芯片间直接互联的架构。</p>
</blockquote>
<p>从 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>N</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">N_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 到 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>N</mi><msub><mi>p</mi><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow></msub></msub></mrow><annotation encoding="application/x-tex">N_{p_{n-1}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.975095em;vertical-align:-0.291765em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3173142857142857em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">n</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.20252142857142857em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.291765em;"><span></span></span></span></span></span></span></span></span></span> 的 GPU 需要对它们的数据求和。 我们可以使用前面提到的 CPU 辅助聚合，也可以使用需要每个 GPU 将其整个数据直接复制到 C 的蛮力复制。 在实践中，最优解应该是这两种策略的组合，取决于 b(Sj,Cj) 和 b(Ni,Sj) 的值。 直觉是我们对 x 的数据应用蛮力复制，并对数据的 y (x+y = 1) 应用 CPU 辅助聚合。 在一定的 x 和 y 下，作业完成时间 J 可以最小化。 我们分别计算两条链路的流量。 在 e(Sj,Cj) 上，流量由 n 次蛮力复制加上 CPU 辅助聚合的流量组成。 在 e(Ni,Cj) 上，流量由一个蛮力副本和 CPU 辅助聚合的完整流量组成。</p>
<blockquote>
<p>这里其实没明白为什么</p>
</blockquote>
<figure data-type="image" tabindex="9"><img src="https://tva1.sinaimg.cn/large/008i3skNgy1gv5ksbqn8lj60vw09cgma02.jpg" alt="Pasted image 20210906172651" loading="lazy"></figure>
<p>（一通计算后）这意味着最佳解决方案是这样工作的：每个 GPU 对其 1/5 数据应用蛮力复制，并对其余 4/5 数据使用 CPU 辅助聚合。 因此，我们有以下主要结论：</p>
<p><strong>CPU 辅助聚合接近最佳</strong><br>
当 x = 0 时，解决方案是我们的 CPU 辅助聚合，作业完成时间为 J(0) = 0.141s。 根据计算，最佳时间为 0.129 秒。 因此，我们的策略非常接近最佳解决方案，性能差异为 9%。 然而，在实践中，强力复制对 CPU 内存有很大压力——与 CPU 辅助聚合相比，任何使用强力复制的张量都将消耗 4倍 CPU 内存带宽。 CPU内存真的没有4倍带宽的PCIe链路，尤其是FP16求和。因此，我们选择根本不使用蛮力复制并坚持使用 CPU 辅助聚合。</p>
<p><strong>CPU 辅助聚合优于基于环的 allreduce</strong></p>
<figure data-type="image" tabindex="10"><img src="https://tva1.sinaimg.cn/large/008i3skNgy1gv5kssysyxj60w8076gn102.jpg" alt="Pasted image 20210906173540" loading="lazy"></figure>
<p>所以很容易证明 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>J</mi><mrow><mi>c</mi><mi>a</mi></mrow></msub><mo>&lt;</mo><msub><mi>J</mi><mrow><mi>a</mi><mi>r</mi></mrow></msub></mrow><annotation encoding="application/x-tex">J_{ca} &lt; J_{ar}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.09618em;">J</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.09618em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">c</span><span class="mord mathdefault mtight">a</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.09618em;">J</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.09618em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">a</span><span class="mord mathdefault mtight" style="margin-right:0.02778em;">r</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 对于任何 n, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi><mo>&gt;</mo><mo>=</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">p&gt;=2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7335400000000001em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">p</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&gt;</span></span><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span></span></span></span> 总是成立。例如，使用我们的 PCIe 机器的值，让 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi><mo>=</mo><mn>2</mn><mo separator="true">,</mo><mi>n</mi><mo>=</mo><mn>4</mn><mo separator="true">,</mo><msub><mi>b</mi><mrow><mi>b</mi><mi>o</mi><mi>t</mi><mi>t</mi><mi>l</mi><mi>e</mi><mi>n</mi><mi>e</mi><mi>c</mi><mi>k</mi></mrow></msub><mo>=</mo><mn>80</mn><mi>G</mi><mi>b</mi><mi>p</mi><mi>s</mi></mrow><annotation encoding="application/x-tex">p = 2, n = 4, b_{bottleneck} = 80Gbps</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">p</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8388800000000001em;vertical-align:-0.19444em;"></span><span class="mord">2</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">n</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord">4</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">b</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">t</span><span class="mord mathdefault mtight">t</span><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span><span class="mord mathdefault mtight">e</span><span class="mord mathdefault mtight">n</span><span class="mord mathdefault mtight">e</span><span class="mord mathdefault mtight">c</span><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord">8</span><span class="mord">0</span><span class="mord mathdefault">G</span><span class="mord mathdefault">b</span><span class="mord mathdefault">p</span><span class="mord mathdefault">s</span></span></span></span>（跨越 PCIe 交换机的内存副本的带宽 ) 和 b(Sj,Cj) = 105Gbps 我们得到 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>J</mi><mrow><mi>c</mi><mi>a</mi></mrow></msub></mrow><annotation encoding="application/x-tex">J_{ca}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.09618em;">J</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.09618em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">c</span><span class="mord mathdefault mtight">a</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 比 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>J</mi><mrow><mi>a</mi><mi>r</mi></mrow></msub></mrow><annotation encoding="application/x-tex">J_{ar}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.09618em;">J</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.09618em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">a</span><span class="mord mathdefault mtight" style="margin-right:0.02778em;">r</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 小 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>23.7</mn></mrow><annotation encoding="application/x-tex">23.7%</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span><span class="mord">3</span><span class="mord">.</span><span class="mord">7</span></span></span></span>。</p>
<h3 id="422-基于-nvlink-的拓扑">4.2.2 基于 NVLink 的拓扑</h3>
<p>图 8(a) 显示了我们数据中心的另一个机器模型——带有 NVLinks 的 GPU 机器。 有四个 PCIe 交换机，每个交换机连接两个 GPU 卡。 GPU 也通过 NVLink 连接。 NVLinks 为每个 GPU 提供总计 1.2Tbps GPU-GPU 带宽，远高于 PCIe 链路。 NIC 连接到其中一个 PCIe 交换机。</p>
<figure data-type="image" tabindex="11"><img src="https://tva1.sinaimg.cn/large/008i3skNgy1gv5ktg0ciuj60w80es76302.jpg" alt="Pasted image 20210906193948" loading="lazy"></figure>
<p><em>图 8：基于 NVLink 的机器拓扑和 BytePS 数据流。 数据流图中只显示出方向</em></p>
<p>有了NVLink，GPU-to-GPU通信可以完全避免占用PCIe带宽。 因此，我们不再需要 CPU 辅助聚合。 然而，我们发现现有框架，包括最流行的 GPU all-reduce 实现 NCCL（由 TensorFlow、PyTorch、MXNet 和 Horovod 使用），再次不是最佳的。</p>
<p>问题在于，考虑到 NIC，拓扑不对称，它仅连接到一个（四分之一）PCIe 交换机。 同一个PCIe交换机下的网卡和两个GPU要争夺 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>P</mi><mn>0</mn></msub><mo>−</mo><mi>C</mi><mi>P</mi><msub><mi>U</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">P_0-CPU_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">U</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 的PCIe带宽。 请记住，不仅 CS 使用此 PCIe 带宽，而且在同一台 GPU 机器上运行的 SS 也使用它！ <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>P</mi><mn>0</mn></msub><mo>−</mo><mi>C</mi><mi>P</mi><msub><mi>U</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">P_0-CPU_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">U</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 再次成为整个通信的瓶颈。</p>
<p>根据分析，我们应该在本地聚合时将尽可能多的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>P</mi><mn>0</mn></msub><mo>−</mo><mi>C</mi><mi>P</mi><msub><mi>U</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">P_0-CPU_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">U</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> PCIe带宽留给网卡。 对于这种拓扑，BytePS 使用reduce 和broadcast 而不是reduce-scatter 和all-gather——来自所有GPU 的张量首先 reduce 到GPU2，然后将结果从 GPU2 复制到CPU0 内存。 图 8(b) 显示了这些步骤。 之后，当 CS 从 SS 得到聚合结果时，GPU2 会将数据复制到 GPU 内存中，并将它们广播到其他 GPU。 这样，我们完全阻止了 GPU 使用 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>P</mi><mn>0</mn></msub><mo>−</mo><mi>C</mi><mi>P</mi><msub><mi>U</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">P_0-CPU_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">U</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 带宽进行通信，因此 NIC 可以运行到 100Gbps 带宽。</p>
<blockquote>
<p>点评：这个拓扑的bug 在于gpu 连接是一个环，建立在特殊的拓扑基础上。<br>
从内存拷贝到网卡用PCIE， 网线会是另外的瓶颈</p>
</blockquote>
<p>这种方法似乎会在 GPU2 上创建流量热点。 但是，NVLinks 的带宽比 PCIe 链路大得多，因此即使在热点上，GPU 间通信也永远不是瓶颈。 同时，用于GPU-CPU复制的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>P</mi><mn>1</mn></msub><mo>−</mo><mi>C</mi><mi>P</mi><msub><mi>U</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">P_1-CPU_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">U</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>  PCIe链路与网卡100Gbps带宽大致相同，因此也不是瓶颈。</p>
<p>BytePS 达到了最优结果——没有机内带宽瓶颈。 不幸的是，现有的解决方案，如 NCCL，由于 GPU0 和 NIC 之间的距离很近，往往让 GPU 使用 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>P</mi><mn>0</mn></msub><mo>−</mo><mi>C</mi><mi>P</mi><msub><mi>U</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">P_0-CPU_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">U</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 瓶颈链路。</p>
<p>因此，它的通信性能低于我们在基于 NVLink 的机器中的解决方案。</p>
<blockquote>
<p>点评：这个看起来应该只是一个微小的工作吧</p>
</blockquote>
<h3 id="423-讨论">4.2.3 讨论</h3>
<p>PCIe-only 和基于 NVLink 拓扑的解决方案大不相同。 这表明不存在万能的最优解。 <strong>匹配内部通信必须适应不同的内部拓扑</strong>。 诚然，在我们的环境中使用的拓扑肯定比上述两种更多。 但是，我们认为以上两个是具有代表性的，因为它们分别类似于服务器供应商和 NVIDIA推荐的参考设计。</p>
<p>尽管存在差异，但我们总结了两个原则 - 1）当两个 GPU 不在同一个 PCIe 交换机下时，始终避免直接 GPU 到 GPU 内存复制，因为它在实践中很慢。 2) 始终尽量减少 PCIe 交换机到 GPU 和 NIC 共享的 CPU 链路上的流量。 我们提出以下最佳实践程序。 设 Sn 是带有 GPU 和 NIC 的 PCIe 交换机的数量，Sg 是只有 GPU 的 PCIe 交换机的数量。</p>
<ol>
<li>如果 Sn &gt; 0 且 Sg &gt; 0，则拓扑是非对称的，就像我们基于 NVLink 的拓扑一样。 CS 应该使用reduce 和broadcast，使用不与NIC 竞争的GPU 作为reduce 或broadcast 根。</li>
<li>如果 Sn = 0 或 Sg = 0，拓扑是对称的，就像我们的 PCIe-only 情况一样。 CS 应该使用 reduce-scatter 和 allgather 来平衡所有 PCIe 交换机上的流量。 如果没有 NVLink，则应使用 CPU 辅助聚合（第 4.2.1 节）。</li>
</ol>
<p><strong>多网卡拓扑</strong>。 虽然我们讨论的两种具体拓扑只有一个网卡，但上述原理可以直接扩展到多网卡拓扑——它只是改变了 Sn 和 Sg 的值。</p>
<p><strong>GPU 直接 RDMA (GDR)</strong>。 GDR 可以潜在地减少 PCIe 流量。 但是，GDR 要求 GPU 和 RDMA 网卡在同一个 PCIe 交换机上，否则即使使用 100GbE 网卡，吞吐量也可能低于 50Gbps [12]，我们自己的测量也证实了这一点。 因此，GDR 对我们的设置没有好处——仅 PCIe 拓扑不能满足要求，我们已经避免了基于 NVLink 拓扑的任何 PCIe 瓶颈。 此外，像 AWS 这样的大多数云都不支持 GDR。 因此，BytePS 目前不使用 GDR。</p>
<blockquote>
<p>点评：byteps 不支持 GPU 直接RDMA</p>
</blockquote>
<p>我们可以看到最优的<strong>机内通信策略与内部拓扑紧密耦合</strong>。 构建一个分析器来自动检测拓扑、探测带宽并生成最佳策略是未来有趣的工作。</p>
<h1 id="5-求和服务">5 求和服务</h1>
<p>为了获得最佳的机器间通信时间（第 4.1 节），BytePS 需要一个可以在每台机器的 CPU 上运行并与 CS 通信的模块。 问题是，它在训练算法中的作用是什么？ 我们最初的尝试是遵循之前的 PS 设计，其中 PS 进程负责运行优化器。 优化器聚合来自所有 GPU 的梯度，并使用各种优化器更新 DNN 模型参数。</p>
<h2 id="cpu-瓶颈">CPU 瓶颈</h2>
<p>不幸的是，很快我们发现 CPU 成为系统中的瓶颈。 我们用一个实验来证明这一点。 我们使用典型的非协同 PS 设置训练 VGG16 DNN ：使用一台 Tesla V100 GPU 机器和一台 CPU 机器（Intel Xeon Platinum CPU，具有超线程的 32 核和 Intel MKL ），通过 100GbE 以太网连接 . GPU 机器运行前向和反向传播，CPU 机器使用全部 32 个 CPU 内核运行优化器。</p>
<figure data-type="image" tabindex="12"><img src="https://tva1.sinaimg.cn/large/008i3skNgy1gv5ktx6e3rj60ww0e876a02.jpg" alt="Pasted image 20210906204431" loading="lazy"></figure>
<p><em>图 9：优化器的 CPU 很慢，但求和的 CPU 不慢</em></p>
<p>图 9(a) 显示，即使有 32 个内核并启用了 MKL，在 CPU 机器上运行优化器也会减慢端到端的训练速度。 这意味着 CPU 无法匹配网络带宽并成为瓶颈（第 6 节）。 随着优化器算法变得越来越复杂（从更简单的 SGD 到更复杂的 RMSProp），瓶颈效应变得更加严重。</p>
<p>根本原因。 CPU 瓶颈是由有限的内存带宽引起的。 Adam 等流行的优化器很容易耗尽现代 CPU 的内存带宽。 例如，6 通道 DDR4-2666 内存设置的峰值传输速率高达 1024 Gbps，结合读取和写入 [8]。 很容易估计，例如，Adam 优化器 [42] 需要超过 10 倍的内存访问（读+写）来应用每个梯度更新。 加上 100Gbps 网卡消耗 200 Gbps 内存带宽（读+写），1024 Gbps 内存带宽根本不足以让 Adam 处理 100 Gbps 梯度流。</p>
<blockquote>
<p>点评：这里很硬核</p>
</blockquote>
<h2 id="cpu擅长求和">CPU擅长求和</h2>
<p>上面的实验让我们重新思考 CPU 上的任务。优化器的计算可以分为两个步骤，梯度求和和参数更新，如图 10 所示。</p>
<figure data-type="image" tabindex="13"><img src="https://tva1.sinaimg.cn/large/008i3skNgy1gv5kufwhl2j60v20ay0u502.jpg" alt="Pasted image 20210906205041" loading="lazy"></figure>
<p><em>图 10：all-reduce、PS 和 BytePS 之间的组件放置比较</em></p>
<p>幸运的是，由于高度优化的<strong>AVX 指令 [47]</strong>，现代 x86 CPU 擅长求和。 在图 9(b) 中，我们使用合成浮点张量显示了与上述相同 CPU 上的求和吞吐量。 <strong>FP16和FP32精度的吞吐量都超过200Gbps，高于100Gbps的网卡带宽</strong>。 因此，CPU 上的求和不会成为瓶颈。</p>
<p>BytePS 的解决方案。 基于这些观察，BytePS 将优化器的两个步骤解耦。 我们将计算密集型参数更新移至 GPU，并仅在 CPU 上进行求和——这就是我们将 CPU 模块命名为求和服务 (SS) 的原因。 SS 不仅可以避免 CPU 成为瓶颈，还可以大大降低 CPU 开销。 通过使用 AVX 和 OpenMP 精心实施，SS 在以 100Gbps 吞吐量运行时仅消耗少于 3 个 CPU 内核。 图 10 对 PS、all-reduce 和 BytePS 进行了high-level 比较，了解它们如何将 DNN 训练中的不同组件放置到 GPU 和 CPU 资源上。</p>
<p>由于 Summation Service 将参数更新移动到 GPU 机器上，因此所有 GPU 机器都需要执行相同的参数更新计算，而在传统 PS 中，参数更新只需进行一次。 因此，BytePS 比 PS 使用更多的计算周期来更新参数。 这是我们自愿做出的权衡，以加快端到端的训练速度。 我们将 SS 开销比定义为 参数更新的 FLOP 除以FP 和 BP FLOPS 总和。</p>
<p>VGG-16、ResNet-50、BERTlarge使用SGD作为优化器的比率为138 MFLOPs / 32 GFLOPs、26 MFLOPs / 7.8 GFLOPs、387 MFLOPs / 494 GFLOPs，<strong>均小于0.5%</strong>。 与训练加速相比，引入的开销可以忽略不计（图 9（a））。 上述比率定义假设批量大小为 1。DNN 训练通常使用数十或数百个批量大小。 每批参数更新一次，因此额外的开销在实践中更小。</p>
<p>我们注意到 Horovod [60] 可以选择通过首先将张量复制到 CPU 内存然后执行 CPU-only all-reduce 来将梯度聚合移动到 CPU。 由于它仍然只依赖 GPU 机器上的 CPU 和带宽，因此与 GPU 上的直接 all-reduce 相比，它没有提供通信方面的优势。 BytePS 不同：它利用额外的 CPU 机器进行梯度求和，同时在 GPU 上保持参数更新。</p>
<blockquote>
<p>点评：byteps 用的额外的cpu 和带宽</p>
</blockquote>
<h2 id="支持异步训练">支持异步训练</h2>
<p>虽然分离求和和更新给我们带来了性能上的好处，但它打破了原始 PS 的一个重要特性：像 Asynchronous Parallel [25] 这样的异步训练的支持。 异步并行依赖于保持最新模型参数的 PS 进程，这与 SS 的设计不直接兼容。</p>
<blockquote>
<p>点评：是啊，worker 上做参数更新，那肯定得同步更新</p>
</blockquote>
<p>为了弥补这一差距，我们重新设计了一个新的工作流程，可以启用与 SS 的异步训练，如图 11（b）所示。 简而言之，GPU 更新参数并首先计算 delta 参数。 CS 发送它们并接收最新的参数。 SS 不断地在最新的参数中添加 delta 参数。 接下来，我们证明这个新的训练工作流程在算法收敛方面等同于异步并行。</p>
<figure data-type="image" tabindex="14"><img src="https://tva1.sinaimg.cn/large/008i3skNgy1gv5kv3ivedj60qo0aodgz02.jpg" alt="Pasted image 20210906215412" loading="lazy"></figure>
<p>图 11：PS 和 BytePS 之间的异步训练工作流比较。 g 是梯度。 w 是参数。</p>
<p><strong>定理2</strong> BytePS的异步算法等价于异步并行<br>
<strong>证明</strong>  考虑一个与 n 个 CS 相连的 SS。 我们说 CS 存储本地模型参数，SS 保存最新版本的参数。 我们证明的high level 想法是表明，在给定相同的通信顺序（推和拉顺序）的情况下，我们的算法与异步并行生成相同的状态（即，SS 模块和 n 个 CS 模块的相同参数）。 我们使用 f 作为优化器的一般表示。因此，优化可以表示为 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>w</mi><mo>=</mo><mi>w</mi><mo>+</mo><mi>f</mi><mo>(</mo><mi>g</mi><mi>i</mi><mo separator="true">,</mo><mi>t</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">w=w+ f(gi,t)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mord mathdefault">i</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">t</span><span class="mclose">)</span></span></span></span>，其中 gi,t 表示迭代 t (t  in [1,T]) 时 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>C</mi><msub><mi>S</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">CS_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> (i  in [0,n1]) 的梯度。 分别将 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>w</mi><mrow><mi>p</mi><mi>s</mi></mrow></msub></mrow><annotation encoding="application/x-tex">w_{ps}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.716668em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">p</span><span class="mord mathdefault mtight">s</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>w</mi><mrow><mi>b</mi><mi>y</mi><mi>t</mi><mi>e</mi><mi>p</mi><mi>s</mi></mrow></msub></mrow><annotation encoding="application/x-tex">w_{byteps}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.716668em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">b</span><span class="mord mathdefault mtight" style="margin-right:0.03588em;">y</span><span class="mord mathdefault mtight">t</span><span class="mord mathdefault mtight">e</span><span class="mord mathdefault mtight">p</span><span class="mord mathdefault mtight">s</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> 表示为 PS 和 BytePS 中的参数。 并且将 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>w</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>t</mi></mrow></msub></mrow><annotation encoding="application/x-tex">w_{i,t}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.716668em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathdefault mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> 表示为迭代 t 时每个 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>w</mi><mi>o</mi><mi>r</mi><mi>k</mi><mi>e</mi><msub><mi>r</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">worker_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="mord mathdefault">e</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>（对于 PS）或 CS（对于 BytePS）上的参数。 对于所有 CS 和 SS，该参数被初始化为 w0。 经过 T 次迭代，我们可以得到更新后的参数为：</p>
<figure data-type="image" tabindex="15"><img src="https://tva1.sinaimg.cn/large/008i3skNgy1gv5kxlas4dj614q09gmxm02.jpg" alt="Pasted image 20210906220857" loading="lazy"></figure>
<blockquote>
<p>点评：这个证明其实没有谈到异步时间的问题</p>
</blockquote>
<h1 id="6-实现">6 实现</h1>
<p>虽然 BytePS 的核心对于任何训练框架都是通用的，但 BytePS 还为 TensorFlow、PyTorch 和 MXNet 实现了插件，以方便用户使用。 核心是用 C++ 实现的，而框架插件包含 C++ 和 Python。 BytePS 总共包含大约 7.8K 行 Python 代码和 10K 行 C++ 代码。 作为主要的在线服务提供商，我们在内部部署了 BytePS。 BytePS 也已经开源 [4] 并吸引了成千上万的用户。</p>
<h2 id="61-多阶段流水线">6.1 多阶段流水线</h2>
<p>加速多步骤过程的一种常见方法是构建一个多级流水线，该流水线与每个步骤的处理时间重叠。 我们结合了先前工作中的张量分区和流水线的想法 [34, 55]。 例如，对于 PCIe-only 拓扑，CS 有六个步骤。 它映射到 <strong>BytePS 运行时</strong>中的 6 级管道。 我们实现 BytePS 以灵活构建管道而无需重新编译。 管道中的每个阶段都被实现为一个具有张量优先级队列的独立线程。 优先级的分配类似于 [34,55]。 正如 §4.1.1 中分析的那样，大张量被分割成多个不超过 4MB 的小张量。 接下来，每个小张量被排入第一个队列，并在一个阶段完成处理后移向下一个队列，直到它从最后一个队列中出列。</p>
<h2 id="62-解决-rdma-性能问题">6.2 解决 RDMA 性能问题</h2>
<p>对于机器间通信，我们使用 RDMA RoCEv2。 每台机器有一个 100GbE 网卡，RDMA 网络提供完整的双向带宽。 为了充分发挥 RDMA 的优势，我们经历了完整的设计和调试之旅，我们分享如下。</p>
<p><strong>RDMA 内存管理</strong></p>
<p>为了提高性能，我们的目标是避免不必要的内存复制 [72] 并在 CPU 内存上实现零复制。 BytePS 基于 RDMA WRITE，因为它是常见 RDMA 用法中性能最高的 [39]。</p>
<blockquote>
<p>点评：RDMA这些论文也看看</p>
</blockquote>
<p>传统的单边 RDMA 操作（WRITE 和 READ）至少需要两次往返：获取远程地址，并将值写入（读取）到（从）该地址 [39, 40, 50, 70]。 我们通过利用 DNN 训练在每次迭代中始终发送相同的张量集这一事实来优化过程。传统的单边 RDMA 操作（WRITE 和 READ）至少需要两次往返：获取远程地址，并将值写入（读取）到（从）该地址 [39, 40, 50, 70]。 我们通过利用 <strong>DNN 训练在每次迭代中始终发送相同的张量集这一事实</strong>来优化过程。</p>
<blockquote>
<p>点评：也就是寻址只需要一次，那特征准入没发解决吧</p>
</blockquote>
<p>只有在第一次迭代时，BytePS 才会初始化所有需要的张量，向 RDMA NIC 注册缓冲区并交换所有远程地址。 然后 BytePS 存储远程缓冲区信息并在其余迭代中直接重用。</p>
<blockquote>
<p>点评：也就是用空间换时间，多占M的空间</p>
</blockquote>
<p><strong>解决慢速接收器症状</strong><br>
我们还遇到了 [30] 中报告的缓慢接收器症状——NIC 向网络发送了许多 PFC。 那些过多的 PFC 会减慢张量传输速度，这可能会对其他流量造成附带损害。 在这里，我们报告了这种症状的几个其他原因以及我们如何解决这些问题</p>
<p>我们的第一个发现是内部 RDMA 环回流量会导致内部 incast，并推动 NIC 生成 PFC。 BytePS 在每台 GPU 机器上运行 CS 和 SS。 它们之间的流量，我们称之为环回流量，不消耗 NIC 的外部以太网带宽，但会消耗内部 CPU-NIC PCIe 带宽。 最初，我们没有添加任何特殊设计——我们坚持使用 RDMA  [9] 来处理环回流量，并认为 NIC DMA 可以处理它。 但是，我们意识到它在 NIC 上创建了 2:1 的 incast，将 RX 和环回作为两个入口端口，将 DMA 到内存引擎作为一个出口端口！</p>
<blockquote>
<p>点评：不太理解这段</p>
</blockquote>
<p>为了解决这个问题，我们实现了一个共享内存 (shm) 数据路径。 当 CS 检测到 SS 与自己在同一台机器上时，CS 会简单地通知 SS 数据在共享内存中。 SS完成求和后，SS将结果从自己的缓冲区复制回CS的共享内存。 因此，消除了环回 RDMA 流量。</p>
<blockquote>
<p>点评：使用共享内存代替socket，正常做法，local 不用rdma</p>
</blockquote>
<p>我们的第二个发现是我们需要为 RDMA 使用页面对齐的内存。 否则可能会触发 PFC。 我们的假设是硬件 DMA 将传输单元与页面大小（例如 4096 字节）对齐。 因此，<strong>使用页对齐地址对 DMA 引擎更友好，因为它减少了需要写入的页数</strong>。</p>
<p>我们的第三个发现是，RDMA NIC RX 性能会受到<strong>并发</strong>发送的实现方式的影响！ 最后，我们不仅使用页面对齐的内存，而且在发送方的每个 RDMA WRITE 只强制执行一个scater-gather entry (sge)。</p>
<p>在整个过程中，我们联系了网卡供应商，并与他们的软硬件专家进行了长时间的讨论。 在撰写本文时，我们还没有得到最后两个问题的官方根本原因。</p>
<p>在所有优化之后，BytePS 实现可以按预期运行。 表 2 显示了应用上述三种优化中的每一种后的性能改进。 NIC 产生的 PFC 可忽略不计。</p>
<blockquote>
<p>点评：相当于byteps硬核修改了 rdma的实现？</p>
</blockquote>
<figure data-type="image" tabindex="16"><img src="https://tva1.sinaimg.cn/large/008i3skNgy1gv5kyusj8dj612009w40202.jpg" alt="Pasted image 20210907094705" loading="lazy"></figure>
<p>正如我们在 §4.1 中所讨论的，BytePS 在网络中创建了许多多对一的通信模式。 多对一以在 TCP/IP 网络中创建 incast 和丢包而闻名 [66]。 但是 BytePS 使用 RDMA/RoCEv2，它依赖于无损结构和 DCQCN [75] 进行拥塞控制。 我们在 BytePS 中没有观察到 incast 问题。</p>
<blockquote>
<p>点评：<a href="https://www.cnblogs.com/liusikun/p/5663193.html">tcp incast 问题</a></p>
</blockquote>
<h2 id="63-byteps-的使用">6.3 BytePS 的使用</h2>
<p>BytePS易于使用。 我们提供几乎与 Horovod、PyTorch 原生 API 和 TensorFlow 原生 API 相同的 Python 接口。 用户可以选择其中任何一个，并以最小的努力迁移到 BytePS。 例如，对于 Horovod-MNIST 示例，我们只需要更改一行 Python 代码，从“import horovod”到“import byteps”。 事实上，我们能够将大部分基于 Horovod 的内部训练任务自动转换为 BytePS。</p>
<h1 id="7-evaluation">7. Evaluation</h1>
<p>在本节中，我们展示了 BytePS 不仅在微基准测试中实现了最佳通信性能，而且还显着加速了生产环境中的训练工作。 我们列出了一些关于结果高保真度的亮点.</p>
<ul>
<li>所有使用的资源都由生产集群的调度器分配。调度器使用非抢占式资源调度——一旦调度了一个训练作业，它将拥有固定数量的 CPU 机器，不会改变。即使是我们展示的最大规模的任务，也使用运行许多生产任务的集群的不到 5% 的 GPU。</li>
<li>我们使用大的训练批次大小。<strong>较小的批大小意味着更少的 GPU 内存消耗但更多的通信</strong>，因此端到端的改进将更加明显。然而，我们所有的任务几乎都使用了 GPU 内存，因此针对 all-reduce 和 PS 的加速数字是 BytePS 的下限。</li>
<li>虽然我们不能透露任何内部使用的具体模型，但显示的任务和 DNN 模型结构高度代表了生产工作负载。该代码也可公开获取以实现可重复性。</li>
<li>我们将BytePS 与最先进的PS 和allreduce 实现进行比较，无需修改。例如，我们不会将 §6.2 中提到的 RDMA 优化应用于 native-PS 和 all-reduce</li>
</ul>
<p>我们使用的集群有一个全二分带宽的 RoCEv2 网络。 所有机器都有一个 100GbE 网卡。 我们注意到 TensorFlow、PyTorch 和 MXNet 可以重叠 DNN 计算和通信 [34, 55]，因此即使端到端性能的微小改进也可以表明通信的巨大改进。</p>
<blockquote>
<p>点评：这里说的应该是非稀疏场景的情况</p>
</blockquote>
<blockquote>
<p>补充：<strong>Ro****CE</strong>是在InfiniBand Trade Association（IBTA）标准中定义的<strong>网络</strong>协议，允许通过以太网络使用RDMA。<br>
大致有三类RDMA网络，分别是Infiniband、RoCE、iWARP。其中，Infiniband是一种专为RDMA设计的网络，从硬件级别保证可靠传输 ，而RoCE 和 iWARP都是基于以太网的RDMA技术，支持相应的verbs接口。<br>
<a href="https://cloud.tencent.com/developer/article/1771431">浅析RoCE网络技术 - 云+社区 - 腾讯云</a></p>
</blockquote>
<h1 id="8-观察与讨论">8. 观察与讨论</h1>
<p>在本节中，我们将分享我们的一些观察和讨论，旨在激发未来的研究。</p>
<p>即使没有额外的 CPU 机器，BytePS 的性能也优于 all-reduce。 理论上，当没有额外的 CPU 机器可用时，all-reduce 和 BytePS 的通信时间是相同的（第 4.1 节）。 在实践中，我们观察到 BytePS 在这种情况下仍然明显优于 all-reduce。 一个原因是 BytePS 有比 all-reduce 更好的机内通信策略。 然而，即使没有机器内优化，BytePS 仍然优于 all-reduce。</p>
<p>我们假设 BytePS 比 all-reduce 具有允许更多“异步性”的优势。 All-reduce 通常需要额外的带外同步来保证跨节点的顺序一致，而 BytePS 没有这个开销。 然而，为了分析它，我们需要一个分布式分析器，它可以构建分布式训练中跨所有节点的执行和通信的完整时间线</p>
<blockquote>
<p>点评：确实没有看到在cpu上进行同步的情况</p>
</blockquote>
<p>GPU 集群调度器应该考虑动态 CPU 资源。 通过利用额外的 CPU 机器，BytePS 可以加速 DNN 训练。 由于 BytePS 可以适应任意数量的 CPU 机器，因此它具有弹性——集群调度程序可以根据实时条件为现有作业扩展或缩减 CPU 机器。 由于收敛问题[16, 74]，大多数现有的调度器将作业的 GPU 数量保持不变。 幸运的是，BytePS 中的 CPU 机器数量只影响系统性能，而不影响模型收敛。 我们计划为 BytePS 添加弹性支持，这将使 BytePS 在训练过程中动态调度 CPU 资源。</p>
<blockquote>
<p>点评：弹性调度，但是hash 索引要变，可能这里利用一致性hash</p>
</blockquote>
<p>模型并行支持。 当减少跨 GPU 的张量时，BytePS 可以加速通信。 一些模型并行方法，例如 Megatron-LM  和 MeshTensorFlow ，也依赖于 all-reduce 原语进行通信。 因此，BytePS 也可以通过替换 all-reduce 操作来加速它们。</p>
<h1 id="9-related-work">9 Related Work</h1>
<p><strong>计算加速</strong>：<br>
为了加速前向传播和反向传播，社区已经制定了许多先进的编译器和库，包括 cuDNN [10]、MKL [7]、TVM [23]、XLA [17]、Astra [64] 和其他计算图优化，例如张量融合 [14] 和图替换 [37]。 他们专注于加速 DNN 计算。 它们是 BytePS 的补充并可与 BytePS 一起使用。</p>
<blockquote>
<p>点评：与社区兼容是很重要的一点</p>
</blockquote>
<p><strong>通信加速</strong>：<br>
加速通信有几个方向：（1）梯度压缩[21, 45]被提出来减少通信流量，即使用半精度进行梯度传输，代价是潜在的精度下降。 (2) 通信调度和流水线：最近的工作探索通过基于优先级的调度和张量分区来更好地重叠计算和通信 [31, 34, 55]。 想法是张量分区可以同时进行双向通信，并且在通信期间，前几层具有更高的优先级，因为下一次迭代的 FP 需要更快地使用它们。 这些想法是对 BytePS 的补充，并且它们已集成到我们的实现中。 Pipedream [51] 增加了多个批次之间的并行性。 BytePS 还可以潜在地加速其数据并行阶段。</p>
<blockquote>
<p>点评：快手也用了梯度压缩，介绍说精度不下降</p>
</blockquote>
<p><strong>分层 all-reduce</strong>：<br>
一些工作建议在 all-reduce 期间利用分层拓扑 [24, 49]，以最小化瓶颈链接处的流量。 然而，他们仍然依赖于资源同质的假设，而忽略了 CPU 资源。 BytePS 可以通过利用异构资源来超越它们。 其实最新的NCCL包括了分层的、基于树的all-reduce，和结果差别不大。</p>
<p><strong>机内优化</strong>：Blink [68] 还通过利用 NVLinks 和 PCIe 链路上的混合传输优化了单台机器内的多个 GPU 通信。 但是，Blink 没有优化分布式训练案例，其中主要的通信瓶颈是 NIC 及其 PCIe 连接，而不是速度更快的 NVLink。 BytePS 仔细调度机器内流量以更好地利用瓶颈带宽——网卡带宽。 我们的机内设计也考虑了网卡消耗的 PCIe 带宽，而 Blink 只关注 GPU 的 PCIe 连接。</p>
<blockquote>
<p>补充：利用 GPU 间所有异构数据传输通道，实现数据聚合的最优解决方案 Blink。相比 NCCL/Horovod，Blink 提高 GPU 间数据聚合的通信效率高达 8 倍，最多可以缩短分布式机器学习模型总训练时间的 40%<br>
<a href="https://blog.csdn.net/danteLiujie/article/details/102901189">Blink:网络自适配的GPU集群通信库 - 深度学习集群_danteLiujie的专栏-CSDN博客</a></p>
</blockquote>
<p><strong>用于加速 DNN 训练的新硬件芯片或架构</strong>：<br>
最近有许多新芯片，如 TPU [38] 和 Habana [6]，专门用于 DNN 训练。 事实上，BytePS 的设计并不是针对 GPU 的，只要它们也是 PCIe 设备就应该适用于它们。 有些人还建议使用 InfiniBand 交换机 ASIC [28] 来加速 all-reduce，或使用 P4 交换机 [58, 59] 来加速 PS。 E3 [46] 利用 SmartNIC 来加速网络应用程序，并且可以通过将梯度求和从 CPU 卸载到 SmartNIC 来潜在地使 BytePS 受益。 PHub [48] 提出了一种具有定制网络配置的机架级硬件架构，例如，一台服务器上有 10 个 NIC。 BytePS 专注于在商品数据中心使用普遍可用的 CPU 和 GPU 服务器。</p>
<h1 id="10-结论">10 结论</h1>
<p>BytePS 是一个统一的分布式 DNN 训练加速系统，可在异构 GPU/CPU 集群中实现最佳通信效率。 BytePS 处理不同数量的 CPU 机器的情况，并将传统的 all-reduce 和 PS 作为其框架的两个特殊情况。 为了进一步加速 DNN 训练，<strong>BytePS 提出了 Summation Service，并将 DNN 优化器拆分为两部分：梯度求和和参数更新。</strong> 它将 CPU 友好的部分，梯度求和保留在 CPU 中，并将计算量更大的参数更新移动到 GPU。 我们已经实施了 BytePS 并解决了许多实施问题，包括<strong>影响 RDMA 性能的问题</strong>。 BytePS 已被部署、广泛使用和开源 [4]。 已经基于它开发了多个外部项目。 重现评估的工件附录位于 [3]。</p>
<!-- more -->
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[百度的《AIBox: CTR Prediction Model Training on a Single Node》]]></title>
        <id>https://DragonFive.github.io/post/bai-du-de-lesslessaibox-ctr-prediction-model-training-on-a-single-nodegreatergreater/</id>
        <link href="https://DragonFive.github.io/post/bai-du-de-lesslessaibox-ctr-prediction-model-training-on-a-single-nodegreatergreater/">
        </link>
        <updated>2020-12-11T12:43:07.000Z</updated>
        <content type="html"><![CDATA[<p>AIBox 是百度提出的训练框架，论文 AIBox: CTR Prediction Model Training on a Single Node 进行了相关介绍。</p>
<h1 id="一-aibox-的技术创新及优势">一、AIBox 的技术创新及优势</h1>
<p>创新点与动机：<br>
AIBox 的核心想法就是想在一台机器上用GPU加速训练，但是参数实在太大了，所以就把计算密集的模型运算部分（joint learning）放在GPU完成，把取embedding 部分放在cpu部分完成(embedding learning)，这就是AiBox 的第一个创新：把网络切分为两部分。</p>
<p>但即便主存用了1TB 的内存，embedding 还是太大了，10^12 个key，每个key 的 weight 即使用8个字节存，key用8个字节存，也需要1.6TB , 所以论文提出了第二个创新：把embedding 存在SSD上，同时为了降低延迟和减少写操作对ssd寿命的影响，建立了二级缓存机制。</p>
<p>为了提高速度，AIBOX使用了流水线，把从hdfs 读数据(socket IO)，从SSD查Embedding （SSD io） 和 cpu+gpu 计算组成3阶段的 pipeline</p>
<p>优势：<br>
AIBOX不存在像分布式系统普遍存在的网络通信开销问题，然后在系统稳定性方面AIBOX与具有数千台机器的分布式集群相比更加稳定不会轻易宕机，而且在同步开销方面AIBOX只是涉及到一些内存锁和GPU片之间的少量通信。</p>
<h1 id="二-关于网络结构切分">二、关于网络结构切分</h1>
<p>the first module focuses on the embedding learning with high-dimensional &amp; sparse features and the second module is for joint learning with dense features resulted from the first module.</p>
<p>The embedding learning is processed on CPUs to help learn low dimensional dense embedding representations.</p>
<p>By transferring the learned embedding vectors from CPUs to GPUs, the computation-intensive joint learning module can make full use of the powerful GPUs for CTR prediction.</p>
<p>CPU 部分把数据从稀疏特征转化成 embedding （embedding learning），然后把embedding 传到 GPU，GPU进行一轮训练 (joint learning)</p>
<p>论文这部分讲了一些网络的设计细节，但这块感觉跟 AIBox本身没什么关系，论文写到：</p>
<p>把第一隐含层和最后一层隐含层的结果合并起来，第一层包含了low-level 的与输入信息最相关的feature，最后一层包含了high-level 的最抽象和有用的信息。这样会得到更准确的CTR预估结果。</p>
<p>训练两阶段(cpu+gpu)，梯度更新也是两阶段(gpu+cpu)。</p>
<h1 id="三-aibox-架构划分">三、AIBox 架构划分</h1>
<p><strong>架构：</strong><br>
分为三部分：CPU、GPU和 sparse table buff</p>
<ul>
<li>
<p>cpu模块：协调调度和embedding学习<br>
从hdfs读数据(一个pass)，向Sparse Table模块查embedding，然后发给GPU<br>
拿到gpu传来的梯度，更新sparse table<br>
定期save ckpt 到 hdfs</p>
</li>
<li>
<p>sparse table：把10^12 的离散特征的数据存储到ssd上的kv系统里<br>
内存中的key hash 索引存了特征到文件的映射关系，<br>
in-memory cache strategy 构造cache 和 buffer 来减少延迟</p>
</li>
<li>
<p>gpu模块：联合学习<br>
cpu传来的 embedding 被放入 HBMs 中，然后被fed 给 dense 联合学习网络<br>
emb通过pci-e总线进行传输<br>
一个CUDA stream进行数据传输，另一个cuda stream 进行学习计算<br>
HBMs如同片上ps一样工作，<br>
每个pass 在每个gpu 上计算新的参数，各gpu通过NVLink进行同步</p>
</li>
</ul>
<p><strong>3阶段流水线：network, SSDs and CPUs + GPUs</strong></p>
<figure data-type="image" tabindex="1"><img src="https://DragonFive.github.io//post-images/1625143570973.png" alt="" loading="lazy"></figure>
<figure data-type="image" tabindex="2"><img src="https://DragonFive.github.io//post-images/1625143575338.png" alt="" loading="lazy"></figure>
<h1 id="四-sparsetable-架构">四、sparseTable 架构</h1>
<p>由两部分构成：key hash index and bi-level cache management</p>
<h2 id="一key-hash-index">（一）key hash index</h2>
<p>Key Hash Index 存的是 10^12 个 feature key 到 ssd 文件的映射关系，直接每个key 存一个文件需要1.6TB大小的内存，放不下。</p>
<p>通过对key 取模进行分组建立 group 与 file 的对应关系，放在内存中。</p>
<p>group(key) → key mod 1012/m. We set m = ⌊BLOCK/(8 + sizeof(value))⌋,其中Block 是每次从ssd取数据的最小单元。</p>
<p>hash函数可以通过预训练一个模型来最大化feature 共现，把共现的feature 分到同样的桶里面。</p>
<h2 id="二二级缓存机制">（二）二级缓存机制</h2>
<p>ssd 的延迟是内存的1000倍，ssd是微秒级别延迟，内存是纳秒级别延迟</p>
<p>在一个 pass of mini batch 中只有1%的参数会被用到，所以我们可以用in-memory cache 来存储高频访问的hot parameters</p>
<p>SSD有物理性能限制：每个存储单元只能被写入（擦除）数千次，cache机制可以作为参数缓存，来减少更新参数对SSD使用寿命的影响</p>
<p>使用两个分离的链表进行拉链来提升探测性能。对每个ssd文件使用Bloom filter来减少不必要的读取。</p>
<p><strong>第一级缓存</strong></p>
<p>使用 si =hash1(g_id) 来算出一个 cache slot 槽，对应一个ssd 文件，对于参数并未进行真正初始化，而是在第一次访问到参数的时候，先用 bloom filter 探测key 是否在 slot 集合里，如果不在就不用读取这个文件，而是直接使用默认值，以此来减少不必要的ssd读取。</p>
<p><strong>二级缓存</strong></p>
<p>hash2(g_id, bucket)</p>
<p>对 一级的槽进行分桶bucket，来使得拉的链比较短。bucket 参数通过调节可以权衡空间和探测效率</p>
<p><strong>两条拉链</strong></p>
<ul>
<li>LRU 链用于保存最近访问过的key，以此来减少探测次数</li>
<li>LFU链按访问频次来保存key，用于缓存管理，只有当LFU满了需要删除低频key时，相应的数据才会写回到ssd上面</li>
</ul>
<p>由于经常有链条中的节点进行增删，所以使用线程池以Slab memory allocation mechanism机制 进行管理。</p>
<figure data-type="image" tabindex="3"><img src="https://DragonFive.github.io//post-images/1625143705399.png" alt="" loading="lazy"></figure>
<p><strong>文件管理系统</strong><br>
batch产生的小文件对于先有的文件系统有很大的压力，把许多小文件组成一个组来创建较少的文件。小文件的名字由大文件的名字加上offset构成，保存在第一级cache slot 中</p>
<p>监控文件系统的大小，合并访问量少的小问题，当model_size 达到最大冗余度的时候删掉访问少的文件，MAX_REPLICATION=SSD capacity ∗ (85% + overprovisioning)/model size<br>
<img src="https://DragonFive.github.io//post-images/1625143710125.png" alt="" loading="lazy"></p>
<h1 id="五-实验部分">五、实验部分</h1>
<p><strong>实验</strong><br>
AIBox 8 个GPU， 服务器级别的cpu, 1T 内存，Raid 0 nvme ssd<br>
MPI集群方式用75个计算节点</p>
<ul>
<li>
<p>AIBox 的硬件和维护费用比集群训练方式少 10%，执行时间多25%</p>
</li>
<li>
<p>AIBox 的auc 比集群方式稍好，可能是因为AIBox 这种单节点的方式，同步参数频率更高</p>
</li>
</ul>
<figure data-type="image" tabindex="4"><img src="https://DragonFive.github.io//post-images/1625143749656.png" alt="" loading="lazy"></figure>
<p>六、总结与一些细节问题：<br>
论文介绍了 AIBox 架构的一些细节方面，借助一系列系统设计方案如缓存机制来解决问题，通过这些并不是很fashion的技术合并，论文实现了集中式训练的技术突破，这种通过技术积累然后撬动难题的解决问题的方式值得我们学习。</p>
<p>但是还有一些细节没有讲清楚：</p>
<ol>
<li>
<p>AIBox 有几个worker 进行工作，他们是数据并行，还是使用同样的数据进行训练（文中提到AIBox 会在每个pass of mini batch 进行同步，所以应该不是一个worker 在参与训练）</p>
</li>
<li>
<p>AIBox 使用集中的训练方式，那如果这台机器挂掉，是不是根本没有办法进行恢复，只能另找一个机器从 ckpt 训练</p>
</li>
<li>
<p>文章没有介绍使用的具体计算引擎 (怀疑跟 horovod 接近）</p>
</li>
<li>
<p>同样文章没有介绍参数同步的细节，没有相关 all_reduce 的介绍（可以是使用了一个开源的框架，而这部分论文没有进行改进，所以没有做深入介绍）</p>
</li>
<li>
<p>文章开头提到使用 in-HBM ps 来减少数据传输，但是后面没有详细进行介绍</p>
</li>
</ol>
<p>总体上感觉这篇论文实用性强，但是细节介绍得不多</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[seastar教程翻译]]></title>
        <id>https://DragonFive.github.io/post/seastar-jiao-cheng-fan-yi/</id>
        <link href="https://DragonFive.github.io/post/seastar-jiao-cheng-fan-yi/">
        </link>
        <updated>2020-07-06T06:41:00.000Z</updated>
        <content type="html"><![CDATA[<h1 id="引言">引言</h1>
<p>Seastar是一个用于在现代多核机器上实现高性能复杂服务端应用的C++库。</p>
<p>传统来说，针对服务端医用的库和框架主要分为2大阵营：</p>
<ul>
<li>一些专注于性能，另一些专注于处理复杂性。</li>
<li>一些框架性能非常好，但是只能用来搭建简单的应用</li>
</ul>
<p>例如，DPDK只允许应用独立地处理包 (DPDK allows applications which process packets individually)，而其他一些框架通过牺牲运行效率来实现搭建非常复杂的应用。Seastar是我们集两者之长的一次尝试：创造一个可以用于搭建复杂服务端应用并达到最优性能的库。</p>
<p>Seastar使用2个概念——<strong>future 和 continuation</strong>  ——提供了一个完整的异步编程框架。它们提供了对任何类型的异步事件的一种一致的表达和处理方法，包括但不限于网络I/O，磁盘I/O，以及各种事件的复杂组合。</p>
<p>在现代多核多socket机器上在核间共享数据会带来严重的性能损失。Seastar程序采用了share-nothing模型，也就是说，内存被分给各个核，每个核只处理自己的那份内存中的数据，核间通信需要通过显示的消息传输完成。</p>
<h1 id="异步编程">异步编程</h1>
<p>同步、每个进程一个连接的这种server模式不是没有缺点和成本的。慢慢地，server的作者们发现创建一个新进程是很慢的，context switch很慢，每次处理都会有显著的overhead——最明显的是对堆栈大小的要求。Server和内核的作者们非常努力地去缓解这些overhead：他们从进程切换至线程，从创建新线程切换至使用线程池，降低了每个线程的默认堆栈大小，以及提高了虚拟内存大小来partially-utilize stacks。但是仍旧，使用同步设计的server的性能不够理想，扩展性不佳。在1999年，Dan Kigel普及了&quot;the C10K problem&quot;，需要单个server能够高效处理10000个并发连接——其中大多数为慢甚至inactive的连接。</p>
<h2 id="异步方式">异步方式</h2>
<p>这个问题的解决方法，也是在后续的几十年中逐渐变得流行的方法，就是抛弃方便但是低效的同步设计，转为一种新型的设计——异步，或者说是事件驱动的server 。一个事件驱动的server仅有一个线程，或者更准确的说，每个CPU一个线程。这个线程会运行一个循环，在循环的每个迭代步里，通过<code>poll()</code>（或者是更高效的<code>epoll</code> [[高性能开发综述#I O 优化：多路复用技术]]）来监察绑定在开启的file descriptor上的新事件，如sockets。举例来说，一个socket变得可读（有新数据抵达）或者变得可写（我们可以用这个连接发发送更多数据了）都是一个事件。应用通过进行一些非阻塞性的操作，修改一个或多个fd，后者维护这个连接的状态来处理这些事件。</p>
<p>基于事件驱动的服务器，无需为每一个请求创建额外的对应线程，虽然可以省去创建线程与销毁线程的开销，但它在处理网络请求时，会把侦听到的请求放在事件队列中交给观察者，事件循环会不停的处理这些网络请求。  在事件循环中，每一次循环都会查看是否有事件待处理，如果有的话就取出来执行。</p>
<p>写异步server应用的人们在今天仍然会遇到2个主要问题。</p>
<ul>
<li>复杂性：写一个简单的异步server是很简单的。然而写复杂异步server的难度臭名昭著。我们不再能用一些简单易读的函数调用来处理一个连接，而是需要引入大量<strong>回调函数，和一个复杂的状态机</strong>，用于记录对于哪些事件应该调用哪些函数。</li>
<li>非阻塞：因为context switch很慢，所以一个核只有一个线程是对性能很重要的。然而，如果我们每个核只有一个线程，处理事件的函数永远不能阻塞，不然这个核就会被闲置。这时候如果有IO ，就需要有多线程。</li>
</ul>
<p>当需求更高的性能的时候，server应用，以及其使用的框架，需要考虑以下问题：</p>
<p>现代机器：现代的机器和约10年前机器有着非常大的区别。他们有很多核和很深的内存层级（从L1 cache到NUMA），这种结构更适合特定的编程范式。：不可扩展性的编程范式（如，加锁）可能会极大地影响程序在多核上的性能；共享内存和无锁同步primitives虽然是可用的（比如原子操作和memory-ordering fences），但是比只用一个核的cache中的数据进行操作要慢很多，并且也会让程序不好扩展至多核。</p>
<p>Seastar是一个旨在解决上述的4个问题的异步框架：它是一个用于实现同时包括磁盘和网络I/O的复杂server的框架。它的fast path完全是<strong>单线程的（每核）</strong>，可扩展至多核并最小化了代价高昂的核间内存共享。Seastar是一个C++14的库，让用户能利用上复杂的编译优化特征与充分的控制力，且没有运行时overhead。</p>
<h2 id="非阻塞异步的事件驱动">非阻塞异步的事件驱动</h2>
<p>Seastar是一个让人可以比较直接地实现非阻塞、异步代码的事件驱动框架。它的API基于future。Seastar利用了如下的概念以获得极致的性能。</p>
<ul>
<li>Cooperative micro-task scheduler：每个核执行一个协作任务调度器而不是一个线程，每个任务都很轻量，只处理上一次 I/O 操作的结果并提交新的任务。</li>
<li>Share-nothing SMP架构：每个内核独立于 SMP 系统中的其他内核运行，核间通过消息传递进行通信，一个seastar核经常称作一个分片。</li>
<li>基于future的APIs：</li>
<li><strong>Share-nothing TCP stack</strong>：Seastar可以直接使用本机操作系统的TCP stack。在此之外，它也提供了一套基于task scheduler和share-nothing架构的高性能TCP/IP stack。这套stack提供双向零拷贝：你可以直接用TCP stack的buffer处理数据，并在不进行拷贝的情况下发送你的数据结构。</li>
<li>DMA-based存储APIs：基于网络栈，提供zero copy的存储api</li>
</ul>
<p>[[seastar smp]], [[seastar networking]]</p>
<h1 id="getting-started">Getting started</h1>
<p>最简单的Seastar程序如下：</p>
<pre><code class="language-cpp">#include &lt;seastar/core/app-template.hh&gt;
#include &lt;seastar/core/reactor.hh&gt;
#include &lt;iostream&gt;

int main(int argc, char** argv) {
    seastar::app_template app;
    app.run(argc, argv, [] {
            std::cout &lt;&lt; &quot;Hello world\n&quot;;
            return seastar::make_ready_future&lt;&gt;();
    });
}
</code></pre>
<p>如我们在例子中所示，每个Seastar程序必须定义并运行一个<code>app_template</code>对象。这个对象会在1个或多个CPU上启动主事件循环(event loop)（the Seastar engine），并运行给定的函数一次——在本例中，是一个lambda函数。</p>
<p><code>return make_ready_future&lt;&gt;();</code>会使事件循环以及整个程序在打印&quot;Hello world&quot;之后立即退出。在一个更典型的Seastar程序中，我们会希望事件循环持续运行，并处理收到的包，直到显式退出。这样的程序会返回一个用于判断何时退出的 <code>future</code>。我们将在下文介绍future以及如何使用它。无论何时都不要使用常规的C <code>exit()</code>，因为其会阻止Seastar正确地在退出时进行清理工作。</p>
<p>如例子所示，所有Seastar的函数都处于&quot;<code>seastar</code>&quot; namespace中。我们推荐使用namespace 前缀而不是<code>using namespace seastar</code>，在之后的例子中也会如此。</p>
<p>一些编译选项可以参考 <a href="https://github.com/scylladb/seastar/blob/master/doc/tutorial.md#introduction">wiki 原文</a>。</p>
<h1 id="线程和内存">线程和内存</h1>
<h2 id="seastar-线程">Seastar 线程</h2>
<p>正如在引言中提到的，基于Seastar的程序每个CPU上运行单一线程。每个线程有自己的事件循环，在Seastar的术语中被称为 engine。默认情况下，Seastar程序会占据所有可用的核，每个核启动一个线程。</p>
<p>我们可以通过如下程序来验证这点，其中<code>seastar::smp::count</code>是启动的线程总数：</p>
<pre><code class="language-cpp">#include &lt;seastar/core/app-template.hh&gt;
#include &lt;seastar/core/reactor.hh&gt;
#include &lt;iostream&gt;

int main(int argc, char** argv) {
    seastar::app_template app;
    app.run(argc, argv, [] {
            std::cout &lt;&lt; seastar::smp::count &lt;&lt; &quot;\n&quot;;
            return seastar::make_ready_future&lt;&gt;();
    });
}
</code></pre>
<p>在一个4硬件线程的机器上（2核并开启超线程），Seastar默认会使用4个engine thread。</p>
<pre><code class="language-bash">$ ./a.out
4
</code></pre>
<p>这4个engine thread会被绑定至（a la <code>taskset(1)</code>）不同的硬件线程。注意，如我们提到的，启动函数只在一个线程上运行，所以我们只看到&quot;4&quot;被打印了1次。之后的教程会告诉大家该如何使用所有的线程。</p>
<p>用户可以通过传入一个命令行参数<code>-c</code>来告诉Seastar去启动更少的线程数。例如，可以通过如下方式只启动2个线程：</p>
<pre><code class="language-bash">$ ./a.out -c2
2
</code></pre>
<p>在这种情况下，Seastar会保证每个线程绑定在不同的核上，我们不会让这2个线程在同一个核上作为超线程相互争夺的（不然会影响性能）。</p>
<p>我们不能分配超过硬件线程总数的线程，这么做会报错：</p>
<pre><code class="language-bash">$ ./a.out -c5
terminate called after throwing an instance of 'std::runtime_error'
  what():  insufficient processing units
abort (core dumped)
</code></pre>
<p>上面的程序来自于app.run的异常。为了能更好的catch这种启动异常并在不生成core dump的情况下优雅退出，我们可以这样写：</p>
<pre><code class="language-cpp">#include &lt;seastar/core/app-template.hh&gt;
#include &lt;seastar/core/reactor.hh&gt;
#include &lt;iostream&gt;
#include &lt;stdexcept&gt;

int main(int argc, char** argv) {
    seastar::app_template app;
    try {
        app.run(argc, argv, [] {
            std::cout &lt;&lt; seastar::smp::count &lt;&lt; &quot;\n&quot;;
            return seastar::make_ready_future&lt;&gt;();
        });
    } catch(...) {
        std::cerr &lt;&lt; &quot;Failed to start application: &quot;
                  &lt;&lt; std::current_exception() &lt;&lt; &quot;\n&quot;;
        return 1;
    }
    return 0;
}

</code></pre>
<pre><code class="language-bash">$ ./a.out -c5
Couldn't start application: std::runtime_error (insufficient processing units)
</code></pre>
<p>注意这样<strong>不能</strong>catch到程序实际的异步代码引发的异常。对于那些异常我们会在后文介绍。</p>
<h2 id="seastar-内存">Seastar 内存</h2>
<p>正如在引言中介绍的，Seastar程序会将他们的内存分片 (shard)。每个线程会被预分配一大块内存（在运行这个线程的那个NUMA 节点上），并且只使用这块被分配的内存进行程序中的内存分配（例如在<code>malloc()</code>或<code>new</code>中）。</p>
<p>默认除去给OS保留的1.5G或7%的内存外的<strong>全部内存</strong>都会被通过这种方式分配给应用。这个默认值可以通过<code>--reserve_memory</code>来改变给系统剩余的内存，或者<code>-m</code>来改变给Seastar应用分配的内存来改变。内存值可以以字节为单位，或者用&quot;k&quot;, &quot;M&quot;, &quot;G&quot;, &quot;T&quot;为单位。这些单位均遵从2的幂。</p>
<p>试着给Seastar超过物理内存的内存值会直接异常退出：</p>
<pre><code class="language-bash">$ ./a.out -m10T
Couldn't start application: std::runtime_error (insufficient physical memory)

</code></pre>
<h1 id="future和continuation简介">future和continuation简介</h1>
<p>future和continuation是Seastar的异步编程模型的基石。通过组合它们可以轻松地组件大型、复杂的异步程序，并保证代码可读、易懂。</p>
<h2 id="future">future</h2>
<p>future是一个还不可用的计算结果。例如：</p>
<ul>
<li>我们从网络读取的数据buffer</li>
<li>计时器的到时</li>
<li>磁盘写操作的完成</li>
<li>一个需要其他一个或多个future的值来进行的计算的值</li>
</ul>
<p><code>future&lt;int&gt;</code>类型承载了一个终将可用的int——现在可能已经可用，或者还不能。成员函数<code>available()</code>可以用来测试一个值是否已经可用，<code>get()</code>可以用来获取它的值。<code>future&lt;&gt;</code>类型表示一些终将完成的事件，但是不会返回任何值。</p>
<p>future往往是一个<strong>异步函数</strong>的返回值。异步函数是指一个返回future，并将会将这个future的值确定下来的函数。因为异步函数_保证_将确定future的值，有时他们被称作&quot;promise&quot;。</p>
<p>Seastar的<code>sleep()</code>函数就是一个简单的异步函数的例子：</p>
<pre><code class="language-cpp">future&lt;&gt; sleep(std::chrono::duration&lt;Rep, Period&gt; dur);
</code></pre>
<p>这个函数设置了一个计时器，从而使在经过指定时间之后future变得可用（并没有携带的值）。</p>
<h2 id="continuation">continuation</h2>
<p><strong>continuation</strong>是一个在future变得可用时会运行的回调函数（常为lambda函数）。continuation会用<code>then()</code>方法附于一个future。例如：</p>
<pre><code class="language-cpp">#include &lt;seastar/core/app-template.hh&gt;
#include &lt;seastar/core/sleep.hh&gt;
#include &lt;iostream&gt;

int main(int argc, char** argv) {
    seastar::app_template app;
    app.run(argc, argv, [] {
        std::cout &lt;&lt; &quot;Sleeping... &quot; &lt;&lt; std::flush;
        using namespace std::chrono_literals;
        return seastar::sleep(1s).then([] {
            std::cout &lt;&lt; &quot;Done.\n&quot;;
        });
    });
}
</code></pre>
<p>在这个例子里，我们从<code>seastar::sleep(1s)</code>中获得了一个future，并把一个打印&quot;Done.&quot;的continuation附于其上。1s后future会变得可用，这时continuation就会被执行。运行这个程序，我们的确会看到立即被打印的&quot;Sleeping...&quot;，和1s后打印的&quot;Done.&quot;，之后程序退出。</p>
<p><code>then()</code>的返回值也是一个future，从而使得链式的continuation变得可能，这点我们之后会提到。现在我们只需要注意我们把这个future作为了<code>app.run</code>的返回值，所以程序会在运行完sleep以及其continuation后才会退出。</p>
<p>为了避免在左右的例子里都重复&quot;app_engine&quot;部分的代码，让我们创建一个可以复用的模板：</p>
<pre><code class="language-cpp">#include &lt;seastar/core/app-template.hh&gt;
#include &lt;seastar/util/log.hh&gt;
#include &lt;iostream&gt;
#include &lt;stdexcept&gt;

extern seastar::future&lt;&gt; f();

int main(int argc, char** argv) {
    seastar::app_template app;
    try {
        app.run(argc, argv, f);
    } catch(...) {
        std::cerr &lt;&lt; &quot;Couldn't start application: &quot;
                  &lt;&lt; std::current_exception() &lt;&lt; &quot;\n&quot;;
        return 1;
    }
    return 0;
}
</code></pre>
<p>使用这个<code>main.cc</code>，上述的sleep例子变成了：</p>
<pre><code class="language-cpp">#include &lt;seastar/core/sleep.hh&gt;
#include &lt;iostream&gt;

seastar::future&lt;&gt; f() {
    std::cout &lt;&lt; &quot;Sleeping... &quot; &lt;&lt; std::flush;
    using namespace std::chrono_literals;
    return seastar::sleep(1s).then([] {
        std::cout &lt;&lt; &quot;Done.\n&quot;;
    });
}
</code></pre>
<p>至此，这个样例非常普通——没有并行，我们用普通的<code>POSIX sleep()</code>也能做到。事情会在我们启动多个<code>sleep</code>的时候变得有趣。<code>future</code>和<code>continuation</code>让并行变得非常简单与自然：</p>
<pre><code class="language-cpp">#include &lt;seastar/core/sleep.hh&gt;
#include &lt;iostream&gt;

seastar::future&lt;&gt; f() {
    std::cout &lt;&lt; &quot;Sleeping... &quot; &lt;&lt; std::flush;
    using namespace std::chrono_literals;
    seastar::sleep(200ms).then([] { std::cout &lt;&lt; &quot;200ms &quot; &lt;&lt; std::flush; });
    seastar::sleep(100ms).then([] { std::cout &lt;&lt; &quot;100ms &quot; &lt;&lt; std::flush; });
    return seastar::sleep(1s).then([] { std::cout &lt;&lt; &quot;Done.\n&quot;; });
}
</code></pre>
<p>每个<code>sleep()</code>和<code>then()</code>都会立即退出：<code>sleep()</code>仅仅启动计时器，而<code>then()</code>只是设置到到时的时候应该调用什么函数。所以3行代码都会马上被执行，f也会直接返回。在那之后，时间循环会开始等3个future变得可用，且每个可用的时候都会运行他们对应的continuation。上述代码的输出显然会是：</p>
<pre><code class="language-cpp">$ ./a.out
Sleeping... 100ms 200ms Done.
</code></pre>
<p><code>sleep()</code>返回的是<code>future&lt;&gt;</code>，也就是在完成时不会返回任何值。更有趣的future 会在可用时返回一个（或多个）任意类型的值。在下面的例子里，有一个返回<code>future&lt;int&gt;</code>的函数，以及对应的<code>continuation</code>。注意<code>continuation</code>是如何得到<code>future</code>中包着的值的：</p>
<pre><code class="language-cpp">#include &lt;seastar/core/sleep.hh&gt;
#include &lt;iostream&gt;

seastar::future&lt;int&gt; slow() {
    using namespace std::chrono_literals;
    return seastar::sleep(100ms).then([] { return 3; });
}

seastar::future&lt;&gt; f() {
    return slow().then([] (int val) {
        std::cout &lt;&lt; &quot;Got &quot; &lt;&lt; val &lt;&lt; &quot;\n&quot;;
    });
}
</code></pre>
<p>我们需要再解释一下<code>slow()</code>。和往常一样，这个函数立即返回一个<code>future&lt;int&gt;</code>，不会等sleep完。但是其中<code>continuation</code><strong>返回的是3，而非<code>future</code></strong>。我们在下文介绍<code>then()</code>是怎么返回future的，以及这种机制是怎么让链式future变得可能的。</p>
<p>这个例子展示了future这种编程模型的便捷性。因为其让程序员可以很简洁地包装复杂的异步程序。<code>slow()</code>可能实际是调用了一个复杂的设计多步的异步操作，但是它可以和<code>sleep()</code>同样被使用，并且Seastar的engine会保证在正确的时间运行continuation。</p>
<h2 id="ready-futures">Ready futures</h2>
<p>一个future可能在运行<code>then()</code>之前就已经准备好了。我们优化过这种这种重要的情况。对于这种情况，往往<code>continuation</code>会被直接运行，而不再用被注册进事件循环，等待事件循环的下一个迭代步了。</p>
<p>在_大多数时候_都会进行上述优化，除了以下情况：<code>then()</code>的内部实现里面维护了一个会记录有多少个continuation被立刻执行了用的计数器，在超过一定量continuation被直接运行后（目前限制为256个），下一个continuation一定会被转移到事件循环里。之所以引入这种机制是因为我们发现在一些情况下（例如后文会讨论的future循环），每个准备好的continuation会立刻生成一个新的准备好的continuation。</p>
<p>那么如果没有上述的计数器限制，我们就会一直在立即执行continuation，而不再进入事件循环，从而导致事件循环饿死（starve）。让事件循环可以正常运行非常重要，不然无法运行在事件循环中的其他的continuation了，也会饿住事件循环会进行的<strong>polling</strong>（例如，检查是否网卡有新的活动 <em>(activity)</em> ），这种polling非常重要。</p>
<p><code>make_ready_future&lt;&gt;</code>可以被用来返回一个已经准备好了的future。下面的例子和之前的几乎完全相同，只是<code>fast()</code>会返回一个已经准备好了的future。所幸future的接受者不在乎这个区别，并会用相同的方式处理这两种情况：</p>
<pre><code class="language-cpp">#include &lt;seastar/core/future.hh&gt;
#include &lt;iostream&gt;

seastar::future&lt;int&gt; fast() {
    return seastar::make_ready_future&lt;int&gt;(3);
}

seastar::future&lt;&gt; f() {
    return fast().then([] (int val) {
        std::cout &lt;&lt; &quot;Got &quot; &lt;&lt; val &lt;&lt; &quot;\n&quot;;
    });
}
</code></pre>
<h1 id="coroutines协程">Coroutines（协程）</h1>
<p>注意：协程需要 C++20 和支持的编译器。 众所周知，Clang 10 及更高版本可以工作。<br>
使用 Seastar 编写高效异步代码的最简单方法是使用协程。 协程不具有传统 continuations（如下）的大部分缺陷，因此是编写新代码的首选方式。</p>
<p>协程是一个函数，这个函数返回 <code>seastar::future&lt;T&gt;</code> ，并且使用 <code>co_await</code> 或 <code>co_return</code>关键字。协程对其调用者和被调用者不可见。它们以任一角色与传统的 Seastar 代码集成。如果您不熟悉 C++ 协程，您可以看 <a href="https://medium.com/pranayaggarwal25/coroutines-in-cpp-15afdf88e17e">A more general introduction to C++ coroutines</a>，本节重点介绍协程如何与 Seastar 集成。</p>
<p>这是一个简单的 Seastar 协程示例：</p>
<pre><code class="language-cpp">#include &lt;seastar/core/coroutine.hh&gt;

seastar::future&lt;int&gt; read();
seastar::future&lt;&gt; write(int n);

seastar::future&lt;int&gt; slow_fetch_and_increment() {
    auto n = co_await read();     // #1
    co_await seastar::sleep(1s);  // #2
    auto new_n = n + 1;           // #3
    co_await write(new_n);        // #4
    co_return n;                  // #5
}

</code></pre>
<p>在#1 中，我们调用 read() 函数，它返回一个<code>future</code>。<code>co_await</code> 关键字指示 <code>Seastar</code> 检查返回的<code>future</code>。</p>
<p>如果<code>future</code>已准备好，则从<code>future</code>中提取值（一个整数）并分配给 n。 如果<code>future</code>未准备好，协程会安排自己在<code>future</code>准备好时被调用，并将控制权返回给<code>seastar</code>。 一旦<code>future</code>准备好，协程就会被唤醒，从<code>future</code>中提取值并赋值给n。</p>
<p>在#2 中，我们调用 seastar::sleep() 并等待返回的<code>future</code>准备就绪，它会在一秒钟内完成。 这表明 n 在 <code>co_await</code> 调用之间被保留，并且协程的作者不需要为协程局部变量安排存储。</p>
<p>第 3 行演示了加法运算，假定读者熟悉它。</p>
<p>在#4 中，我们调用了一个返回 <code>seastar::future&lt;&gt;</code> 的函数。 在这种情况下，<code>future</code>不带数据，因此没有价值被提取和分配。</p>
<p>第 5 行演示了返回值。 整数值用于返回给我们的调用者在调用协程时得到的 <code>future&lt;int&gt;</code>。</p>
<h2 id="exceptions-in-coroutines">Exceptions in coroutines</h2>
<p>#todo，暂时用不到，先不翻译</p>
<h2 id="concurrency-in-coroutines">Concurrency in coroutines</h2>
<p>co_await 运算符允许简单的顺序执行。 多个协程可以并行执行，但每个协程一次只有一个 outstanding 的计算。</p>
<p>#todo，后面有点复杂，先不翻译</p>
<h1 id="continuations延续">Continuations（延续）</h1>
<h2 id="在continuation中捕获状态">在continuation中捕获状态</h2>
<h3 id="按值捕获">按值捕获</h3>
<p>我们已经看到 Seastar Continuations是 lambdas，带着一个future 传递给 then() 方法。<br>
在我们目前看到的例子中，lambda 只不过是匿名函数。 但是 C++11 lambdas 还有一个技巧，这对于 Seastar 中基于future的异步编程非常重要：Lambdas 可以捕获状态。考虑下面的例子：</p>
<pre><code class="language-cpp">#include &lt;seastar/core/sleep.hh&gt;
#include &lt;iostream&gt;

seastar::future&lt;int&gt; incr(int i) {
    using namespace std::chrono_literals;
    return seastar::sleep(10ms).then([i] { return i + 1; });
}

seastar::future&lt;&gt; f() {
    return incr(3).then([] (int val) {
        std::cout &lt;&lt; &quot;Got &quot; &lt;&lt; val &lt;&lt; &quot;\n&quot;;
    });
}

</code></pre>
<p><code>future</code>操作<code>incr(i)</code>需要一定时间执行完成（它需要先睡一会儿......），在这段时间内，它需要保存它正在处理的 i 值。 在早期的事件驱动编程模型中，程序员需要明确定义一个对象来保持这种状态，并管理所有这些对象。在这段时间内，它需要保存它正在处理的 i 值。 在早期的事件驱动编程模型中，程序员需要明确定义一个对象来保持这种状态，并管理所有这些对象。在 Seastar 中，一切都变得简单得多，使用 C++11 的 lambda 表达式：上面示例中的捕获语法“[i]”意味着 i 的值，因为它在调用 incr() 时存在() 被捕获到 lambda 中。 lambda 不仅仅是一个函数——它实际上是一个对象，包含代码和数据。本质上，编译器自动为我们创建了状态对象，我们不需要定义它，也不需要跟踪它（它与<code>Continuations</code>一起保存，当<code>Continuations</code>被延迟时，并在<code>Continuations</code>运行后自动删除） ）。</p>
<p>一个值得理解的实现细节是，当<code>Continuations</code>捕获状态并立即运行时，此捕获不会产生运行时开销。但是，当<code>continuation</code>不能立即运行（因为<code>future</code>还没有准备好）需要保存到后面，就需要在堆上为这个数据分配内存。并且需要将<code>Continuations</code>的捕获数据复制到那里。 这有运行时开销，但它是不可避免的，并且与线程编程模型中的相关开销相比非常小（在线程程序中，这种状态通常驻留在阻塞线程的堆栈上，但堆栈要 比我们的捕获状态大得多，占用大量内存并在这些线程之间的上下文切换时导致大量缓存污染）。</p>
<p>在上面的例子中，我们按值捕获了 i ， i 的值的副本被保存到<code>Continuations</code>中。 C++ 有两个额外的捕获选项：通过引用捕获和通过移动捕获：</p>
<h3 id="引用捕获">引用捕获</h3>
<p>在延续中使用<strong>按引用捕获通常是一个错误，并可能导致严重的错误</strong>。 例如，如果在上面的例子中我们捕获了对 i 的引用，而不是复制它。</p>
<pre><code class="language-cpp">seastar::future&lt;int&gt; incr(int i) {
    using namespace std::chrono_literals;
    // Oops, the &quot;&amp;&quot; below is wrong:
    return seastar::sleep(10ms).then([&amp;i] { return i + 1; });
}
</code></pre>
<p>这意味着延续将包含 i 的地址，而不是它的值。 但是 i 是一个堆栈变量，并且 incr() 函数会立即返回，因此当<code>Continuations</code>最终开始运行时，在 incr() 返回很久之后，该地址将包含不相关的内容。</p>
<p>引用捕获通常是错误规则的一个例外是 do_with() 习语，我们将在后面介绍。 这个习惯用法确保对象在<code>Continuations</code>的整个生命周期中都存在，并使按引用捕获成为可能，并且非常方便。</p>
<h3 id="移动捕获">移动捕获</h3>
<p>在<code>Continuations</code>中使用按<strong>移动捕获</strong>在 Seastar 应用程序中也非常有用。 通过将一个对象移动到一个<code>Continuations</code>中，我们将这个对象的所有权转移给了<code>Continuations</code>，并使得在<code>Continuations</code>结束时对象很容易被自动删除。</p>
<p>例如，考虑一个传统的函数<code>std::unique_ptr&lt;T&gt;</code>。</p>
<pre><code class="language-cpp">int do_something(std::unique_ptr&lt;T&gt; obj) {
     // 根据obj的内容做一些计算，假设结果是17
     return 17;
     // 此时， obj 超出范围，因此编译器 delete()s 它。
</code></pre>
<p>通过以这种方式使用 unique_ptr，调用者将一个对象传递给函数，但告诉它该对象现在是它的专属责任——当函数完成该对象时，它会自动删除它。 我们如何在<code>Continuations</code>中使用 unique_ptr ？ 以下将不起作用</p>
<pre><code class="language-cpp">seastar::future&lt;int&gt; slow_do_something(std::unique_ptr&lt;T&gt; obj) {
    using namespace std::chrono_literals;
    // The following line won't compile...
    return seastar::sleep(10ms).then([obj] () mutable { return do_something(std::move(obj)); });
}
</code></pre>
<p>问题在于，unique_ptr 不能通过值传递到<code>Continuations</code>中，因为这需要复制它，这是被禁止的，因为它违反了仅存在此指针的一个副本的保证。 但是，我们可以将 obj 移动到<code>Continuations</code>中：</p>
<pre><code class="language-cpp">seastar::future&lt;int&gt; slow_do_something(std::unique_ptr&lt;T&gt; obj) {
    using namespace std::chrono_literals;
    return seastar::sleep(10ms).then([obj = std::move(obj)] () mutable {
        return do_something(std::move(obj));
    });
}
</code></pre>
<p>这里使用 std::move() 导致 obj 的移动赋值用于将对象从外部函数移动到延续中。 C++11 中引入的移动（移动语义）的概念类似于浅拷贝，然后使源拷贝无效（这样两个拷贝就不会共存，因为 unique_ptr 禁止这样做）。 将 obj 移入 continuation 后，上层函数就不能再使用它（in this case it's of course ok, 因为我们无论如何都会返回）。</p>
<h3 id="c14捕获语法">c++14捕获语法</h3>
<p>我们在这里使用的 <code>[obj = ...]</code> <strong>捕获语法是 <code>C++14</code> 的新内容</strong>。 这是 Seastar 需要 C++14，并且不支持旧的 C++11 编译器的主要原因。</p>
<p>这里需要额外的 () 可变语法，因为默认情况下，当 C++ 将一个值（在这种情况下，std::move(obj) 的值）捕获到一个 lambda 中时，它使这个值成为只读的，所以我们的 lambda 不能， 在本例中，再次移动它。 添加 mutable 消除了这种人为限制。</p>
<h2 id="求值顺序的注意事项">求值顺序的注意事项</h2>
<p><strong>（只在c++14）</strong></p>
<p>C++14（及更低版本的C++）不能保证<code>continuation</code>中的<code>lambda</code> 捕获会在他们相关的<code>future</code>被计算之后才被求值（见 <a href="https://en.cppreference.com/w/cpp/language/eval_order%EF%BC%89%E3%80%82">https://en.cppreference.com/w/cpp/language/eval_order）。</a></p>
<p>因此，请避免以下编程模式：</p>
<pre><code class="language-cpp">    return do_something(obj).then([obj = std::move(obj)] () mutable {
        return do_something_else(std::move(obj));
    });
</code></pre>
<p>这个例子中<code>[obj = std::move(obj)]</code>可能会先于<code>do_something(obj)</code>，导致出现使用了被move出去之后的<code>obj</code>。</p>
<p>为了保证我们期望的顺序，上面的表达式可以拆分为：</p>
<pre><code class="language-cpp">    auto fut = do_something(obj);
    return fut.then([obj = std::move(obj)] () mutable {
        return do_something_else(std::move(obj));
    });

</code></pre>
<p>c++17后，<code>then</code>对应的对象会在<code>then</code>的参数之前才被求值，使得<code>do_something(obj)</code>会保证先被执行。所以C++17中可以不采用上面的改正方法。</p>
<h2 id="链式continuation">链式continuation</h2>
<p>TODO: 我们已经在上面的 slow() 中看到了链接示例。 谈论从then 的返回，并返回future 并链接更多then。</p>
<h1 id="处理异常">处理异常</h1>
<p>在<code>continuation</code>中抛出的异常被系统隐式捕获并存储在<code>future</code>。 存储此类异常的 <code>future</code> 与就绪的 <code>future</code> 类似，因为它可以启动<code>continuation</code>，但它不包含值——仅包含异常。</p>
<p>在这样的<code>future</code>上调用 <code>.then()</code> 会跳过<code>continuation</code>，并将输入<code>future</code>（调用 .then() 的对象）的异常转移到输出<code>future</code>（<code>.then()</code> 的返回值）。</p>
<pre><code class="language-cpp">line1();
line2(); // throws!
line3(); // skipped
</code></pre>
<p>类似于</p>
<pre><code class="language-cpp">return line1().then([] {
    return line2(); // throws!
}).then([] {
    return line3(); // skipped
});
</code></pre>
<p>通常，需要中止当前的操作链并返回异常，但有时需要更细粒度的控制。 有几种用于处理异常的原语：</p>
<ol>
<li>.then_wrapped(): .then_wrapped() 不是将<code>future</code>携带的值传递给<code>continuation</code>，而是将输入<code>future</code>传递给<code>continuation</code>。 保证<code>future</code>处于就绪状态，因此<code>continuation</code>可以检查它是否包含值或异常，并采取适当的行动。</li>
<li>.finally()：类似于Java的finally块，无论输入的<code>future</code>是否带有异常，.finally()延续都会被执行。 finally 延续的结果是它的输入<code>future</code>，因此 .finally() 可用于在无条件执行的流中插入代码，否则不会改变流。<br>
TODO：给出上面的示例代码。 还要提到 handle_exception，可能会延迟到后面的章节。</li>
</ol>
<h2 id="exceptions-vs-exceptional-futures">Exceptions vs. exceptional futures</h2>
<p>异步函数可以通过以下两种方式之一失败：它可以立即失败，通过抛出异常，或者它可以返回一个最终会失败的未来（解析为异常）。 这两种失败模式看起来与未初始化(uninitiated)相似。但在尝试使用 finally()、handle_exception() 或 then_wrapped() 处理异常时，行为会有所不同。 例如，考虑以下代码：</p>
<pre><code class="language-cpp">#include &lt;seastar/core/future.hh&gt;
#include &lt;iostream&gt;
#include &lt;exception&gt;

class my_exception : public std::exception {
    virtual const char* what() const noexcept override { return &quot;my exception&quot;; }
};

seastar::future&lt;&gt; fail() {
    return seastar::make_exception_future&lt;&gt;(my_exception());
}

seastar::future&lt;&gt; f() {
    return fail().finally([] {
        std::cout &lt;&lt; &quot;cleaning up\n&quot;;
    });
}

</code></pre>
<p>正如预期的那样，此代码将打印<code>cleaning up</code>消息 - 异步函数 fail() 返回一个解析为失败的<code>future</code>，并且 finally() 继续运行，尽管有此失败。</p>
<p>现在考虑在上面的例子中我们对 fail() 有不同的定义：</p>
<pre><code class="language-cpp">seastar::future&lt;&gt; fail() {
    throw my_exception();
}
</code></pre>
<p>在这里，fail() 不会返回失败的<code>future</code>。 相反，它根本无法返回<code>future</code>！ 它抛出的异常会停止整个函数 f()，并且 finally() 延续不会附加到<code>future</code>（从未返回），并且永远不会运行。 现在不打印<code>cleaning up</code>消息。</p>
<p>我们建议为了减少出现此类错误的机会，异步函数应始终返回失败的<code>future</code>，而不是抛出实际异常。 如果异步函数在返回<code>future</code>之前调用另一个函数，并且第二个函数可能会抛出，它应该使用 try/catch 来捕获异常并将其转换为失败的<code>future</code>：</p>
<pre><code class="language-cpp">void inner() {
    throw my_exception();
}
seastar::future&lt;&gt; fail() {
    try {
        inner();
    } catch(...) {
        return seastar::make_exception_future(std::current_exception());
    }
    return seastar::make_ready_future&lt;&gt;();
}
</code></pre>
<p>在这里，fail() 捕获由 inner() 抛出的异常，无论它可能是什么，并返回一个失败的<code>future</code>。 以这种方式编写，将到达 finally() 延续，并打印<code>cleaning up</code>消息。</p>
<p>尽管建议异步函数避免抛出异常，但一些异步函数除了返回异常<code>future</code>外，还会抛出异常。 一个常见的例子是分配内存并在内存不足时抛出 <code>std::bad_alloc</code> 的函数，而不是返回<code>future</code>。 <code>future&lt;&gt; seastar::semaphore::wait()</code> 方法就是这样的一个函数：它返回一个<code>future</code>，如果信号量 <code>was broken()</code> 或等待超时，它可能是异常的，但也可能在分配内存（用来保存waiters列表的内存）失败时抛出异常。 因此，除非一个函数——包括异步函数——被明确标记为“noexcept”，否则应用程序应该准备好处理从它抛出的异常。 在现代 C++ 中，代码通常<strong>使用 RAII</strong> 来保证异常安全，而不用 try/catch。 <strong>seastar::defer() 是一个基于 RAII 的习惯用法</strong>，它确保即使抛出异常也能运行一些清理代码。</p>
<p>Seastar 有一个方便的通用函数<code>futurize_invoke()</code>，在这里很有用。<code>futurize_invoke(func, args...)</code> 运行一个可能返回<code>future</code>值或立即值的函数。并且在这两种情况下都将结果转换为未来值。<code>futurize_invoke()</code> 还将函数抛出的立即异常（如果有）转换为失败的<code>future</code>，就像我们上面所做的一样。</p>
<p>所以使用 <code>futurize_invoke()</code> 我们可以使上面的例子工作，即使 <code>fail()</code> 确实抛出了异常：</p>
<pre><code class="language-cpp">seastar::future&lt;&gt; fail() {
    throw my_exception();
}
seastar::future&lt;&gt; f() {
    return seastar::futurize_invoke(fail).finally([] {
        std::cout &lt;&lt; &quot;cleaning up\n&quot;;
    });
}
</code></pre>
<p>请注意，如果异常风险存在于<code>continuation</code>中，则大部分讨论将变得毫无意义。 考虑以下代码：</p>
<pre><code class="language-cpp">seastar::future&lt;&gt; f() {
    return seastar::sleep(1s).then([] {
        throw my_exception();
    }).finally([] {
        std::cout &lt;&lt; &quot;cleaning up\n&quot;;
    });
}
</code></pre>
<p>在这里，第一个<code>continuation</code>的 <code>lambda</code> 函数确实抛出异常而不是返回失败的<code>future</code>。 然而，我们没有像以前一样的问题，这只是因为异步函数在返回一个有效的<code>future</code>之前抛出了一个异常。</p>
<h1 id="管理生命周期">管理生命周期</h1>
<p>一个异步函数可能会启动一个会在本函数返回后还会运行很久的操作：函数自身会在运行的时候立即返回一个<code>future&lt;T&gt;</code>，但是等这个future准备好需要很久。</p>
<p>当这样的异步操作需要操作现存的对象，或者是临时对象，我们需要留心这些对象的_生命周期_：我们得保证这些对象不会在异步操作完成之前被释放，并保证对象在不被需要的时候能最终被释放（以防内存泄漏）。Seastar提供了很多种用于安全高效地保证对象在合适的时间保活的机制。在本节中，我们会探索这些机制，并告诉大家每一种该何时使用。</p>
<h2 id="传递所有权给continuation">传递所有权给continuation</h2>
<p>我们已经看到如何通过<strong>捕获</strong>(capturing)来让<code>continuation</code>获取一个对象的所有权：</p>
<pre><code class="language-cpp">seastar::future&lt;&gt; slow_incr(int i) {
    return seastar::sleep(10ms).then([i] { return i + 1; });
}

</code></pre>
<p>在上例中，continuation捕获了<code>i</code>的值，换句话说，continuation有一份<code>i</code>的拷贝。当continuation在10ms后开始运行时，它可以访问到这个值，并在其运行结束的时候，continuation本身这个对象会被释放，它捕获的<code>i</code>的拷贝也随之被释放了。continuation拥有<code>i</code>的这个拷贝。</p>
<p>如我们在这里进行的用值捕获(capturing by value)——创建一个我们需要的对象的拷贝——大多数情况下适用于例子中像整数这样的很小的对象。其他一些对象的拷贝成本可能很高，甚至不能被拷贝。例如，下面这么做就不太好。</p>
<pre><code class="language-cpp">seastar::future&lt;&gt; slow_op(std::vector&lt;int&gt; v) {
    // this makes another copy of v:
    return seastar::sleep(10ms).then([v] { /* do something with v */ });
}

</code></pre>
<p>上面这样可能会很低效——因为需要拷贝可能很长的vector<code>v</code>，并且要在continuation中存一份。在本例中，我们没必要复制<code>v</code>——在<code>slow_op</code>中我们已经传值了，并且在capture之后没有再对<code>v</code>做其他的操作了，所以<code>slow_op</code>再返回的时候会释放自己的那份<code>v</code>。</p>
<p>对于这种情况，C++14允许把对象move进continuation：</p>
<pre><code class="language-cpp">seastar::future&lt;&gt; slow_op(std::vector&lt;int&gt; v) {
    // v is not copied again, but instead moved:
    return seastar::sleep(10ms).then([v = std::move(v)] { /* do something with v */ });
}
</code></pre>
<p>C++11引入了move constructor，可以把vector的数据移进<code>continuation</code>，并释放原始的vector。Moving会很快——对于vector，moving操作只需要复制几个指针这样的小field。如之前那样，一旦continuation退出了vector就会被释放——并且其底层的数组（是被move操作移进来的）也会随之被释放。</p>
<p>TODO: 将临时缓冲区作为设计为以这种方式移动的对象的示例。</p>
<p>一些情况下，move对象不是很合适。例如，一些对象的引用或者一些成员以引用的形式被存在其他对象里了，那么这些引用会在这个对象被move之后变得不可用。在一些更复杂的例子里，move constructor甚至都有些慢。对于这些情况，C++提供了<code>std::unique_ptr&lt;T&gt;</code>。<code>std::unique_ptr&lt;T&gt;</code>是一个拥有另一个类型的<code>T</code>的对象的对象。当<code>std::unique_ptr&lt;T&gt;</code>被move了，内部的<code>T</code>对象并没有变化——仅仅是指向<code>T</code>对象的指针发生了变化。<code>std::unique_ptr&lt;T&gt;</code>的用例如下：</p>
<pre><code class="language-cpp">seastar::future&lt;&gt; slow_op(std::unique_ptr&lt;T&gt; p) {
    return seastar::sleep(10ms).then([p = std::move(p)] { /* do something with *p */ });
}

</code></pre>
<p><code>std::unique_ptr&lt;T&gt;</code>是传递unique 对象所有权的C++标准机制：对象在同一时刻只会被一段代码拥有，且所有权通过move <code>unique_ptr</code>对象进行转移。<code>unique_ptr</code>对象不能被复制：如果我们尝试用值去捕获<code>p</code>，会引发编译错误。</p>
<h2 id="保持调用者的所有权">保持调用者的所有权</h2>
<p>我们上面描述的技术——赋予它需要处理的对象的<code>continuation</code>所有权——是强大且安全的。 但通常它会变得难以使用且冗长。 当异步操作不仅涉及一个<code>continuation</code>，而且涉及一系列<code>continuation</code>，每个<code>continuation</code>都需要在同一个对象上工作时，我们需要在每个连续的<code>continuation</code>之间传递对象的所有权，这会变得不方便。 当我们需要将同一个对象传递给两个单独的异步函数（或<code>continuation</code>）时，这尤其不方便——在我们将对象移入一个之后，需要返回该对象，以便它可以再次移入第二个。</p>
<pre><code class="language-cpp">seastar::future&lt;&gt; slow_op(T o) {
    return seastar::sleep(10ms).then([o = std::move(o)] {
        // first continuation, doing something with o
        ...
        // return o so the next continuation can use it!
        return std::move(o);
    }).then([](T o) {
        // second continuation, doing something with o
        ...
    });
}
</code></pre>
<p>之所以会出现这种复杂性，是因为我们希望异步函数和<code>continuation</code>获得它们所操作对象的所有权。 一种更简单的方法是让异步函数的调用者<code>continuation</code>是对象的所有者，并将对象的引用传递给需要该对象的各种其他异步函数和<code>continuation</code>。比如：</p>
<pre><code class="language-cpp">seastar::future&lt;&gt; slow_op(T&amp; o) {           // &lt;-- pass by reference
    return seastar::sleep(10ms).then([&amp;o] {// &lt;-- capture by reference
        // first continuation, doing something with o
        ...
    }).then([&amp;o]) {                        // &lt;-- another capture by reference
        // second continuation, doing something with o
        ...
    });
}

</code></pre>
<p>这种方法提出了一个问题：slow_op 的调用者现在负责保持对象 o 处于活动状态，而由 slow_op 启动的异步代码需要这个对象。 但是这个调用者如何知道它启动的异步操作实际需要这个对象多长时间呢？</p>
<p>最合理的答案是异步函数可能需要访问其参数，直到它返回的<code>future</code>得到解决——此时异步代码完成并且不再需要访问其参数。 因此，我们建议 Seastar 代码采用以下约定：</p>
<blockquote>
<p>每当异步函数通过引用获取参数时，调用者必须确保被引用的对象一直存在，直到函数返回的未来被解析。</p>
</blockquote>
<p>请注意，这只是 Seastar 建议的约定，不幸的是 C++ 语言中没有强制执行它。 非 Seastar 程序中的 C++ 程序员经常将大对象作为常量引用传递给函数，以避免慢速复制，并假设被调用的函数不会将这个引用保存在任何地方。 但在 Seastar 代码中，这是一种危险的做法，因为即使异步函数不打算将引用保存在任何地方，它最终可能会通过将此引用传递给另一个函数并最终在<code>continuation</code>中捕获它来隐式保存。</p>
<blockquote>
<p>如果 C++ 的未来版本可以帮助我们发现引用的错误使用，那就太好了。 也许我们可以有一个特殊类型的引用的标签，一个函数可以立即使用的“立即引用”（即在返回<code>future</code>之前），但不能被捕获到<code>continuation</code>中。</p>
</blockquote>
<p>有了这个约定，就可以很容易地编写复杂的异步函数，比如 slow_op，它通过引用传递对象，直到异步操作完成。 但是调用者如何确保对象在返回的<code>future</code>被解析之前一直存在？ 以下是错误的：</p>
<pre><code class="language-cpp">seastar::future&lt;&gt; f() {
    T obj; // wrong! will be destroyed too soon!
    return slow_op(obj);
}
</code></pre>
<p>这是错误的，因为这里的对象 obj 对 f 的调用是本地的，并且一旦 f 返回一个<code>future</code>就被销毁——而不是当这个返回的<code>future</code>被解析时！ 正确的做法是调用者在堆上创建对象 obj（所以它不会在 f 返回时立即被销毁），然后运行 slow_op(obj) 并在<code>future</code>解析时（即，使用 .finally ())，销毁对象。</p>
<p>Seastar 提供了一个方便的习惯用法 do_with() 来正确执行此操作：</p>
<pre><code class="language-cpp">seastar::future&lt;&gt; f() {
    return seastar::do_with(T(), [] (auto&amp; obj) {
        // obj is passed by reference to slow_op, and this is fine:
        return slow_op(obj);
    }
}

</code></pre>
<p>do_with 将使用给定的保活对象执行给定的函数。<br>
do_with 将给定的对象保存在堆上，并使用对新对象的引用调用给定的 lambda。 最后，它确保在算出返回<code>future</code>的后销毁新对象。<br>
通常，do_with 被赋予一个右值，即一个未命名的临时对象或一个 std::move()的对象，并且 do_with 将该对象移动到它在堆上的最终位置。<br>
do_with 返回一个在上述所有内容完成后算出的<code>future</code>（lambda 的<code>future</code>被算出并且对象被销毁）。</p>
<p>为方便起见，do_with 也可以被赋予多个对象以保持活动状态。 例如，这里我们创建了两个对象并让它们保持活动状态，直到算出<code>future</code>：</p>
<pre><code class="language-cpp">seastar::future&lt;&gt; f() {
    return seastar::do_with(T1(), T2(), [] (auto&amp; obj1, auto&amp; obj2) {
        return slow_op(obj1, obj2);
    }
}

</code></pre>
<p>虽然 do_with 可以改变它持有的对象的生命周期，但如果用户不小心复制了这些对象，这些副本可能有错误的生命周期。 不幸的是，像忘记“&amp;”这样的简单错误会导致这种意外的复制。 例如，以下代码是错误的：</p>
<pre><code class="language-cpp">seastar::future&lt;&gt; f() {
    return seastar::do_with(T(), [] (T obj) { // WRONG: should be T&amp;, not T
        return slow_op(obj);
    }
}

</code></pre>
<p>在这个错误的片段中， obj 不是对 do_with 分配的对象的引用，而是它的一个副本——一个副本一旦 lambda 函数返回就会被销毁（lambda函数返回，是说在主线程里面），而不是在它返回的<code>future</code>被解析时被销毁。 这样的代码很可能会崩溃，因为对象在被释放后被使用。 不幸的是，编译器不会警告此类错误。 用户应该习惯于始终使用带有 do_with 的类型“auto&amp;”——如上面正确的例子——以减少出现此类错误的机会。</p>
<p>出于同样的原因，以下代码片段也是错误的：</p>
<pre><code class="language-cpp">seastar::future&lt;&gt; slow_op(T obj); // WRONG: should be T&amp;, not T
seastar::future&lt;&gt; f() {
    return seastar::do_with(T(), [] (auto&amp; obj) {
        return slow_op(obj);
    }
}
</code></pre>
<p>在这里，虽然 obj 被正确地通过引用传递给了 lambda，但我们后来不小心传递了一个 slow_op() 它的副本（因为这里slow_op 是通过值而不是通过引用获取对象），并且这个副本会在slow_op 返回后立即被销毁 ，不要等到返回的算出<code>future</code>。</p>
<p>使用 do_with 时，请始终记住它需要遵守上述约定：我们在 do_with 内部调用的异步函数在返回的 future 被解析后不得使用 do_with 持有的对象。 对于异步函数来说，这是一个严重的释放后使用错误，返回一个未来解决，同时仍然使用 do_with() 的对象进行后台操作。</p>
<p>通常，在留下后台操作的同时解析异步函数很少是一个好主意 - 即使这些操作不使用 do_with() 的对象。 我们不等待的后台操作可能会导致我们耗尽内存（如果我们不限制它们的数量）并且很难干净地关闭应用程序。</p>
<h2 id="共享所有权引用计数">共享所有权（引用计数）</h2>
<p>在本章的开头，我们已经注意到，将对象的副本捕获到<code>continuation</code>中是确保在<code>continuation</code>运行时对象处于活动状态并随后被销毁的最简单方法。 但是，复制复杂对象通常很昂贵（在时间和内存方面）。 有些对象根本无法复制，或者是可读写的，并且<code>continuation</code>应该修改原始对象，而不是新的副本。 所有这些问题的解决方案是引用计数，也就是共享对象：</p>
<p>Seastar 中引用计数对象的一个简单示例是 <code>seastar::file</code>，一个保存打开文件对象的对象（我们将在后面的部分介绍 <code>seastar::file</code>）。文件对象可以被复制，但复制不涉及复制文件描述符（更不用说文件了）。 相反，两个副本都指向同一个打开的文件，并且引用计数增加 1。当一个文件对象被销毁时，文件的引用计数减一，只有当引用计数达到 0 时，底层文件才真正关闭。</p>
<p>文件对象可以非常快速地复制并且所有副本实际上都指向同一个文件，这使得将它们传递给异步代码非常方便； 例如，</p>
<pre><code class="language-cpp">seastar::future&lt;uint64_t&gt; slow_size(file f) {
    return seastar::sleep(10ms).then([f] {
        return f.size();
    });
}
</code></pre>
<p>注意调用 <code>slow_size</code> 就像调用 <code>slow_size(f)</code> 一样简单，传递 f 的副本，不需要做任何特殊的事情来确保 f 只在不再需要时被销毁。 当没有任何东西再提到 f 时，这很自然地发生。</p>
<p>你可能想知道为什么上面例子中的 <code>return f.size()</code> 是安全的：它不是在 f 上启动一个异步操作吗（文件的大小可能存储在磁盘上，所以不是立即可用的），并且 f 可能会立即被销毁，当 我们回来了，什么都没有持有 f 的副本？ 如果 f 真的是最后一个引用，那确实是一个错误，但还有另一个：文件永远不会关闭。 使代码有效的假设是有另一个对 f 的引用将用于关闭它。 <code>close</code> 成员函数持有该对象的引用计数，因此即使没有其他人继续持有它，它也会继续存在。 由于文件对象生成的所有<code>future</code>在关闭之前已完成，因此正确性所需的只是记住始终关闭文件。</p>
<p>引用计数有运行时成本，但通常非常小； 重要的是要记住 Seastar 对象始终仅由单个 CPU 使用，因此引用计数递增和递减操作并不是采用经常用来做引用计数的慢速原子操作的这种方案，而只是常规的 CPU 本地整数操作。 此外，明智地使用 std::move() 和编译器的优化器可以减少不必要的来回增加和减少引用计数的次数。</p>
<p>C++11 提供了一种创建引用计数共享对象的标准方法——使用模板 <code>std::shared_ptr&lt;T&gt;</code>。 <code>shared_ptr</code> 可用于将任何类型包装到引用计数共享对象中，如上面的 <code>seastar::file</code>。 然而，标准 <code>std::shared_ptr</code> 是为多线程应用程序设计的，因此它对引用计数使用缓慢的原子递增/递减操作，我们已经注意到在 <code>Seastar</code> 中这是不必要的。 为此，<code>Seastar</code> 提供了自己的该模板的单线程实现，<code>seastar::shared_ptr&lt;T&gt;</code>。 除了不使用原子操作外，它类似于<code>std::shared_ptr&lt;T&gt;</code>【太牛了】。</p>
<p>此外，<code>Seastar</code> 还提供了一个开销更低的<code>shared_ptr</code>变体：<code>seastar::lw_shared_ptr&lt;T&gt;</code>。 由于需要正确支持多态类型（由一个类创建的共享对象，并通过指向基类的指针访问），功能齐全的 <code>shared_ptr</code> 变得复杂。 它使得 <code>shared_ptr</code> 需要向共享对象添加两个变量，并且每个 <code>shared_ptr</code> 副本添加两个变量。 简化的 <code>lw_shared_ptr</code> - 不支持多态类型 - 在对象中只添加一个变量（引用计数），并且每个副本只是一个变量 - 就像复制常规指针一样。 出于这个原因，轻量级的 <code>seastar::lw_shared_ptr&lt;T&gt;</code>应该尽可能优先（T 不是多态类型），否则 <code>seastar::shared_ptr&lt;T&gt;</code>。 更慢的 <code>std::shared_ptr&lt;T&gt;</code>永远不应该用在分片 <code>Seastar</code> 应用程序中。</p>
<h2 id="在堆栈上保存对象">在堆栈上保存对象</h2>
<p>如果我们可以像通常在同步代码中那样将对象保存在堆栈上，那不是很方便吗？ 类似于：</p>
<pre><code class="language-cpp">int i = ...;
seastar::sleep(10ms).get();
return i;

</code></pre>
<p><code>Seastar</code> 允许编写此类代码，方法是使用带有自己的堆栈的<code>seastar::thread</code> 对象。 使用<code>seastar::thread</code>的完整示例可能如下所示：</p>
<pre><code class="language-cpp">seastar::future&lt;&gt; slow_incr(int i) {
    return seastar::async([i] {
        seastar::sleep(10ms).get();
        // We get here after the 10ms of wait, i is still available.
        return i + 1;
    });
}

</code></pre>
<p>我们会在Seastar thread一节介绍 <code>seastar::thread</code>, <code>seastar::async()</code> 和 <code>seastar::future::get()</code> 。</p>
<h1 id="高级future">高级future</h1>
<h2 id="fututre和中断">fututre和中断</h2>
<p>TODO：一个<code>future</code>，例如<code>sleep(10s)</code> 不能被中断。 所以如果我们需要，<code>promise</code> 需要有一个机制来中断它。 提到管道的关闭功能，信号量停止功能等。</p>
<h2 id="future只能被用一次">future只能被用一次</h2>
<p>TODO：假设我们有一个<code>future&lt;int&gt;</code>变量，一旦我们 <code>get()</code>或<code>then()</code> 它，它就会变得无效——我们需要将值存储在其他地方。 想想是否有我们可以建议的替代方案。</p>
<h1 id="fibers纤维">Fibers（纤维？）</h1>
<p>这些<code>fiber</code>并不是线程——每个都只是一串<code>continuation</code>——但是他们也有和传统线程一样的一些要求。例如，我们会避免一个<code>fiber</code>被<code>starve</code>，而另一个一直在运行。又如，<code>fiber</code>之间可能会进行通信——一个fiber生成另一个fiber使用的数据，我们希望能保证2个fiber都能运行，并且如果1个过早地停止了，另一个不要一直<code>hang</code>。</p>
<p>TODO：讨论与Fibers相关的部分，如循环、信号量、门、管道等。</p>
<h1 id="loops循环">Loops（循环）</h1>
<p>大多数很消耗时间的计算都需要循环。Seastar提供了很多用future/promise的方式表示循环的原语。有关Seastar的循环原语一个非常重要的点是，每个迭代步的后面都会有一个抢占点(preemption point)，因而允许其他任务在迭代步之间运行。</p>
<h2 id="repeat">repeat</h2>
<p>一个用<code>repeat</code>创建的循环会执行其body，然后收到一个<code>stop_iteration</code>对象。这个对象会告诉循环是该继续运行（<code>stop_iteration::no</code>）还是该停止（<code>stop_iteration::yes</code>），下一个迭代步会在前一个执行完再执行。被传给<code>repeat</code>的body需要返回<code>future&lt;stop_iteration&gt;</code>。</p>
<pre><code class="language-cpp">seastar::future&lt;int&gt; recompute_number(int number);

seastar::future&lt;&gt; push_until_100(
			seastar::lw_shared_ptr&lt;std::vector&lt;int&gt;&gt; queue,
			int element) {
    return seastar::repeat([queue, element] {
        if (queue-&gt;size() == 100) {
            return make_ready_future&lt;stop_iteration&gt;(stop_iteration::yes);
        }
        return recompute_number(element).then([queue] (int new_element) {
            queue-&gt;push_back(element);// 感觉这里应该是new_element
            return stop_iteration::no;
        });
    });
}

</code></pre>
<h2 id="do_until">do_until</h2>
<p><code>do_until</code>和<code>repeat</code>非常接近，只是其会显示传入一个判断条件。下列代码展示了该如何使用<code>do_until</code>：</p>
<pre><code class="language-cpp">seastar::future&lt;int&gt; recompute_number(int number);

seastar::future&lt;&gt; push_until_100(seastar::lw_shared_ptr&lt;std::vector&lt;int&gt;&gt; queue, int element) {
    return seastar::do_until([queue] { return queue-&gt;size() == 100; }, [queue, element] {
        return recompute_number(element).then([queue] (int new_element) {
            queue-&gt;push_back(new_element);
        });
    });
}

</code></pre>
<p>注意循环body需要返回<code>future&lt;&gt;</code>，从而使我们能够在循环中创建复杂的continuation。</p>
<h2 id="do_for_each">do_for_each</h2>
<p><code>do_for_each</code> 相当于 Seastar 世界中的 for 循环。 它接受一个范围（或一对迭代器）和一个函数体，它按顺序一个一个地应用于每个参数。 只有在第一个迭代完成后才会启动下一个迭代，就像 <code>repeat</code> 一样。 像往常一样，<code>do_for_each</code> 期望它的循环体返回一个 <code>future&lt;&gt;</code>。</p>
<pre><code class="language-cpp">seastar::future&lt;&gt; append(
			seastar::lw_shared_ptr&lt;std::vector&lt;int&gt;&gt; queue1, 
			seastar::lw_shared_ptr&lt;std::vector&lt;int&gt;&gt; queue2) {
    return seastar::do_for_each(queue2, [queue1] (int element) {
        queue1-&gt;push_back(element);
    });
}

seastar::future&lt;&gt; append_iota(
			seastar::lw_shared_ptr&lt;std::vector&lt;int&gt;&gt; queue1,
			int n) {
    return seastar::do_for_each(
			boost::make_counting_iterator&lt;size_t&gt;(0), 
			boost::make_counting_iterator&lt;size_t&gt;(n),
			[queue1] (int element) {
        		queue1-&gt;push_back(element);
			});
}

</code></pre>
<p><code>do_for_each</code> 接受对容器的左值引用或一对迭代器。 这意味着在整个循环执行期间确保容器处于活动状态的责任属于调用者。 如果容器需要延长其生命周期，可以使用 <code>do_with</code> 轻松实现：</p>
<pre><code class="language-cpp">seastar::future&lt;&gt; do_something(int number);

seastar::future&lt;&gt; do_for_all(std::vector&lt;int&gt; numbers) {
    // Note that the &quot;numbers&quot; vector will be destroyed as soon as this function
    // returns, so we use do_with to guarantee it lives during the whole loop execution:
    return seastar::do_with(std::move(numbers), [] (std::vector&lt;int&gt;&amp; numbers) {
        return seastar::do_for_each(numbers, [] (int number) {
            return do_something(number);
        });
    });
}
</code></pre>
<h2 id="parallel_for_each">parallel_for_each</h2>
<p><code>parallel_for_each</code> 是 <code>do_for_each</code> 的高并发变体。 使用 <code>parallel_for_each</code> 时，所有迭代都会同时排队——这意味着无法保证它们以何种顺序完成操作。</p>
<pre><code class="language-cpp">seastar::future&lt;&gt; flush_all_files(seastar::lw_shared_ptr&lt;std::vector&lt;seastar::file&gt;&gt; files) {
    return seastar::parallel_for_each(files, [] (seastar::file f) {
        // file::flush() returns a future&lt;&gt;
        return f.flush();
    });
}

</code></pre>
<p><code>parallel_for_each</code> 是一个强大的工具，因为它允许并行生成许多任务。 这可能会带来巨大的性能提升，但也有一些注意事项。 首先，过高的并发可能会很麻烦——详细信息可以在限制循环的并行性(<strong>Limiting parallelism of loops</strong>.)一章中找到。</p>
<p>要通过整数限制 <code>parallel_for_each</code> 的并发性，请使用下面描述的 <code>max_concurrent_for_each</code>。 有关处理并行性的更多详细信息，请参见限制循环的并行性(<strong>Limiting parallelism of loops</strong>.)一章。</p>
<p>其次，请注意，在 <code>parallel_for_each</code> 循环中执行迭代的顺序是任意的——如果需要严格的顺序，请考虑使用 <code>do_for_each</code>。</p>
<p>TODO：<code>map_reduce</code>，作为 <code>parallel_for_each</code> 的快捷方式（？），它需要产生一些结果（例如，布尔结果的<code>logical_or</code>），因此我们不需要显式创建 <code>lw_shared_ptr</code>（或 <code>do_with</code>）。</p>
<p>TODO：请参阅 seastar 提交“input_stream: Fix possible infinite recursion in consume()”的示例，了解为什么递归是可能替代<code>repeat()</code>的，但很糟糕。 另见我的评论  <a href="https://groups.google.com/d/msg/seastar-dev/CUkLVBwva3Y/3DKGw-9aAQAJ">为什么 Seastar 的迭代原语应该用于尾调用优化</a> 。</p>
<p>【摘录并翻译于此】</p>
<blockquote>
<p><strong>Nadav Har'El</strong><br>
抱歉，伙计们（Avi 实际上最初编写了这段代码，但我对其进行了大修，本应该早已发现这一点）。<br>
我在 Seastar 教程中给自己做了个笔记来解释为什么递归 <strong>不是</strong> Seastar 迭代原语（do_until、repeat 等）的良好替代品。<br>
话虽如此，这里有一些我没有完全理解的东西：<br>
代码做这样的事情：</p>
<pre><code class="language-cpp">future&lt;&gt; consume() {  
return fd.get().then([] { return consume(); });  
}
</code></pre>
<p>所以如果 <code>fd.get()</code> 没有准备好，我们这里根本没有递归——<code>consumer()</code> 立即返回一个 <code>future</code>，稍后会再次调用（没有递归）。<br>
所以递归只有在 fd.get() 立即准备好时才会发生。<br>
但是当此代码立即运行时，由于“返回”并且编译器可以确定代码不需要任何堆栈上的变量，编译器可以进行“尾调用优化”以<em>替换</em> 最后一帧，而不是添加到它，并避免递归。 为什么编译器不这样做？ 是因为 C++ 销毁顺序的原因（例如，C++ 认为它需要保证在 then() 返回之后销毁对象，而不是之前？或者只是优化器的限制？<br>
无论如何，最好避免依赖尾调用优化。</p>
</blockquote>
<h2 id="max_concurrent_for_each">max_concurrent_for_each</h2>
<p><code>max_concurrent_for_each</code> 是 <code>parallel_for_each</code> 的变体，具有受限的并行性。 它接受一个额外的参数 - <code>max_concurrent</code> - 最多 <code>max_concurrent</code> 迭代同时排队，不保证它们完成操作的顺序。</p>
<pre><code class="language-cpp">seastar::future&lt;&gt; flush_all_files(seastar::lw_shared_ptr&lt;std::vector&lt;seastar::file&gt;&gt; files, size_t max_concurrent) {
    return seastar::max_concurrent_for_each(files, max_concurrent, [] (seastar::file f) {
        return f.flush();
    });
}

</code></pre>
<p>确定最大并发限制超出了本文档的范围。 它通常应该从运行软件的系统的实际能力中推导出来，例如并行执行单元或 I/O 通道的数量，以便优化资源利用率而不会使系统不堪重负。</p>
<h1 id="when_all-等待多个future">when_all: 等待多个future</h1>
<p>上面我们见到了<code>parallel_for_each()</code>，其会开启若干异步操作，并等待所有的都完成。Seastar有另一个函数<code>when_all()</code>，用于等待若干已经存在的<code>future</code>完成。</p>
<p><code>when_all()</code>的第一种变体是一个变长函数，也就是<code>future</code>可以作为分隔的参数传入，有多少个<code>future</code>是在编译期就确定下来的。每个<code>future</code>可以类型不同。例如：</p>
<pre><code class="language-cpp">#include &lt;seastar/core/sleep.hh&gt;

future&lt;&gt; f() {
    using namespace std::chrono_literals;
    future&lt;int&gt; slow_two = sleep(2s).then([] { return 2; });
    return when_all(sleep(1s), std::move(slow_two), 
                    make_ready_future&lt;double&gt;(3.5)
           ).discard_result();
}

</code></pre>
<p>这将启动三个 <code>futures</code> - 一个休眠一秒钟（并且不返回任何内容），一个休眠两秒钟并返回整数 2，一个立即返回双精度 3.5 - 然后等待它们。 <code>when_all()</code> 函数返回一个<code>futures</code>，一旦所有三个<code>futures</code>都计算出来了，即在两秒后解决。 这个<code>future</code>也有一个值，我们将在下面解释，但在这个例子中，我们只是等待算出<code>future</code>并丢弃它的值。</p>
<p>请注意，<code>when_all()</code>仅接受右值，它可以是临时值（如异步函数的返回值或 <code>make_ready_future</code>）或保存未来的 <code>std::move()</code>变量。</p>
<p><code>when_all()</code>返回的<code>future</code>解析为一个已经解析<code>future</code>的元组，并包含三个输入<code>future</code>的结果。 继续上面的例子，</p>
<pre><code class="language-cpp">future&lt;&gt; f() {
    using namespace std::chrono_literals;
    future&lt;int&gt; slow_two = sleep(2s).then([] { return 2; });
    return when_all(sleep(1s), std::move(slow_two),
                    make_ready_future&lt;double&gt;(3.5)
           ).then([] (auto tup) {
            std::cout &lt;&lt; std::get&lt;0&gt;(tup).available() &lt;&lt; &quot;\n&quot;;
            std::cout &lt;&lt; std::get&lt;1&gt;(tup).get0() &lt;&lt; &quot;\n&quot;;
            std::cout &lt;&lt; std::get&lt;2&gt;(tup).get0() &lt;&lt; &quot;\n&quot;;
    });
}

</code></pre>
<p>该程序的输出（两秒后出现）是 1, 2, 3.5：元组中的第一个<code>future</code>可用（但没有值），第二个具有整数值 2，第三个具有双精度值 3.5 - 正如预期的那样。</p>
<p>一个或多个等待的<code>future</code>可能会解决中出现异常，但这不会改变 <code>when_all()</code> 的工作方式：它仍然等待所有<code>future</code>解决，每个<code>future</code>都有一个值或异常，并且在返回的元组中 <code>future</code>可能包含异常而不是值。 例如，</p>
<pre><code class="language-cpp">future&lt;&gt; f() {
    using namespace std::chrono_literals;
    future&lt;&gt; slow_success = sleep(1s);
    future&lt;&gt; slow_exception = sleep(2s).then([] { throw 1; });
    return when_all(std::move(slow_success), std::move(slow_exception)
           ).then([] (auto tup) {
            std::cout &lt;&lt; std::get&lt;0&gt;(tup).available() &lt;&lt; &quot;\n&quot;;
            std::cout &lt;&lt; std::get&lt;1&gt;(tup).failed() &lt;&lt; &quot;\n&quot;;
            std::get&lt;1&gt;(tup).ignore_ready_future();
    });
}

</code></pre>
<p>两个<code>future</code>都可用（已解决），但第二个<code>future</code>已失败（导致异常而不是返回一个值）。 请注意我们如何在这个失败的<code>future</code>上调用 <code>ignore_ready_future()</code> ，因为默默地忽略一个失败的<code>future</code>被认为是一个错误，并将导致“异常<code>future</code>被忽略”错误消息。 更典型的是，应用程序将记录失败的<code>future</code>而不是忽略它。</p>
<p>上面的例子表明<code>when_all()</code>使用起来不方便且冗长。 结果被包装在一个元组中，导致冗长的元组语法，并使用准备好的<code>future</code>，必须单独检查所有异常以避免错误消息。</p>
<p>所以SeaStar还提供了一个更容易使用的<code>when_all_succeed()</code>函数。 这个函数当每个给定的<code>future</code>都解决了也是返回一个<code>future</code>。如果它们都成功，它将结果值传递给<code>continuation</code>，而不将它们包装在<code>future</code>或元组中。但是，如果一个或多个<code>future</code>失败，则 <code>when_all_succeed()</code> 将解析为失败的<code>future</code>，其中包含来自失败<code>future</code>之一的异常。如果给定的<code>future</code>有多个失败，其中一个将被传递（未指定选择哪个），其余的将被默默忽略。 例如，</p>
<pre><code class="language-cpp">using namespace seastar;
future&lt;&gt; f() {
    using namespace std::chrono_literals;
    return when_all_succeed(sleep(1s), make_ready_future&lt;int&gt;(2),
                    make_ready_future&lt;double&gt;(3.5)
            ).then([] (int i, double d) {
        std::cout &lt;&lt; i &lt;&lt; &quot; &quot; &lt;&lt; d &lt;&lt; &quot;\n&quot;;
    });
}
</code></pre>
<p>请注意<code>future</code>持有的整数和双精度值如何方便地单独（没有元组）传递到延续。 由于 sleep() 不包含值，因此会等待它，但不会将第三个值传递给<code>continuation</code>。 这也意味着如果我们<code>when_all_succeed()</code>在几个 <code>future&lt;&gt;</code>（没有值）上，结果也是一个 <code>future&lt;&gt;</code>：</p>
<pre><code class="language-cpp">using namespace seastar;
future&lt;&gt; f() {
    using namespace std::chrono_literals;
    return when_all_succeed(sleep(1s), sleep(2s), sleep(3s));
}
</code></pre>
<p>此示例仅等待 3 秒（最多 1、2 和 3 秒）。</p>
<p><code>when_all_succeed()</code> 的示例，但有异常：</p>
<pre><code class="language-cpp">using namespace seastar;
future&lt;&gt; f() {
    using namespace std::chrono_literals;
    return when_all_succeed(make_ready_future&lt;int&gt;(2),
                    make_exception_future&lt;double&gt;(&quot;oops&quot;)
            ).then([] (int i, double d) {
        std::cout &lt;&lt; i &lt;&lt; &quot; &quot; &lt;&lt; d &lt;&lt; &quot;\n&quot;;
    }).handle_exception([] (std::exception_ptr e) {
        std::cout &lt;&lt; &quot;exception: &quot; &lt;&lt; e &lt;&lt; &quot;\n&quot;;
    });
}

</code></pre>
<p>在这个例子中，其中一个futures失败，所以<code>when_all_succeed</code>的结果是一个失败的future，所以正常的<code>continuation</code>没有运行，<code>handle_exception()</code> continuation就完成了。</p>
<p>TODO：还要解释用于 vectors 的 when_all 和when_all_succeed。</p>
<h1 id="semaphores信号量">Semaphores（信号量）</h1>
<p>Seastar 的信号量是标准的计算机科学信号量，适用于未来。 信号量是一个计数器，您可以将单位存入或取走。 如果没有足够的单位可用，从计数器取走单位可能需要等待。</p>
<h2 id="用信号量限制并行性">用信号量限制并行性</h2>
<p>Seastar 中信号量的最常见用途是限制并行性，即限制可以并行运行的某些代码的实例数量。 当每个并行调用使用有限的资源（例如，内存）时，这可能很重要，因此让无限数量的并行调用可以耗尽此资源。</p>
<p>考虑外部事件源（例如，传入的网络请求）导致异步函数 g() 被调用的情况。 想象一下，我们要将并发 g() 操作的数量限制为 100。即，如果 g() 在其他 100 个调用仍在进行时启动，我们希望它延迟其实际工作，直到其他调用之一完成。 我们可以用信号量做到这一点：</p>
<pre><code class="language-cpp">seastar::future&lt;&gt; g() {
    static thread_local seastar::semaphore limit(100);
    return limit.wait(1).then([] {
        return slow(); // do the real work of g()
    }).finally([] {
        limit.signal(1);
    });
}

</code></pre>
<p>在这个例子中，信号量以计数器为 100 开始。 异步操作 slow() 仅在我们可以将计数器减一（wait(1)）时启动，并且当 slow() 完成时，无论成功还是异常 ，计数器加一（信号（1））。 这样，当 100 个操作已经开始工作并且尚未完成时，第 101 个操作将等待，直到其中一个正在进行的操作完成并将一个单元返回给信号量。 这确保了每次我们在上面的代码中最多运行 100 个并发的 slow() 操作。</p>
<p>请注意我们如何使用静态 thread_local 信号量，以便从同一分片对 g() 的所有调用都计入相同的限制。像往常一样，一个 Seastar 应用程序是分片的，所以这个限制是单独的每个分片（CPU 线程）。 这通常很好，因为分片应用程序认为每个分片的资源是分开的。</p>
<p>幸运的是，上面的代码恰好是异常安全的：limit.wait(1) 可以在内存不足时抛出异常（保留一个等待者列表），在这种情况下，信号量计数器不会减少，但后续的<code>continuation</code>是 不运行所以它也不会增加。limit.wait(1) 也可以在信号量被破坏时返回一个特殊的未来（我们将在后面讨论）但在这种情况下额外的 signal() 调用被忽略。最后，slow() 也可能抛出或返回一个特殊的未来，但 finally() 确保信号量仍然增加。</p>
<p>但是，随着应用程序代码变得越来越复杂，无论发生哪个代码路径或异常，都很难确保我们在操作完成后永远不会忘记调用 signal()。 作为可能出错的示例，请考虑以下错误代码片段，它与上面的代码片段略有不同，而且乍一看似乎是正确的：</p>
<pre><code class="language-cpp">seastar::future&lt;&gt; g() {
    static thread_local seastar::semaphore limit(100);
    return limit.wait(1).then([] {
        return slow().finally([] { limit.signal(1); });
    });
}

</code></pre>
<p>但是这个版本不是异常安全的：考虑如果slow() 在返回一个<code>future</code> 之前抛出一个异常会发生什么（这与slow() 返回一个异常的<code>future</code>不同，我们在有关异常处理的部分讨论了这种差异）。在这种情况下，我们减少了计数器，但 finally() 永远不会到达，并且计数器永远不会增加。有一种方法可以修复此代码，即用 seastar::futurize_invoke(slow) 替换对 slow() 的调用。 但是我们在这里试图说明的不是如何修复有问题的代码，而是通过使用单独的 semaphore::wait() 和 semaphore::signal() 函数，您很容易出错。</p>
<p>为了异常安全，在C++中一般不推荐有单独的资源获取和释放函数。 相反，C++ 提供了更安全的机制来获取资源（在这种情况下是信号量单元）并稍后释放它：lambda 函数和 RAII（“资源获取即初始化”）：</p>
<p>基于 lambda 的解决方案是一个函数 seastar::with_semaphore() ，它是上面示例中代码的快捷方式：</p>
<pre><code class="language-cpp">seastar::future&lt;&gt; g() {
    static thread_local seastar::semaphore limit(100);
    return seastar::with_semaphore(limit, 1, [] {
        return slow(); // do the real work of g()
    });
}

</code></pre>
<p>with_semaphore() 与前面的代码片段一样，等待来自信号量的给定数量的单位，然后运行给定的 lambda，当 lambda 返回的<code>future</code>被解析时，with_semaphore() 将单位返回给信号量。 with_semaphore() 返回一个只有在所有这些步骤完成后才能解析的<code>future</code>。</p>
<p>函数 seastar::get_units() 更通用。 它基于 C++ 的 RAII 哲学，为 seastar::semaphore 的单独 wait() 和 signal() 方法提供了一个异常安全的替代方案。该函数返回一个不透明的单位对象，该对象在保持时保持信号量的计数器减少 - 一旦该对象被析构，计数器就会增加回来。 有了这个接口，你不能忘记增加计数器，或增加两次，或增加而不减少。</p>
<p>在创建单位对象时，计数器将始终减少一次，如果成功，则在对象被销毁时增加。 当units对象被移动到continuation中时，不管continuation如何结束，当continuation被破坏时，units对象被破坏并且units被返回给信号量的计数器。 上面使用 get_units() 编写的示例如下所示：</p>
<pre><code class="language-cpp">seastar::future&lt;&gt; g() {
    static thread_local semaphore limit(100);
    return seastar::get_units(limit, 1).then([] (auto units) {
        return slow().finally([units = std::move(units)] {});
    });
}

</code></pre>
<p>请注意需要使用 get_units() 的有点复杂的方式：必须嵌套continuation，因为我们需要将单位对象移动到最后一个continuation。 如果slow() 返回一个future（并且不会立即抛出），finally() 继续捕获units 对象，直到一切都完成，但不运行任何代码。</p>
<p>Seastars 程序员通常应该避免直接使用 semaphore::wait() 和 semaphore::signal() 函数，并且总是优先 with_semaphore()（如果适用）或 get_units()。</p>
<h2 id="限制资源利用量">限制资源利用量</h2>
<p>因为信号量支持等待任意数量的单元，而不仅仅是 1，所以我们可以将它们用于不仅仅是限制并行调用的数量。 例如，假设我们有一个异步函数 using_lots_of_memory(size_t bytes)，它使用 bytes 字节的内存，并且我们希望确保该函数的所有并行调用使用的内存（合起来）不超过 1 MB --- 以及额外的 调用会延迟到之前的调用完成。 我们可以用信号量做到这一点：</p>
<pre><code class="language-cpp">seastar::future&lt;&gt; using_lots_of_memory(size_t bytes) {
    static thread_local seastar::semaphore limit(1000000); // limit to 1MB
    return seastar::with_semaphore(limit, bytes, [bytes] {
        // do something allocating 'bytes' bytes of memory
    });
}

</code></pre>
<p>注意在上面的例子中，调用 using_lots_of_memory(2000000) 将返回一个永远不会解析的future，因为信号量永远不会包含足够的单元来满足信号量等待。 using_lots_of_memory() 可能应该检查字节是否超过限制，并在这种情况下抛出异常。 seastar不会为你做这件事。</p>
<h2 id="限制循环的并行数">限制循环的并行数</h2>
<p>上面，我们看到了一个被一些外部事件调用的函数 g()，并希望控制它的并行性。 在本节中，我们看看循环的并行性，它也可以用信号量来控制。</p>
<p>考虑以下简单循环：</p>
<pre><code class="language-cpp">#include &lt;seastar/core/sleep.hh&gt;
seastar::future&lt;&gt; slow() {
    std::cerr &lt;&lt; &quot;.&quot;;
    return seastar::sleep(std::chrono::seconds(1));
}
seastar::future&lt;&gt; f() {
    return seastar::repeat([] {
        return slow().then([] { return seastar::stop_iteration::no; });
    });
}

</code></pre>
<p>这个循环在没有任何并行性的情况下运行slow() 函数（需要一秒钟才能完成）——下一个slow() 调用仅在前一个调用完成时开始。 但是，如果我们不需要串行化对 slow() 的调用，并且希望允许它的多个实例同时进行呢？</p>
<p>朴素的做法，我们可以通过在上一次调用之后立即开始下一次对slow() 的调用来实现更多的并行性——忽略上一次对slow() 的调用返回的future，而不是等待它解决：</p>
<pre><code class="language-cpp">seastar::future&lt;&gt; f() {
    return seastar::repeat([] {
        slow();
        return seastar::stop_iteration::no;
    });
}

</code></pre>
<p>但在这个循环中，并行度没有限制——在第一个返回之前，数百万个 sleep() 调用可能并行活动。 最终，这个循环可能会消耗所有可用内存并崩溃。</p>
<p>使用信号量允许我们并行运行多个 slow() 实例，但将这些并行实例的数量限制为（在以下示例中）为 100：</p>
<pre><code class="language-cpp">seastar::future&lt;&gt; f() {
    return seastar::do_with(seastar::semaphore(100), [] (auto&amp; limit) {
        return seastar::repeat([&amp;limit] {
            return limit.wait(1).then([&amp;limit] {
                seastar::futurize_invoke(slow).finally([&amp;limit] {
                    limit.signal(1); 
                });
                return seastar::stop_iteration::no;
            });
        });
    });
}

</code></pre>
<p>请注意此代码与我们在上面看到的用于限制函数 g() 并行调用次数的代码有何不同：</p>
<ol>
<li>这里我们不能使用单个 thread_local 信号量。 对 f() 的每次调用都有其并行度为 100 的循环，因此需要自己的信号量“限制”，在循环期间使用 do_with() 保持活动状态</li>
<li>在这里，我们在继续循环之前不等待 slow() 完成，即我们不返回从 futurize_invoke(slow) 开始的future链。 当信号量单元可用时，循环继续进行下一次迭代，而（在我们的示例中）99 个其他操作可能在后台进行，我们不会等待它们。</li>
</ol>
<p>在本节的示例中，我们不能使用 with_semaphore() 快捷方式。 with_semaphore() 返回一个future ，它只在 lambda 返回的future 解析后解析。 但是在上面的例子中，循环需要知道什么时候只有信号量单元可用，才能开始下一次迭代——而不是等待前一次迭代完成。 我们无法通过 with_semaphore() 实现这一点。 但是在这种情况下可以使用更通用的异常安全习惯用法 seastar::get_units()，并且推荐使用：</p>
<pre><code class="language-cpp">seastar::future&lt;&gt; f() {
    return seastar::do_with(seastar::semaphore(100), [] (auto&amp; limit) {
        return seastar::repeat([&amp;limit] {
    	    return seastar::get_units(limit, 1).then([] (auto units) {
	            slow().finally([units = std::move(units)] {});
	            return seastar::stop_iteration::no;
	        });
        });
    });
}

</code></pre>
]]></content>
    </entry>
</feed>