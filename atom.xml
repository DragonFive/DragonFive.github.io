<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://DragonFive.github.io/</id>
    <title>dragon</title>
    <updated>2021-07-02T09:00:38.273Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://DragonFive.github.io/"/>
    <link rel="self" href="https://DragonFive.github.io/atom.xml"/>
    <subtitle>Code is cheap, show me the theory</subtitle>
    <logo>https://DragonFive.github.io/images/avatar.png</logo>
    <icon>https://DragonFive.github.io/favicon.ico</icon>
    <rights>All rights reserved 2021, dragon</rights>
    <entry>
        <title type="html"><![CDATA[华为的《ScaleFreeCTR:a MixCache-based distributed training system for CTR》]]></title>
        <id>https://DragonFive.github.io/post/hua-wei-de-scalefreectr/</id>
        <link href="https://DragonFive.github.io/post/hua-wei-de-scalefreectr/">
        </link>
        <updated>2021-06-15T12:23:03.000Z</updated>
        <content type="html"><![CDATA[<p>华为诺亚方舟提出了SFCTR <a href="https://arxiv.org/pdf/2104.08542.pdf">ScaleFreeCTR: a MixCache-based distributed training system for CTR</a>，scalefree 可能是因为数据规模对训练吞吐没有影响，后面实验部分有具体的数据。</p>
<h1 id="一-动机与主要创新">一、动机与主要创新</h1>
<p>现有的分布式CTR训练框架使用CPU内存来保存和更新参数，使用gpu 进行前向和反向计算（也有用CPU的），会有两个瓶颈</p>
<ol>
<li>
<p>CPU 和 gpu 之间的pull 和 push 操作有一定的延迟</p>
</li>
<li>
<p>cpu 进行参数的同步和更新比较慢</p>
</li>
</ol>
<p>分布式训练的关键在于</p>
<ol>
<li>关键在于减少host_gpu之间的延迟</li>
<li>减少host-gpu 及gpu之间的数据传输量也很重要</li>
</ol>
<p>推荐中的参数有两个特点：</p>
<ol>
<li>实际 working parameters 比较少，sparse 参数和 MLP参数都很少</li>
<li>sparse 特征符合幂律分布，小部分特征被高频访问</li>
</ol>
<p>根据两个特点，可以有两个方法</p>
<ol>
<li>使用缓存机制减少 host-gpu 延迟</li>
<li>通过重组batch数据来减少参数传输量(unique?)</li>
</ol>
<p>由此提出了 SFCTR:<br>
在CPU中通过 虚拟sparse id op 来减少host-gpu 和gpu-gpu 的数据传输量，使用 mixcache 实验特征预取来减少传输延迟，使用3级pipeline 来减少整体训练时长。</p>
<p>系统将会在MindSpore 上开源，现在看似乎还没有开源。</p>
<h1 id="二-相关工作">二、相关工作</h1>
<h2 id="一论文介绍的跟sfctr无关但挺有用的经验知识">（一）论文介绍的跟SFCTR无关，但挺有用的经验知识</h2>
<p>为了提高训练效率，有两种通用的做法：</p>
<ol>
<li>增量学习（batch训练的补充，用最近的数据更新模型）</li>
<li>分布式训练（使用额外的训练资源）</li>
</ol>
<p>CTR模型稀疏部分参数量太大，所以不能使用reduce 数据并行，大多数考虑用了模型并行。<br>
模型并行解决方案ps 架构的局限性：<br>
Ps server 保存并同步参数，worker执行前向和反向计算，</p>
<ul>
<li>worker pull and push from ps</li>
<li>Ps 从worker接收梯度之后进行同步<br>
分布式训练包括两个阶段：计算和参数同步。</li>
</ul>
<p>百度的综述 <a href="https://arxiv.org/abs/2003.05622">Distributed Hierarchical GPU Parameter Server for Massive Scale Deep Learning Ads Systems</a> 介绍了三种同步模式：</p>
<ul>
<li>BSP(bulk sync parallel)，严格对所有worker的更新进行同步</li>
<li>SSP(stale sync parallel)，对快worker 进行同步</li>
<li>ASP（async parallel）, 不同步gradient</li>
</ul>
<p>后两种方式虽然提升了训练效率，但是降低了模型性能。SFCTR 使用的是BSP，XDL使用的是ASP。</p>
<h1 id="三-sfctr-架构">三、SFCTR 架构</h1>
<p>SFCTR 由三部分构成</p>
<ul>
<li>Data-Loader, 提出虚拟sparse id op 来减少batch中重复的特征emb(unique?)</li>
<li>Host-Manager, 使用混合缓存策略来减少host-gpu延迟，MixCache 的管理器部分在 CPU 的内存中，MixCache 的缓冲区在 GPU 的 HBM 中</li>
<li>GPU-WORKER<br>
<img src="https://DragonFive.github.io//post-images/1625142600332.png" alt="" loading="lazy"></li>
</ul>
<p><strong>3级pipeline</strong><br>
把 Data-Loader,Host-Manager 和gpu worker 中，三阶段资源不同 Disk, CPU and GPU在三个不同的线程里完成</p>
<figure data-type="image" tabindex="1"><img src="https://DragonFive.github.io//post-images/1625142730082.png" alt="" loading="lazy"></figure>
<h2 id="1data-loader">（1）data loader</h2>
<p>sparse id op 来减少batch中重复的特征emb，就是xdl中的unique。减少host-gpu 和gpu-gpu 的数据传输量。</p>
<h2 id="2host-manager">（2）HOST-MANAGER</h2>
<p>MixCache用来减少延迟，在每个GPU的HBM 上申请一个cache buffer，使用modulo 哈希方法对 working parameters 进行分组，放在不同的GPU 上，embedding参数在data loader执行完VSI op 之后检查哪些参数 gpu 上没有就把那些参数传输到gpu 上。</p>
<p>当cache满了之后，满足两种情况的emb 会回传到host。</p>
<ol>
<li>参数完成了更新</li>
<li>下个batch不需要这个参数</li>
</ol>
<h2 id="3gpu-worker">（3）GPU-WORKER</h2>
<p>不同的GPU保存了不同的参数，所以前向和反向的时候都需要同步。<br>
前向传播，每个worekr从其它worker拿到batch所需要的参数，使用all-reduce 通信方式，一个gpu只需要跟另外两个gpu通信两次，首先通过gather_cache ，从cache buffer 中获得local common emb, 因为global_id 顺序一致，所以可以做all_reduce同步, 然后通过all_reduce 或者 global common emb，最后通过vis 算出来自己worker需要执行的batch emb<br>
<img src="https://DragonFive.github.io//post-images/1625142803471.png" alt="" loading="lazy"></p>
<p>梯度更新<br>
<img src="https://DragonFive.github.io//post-images/1625142812224.png" alt="" loading="lazy"></p>
<h1 id="四-sfctr-执行流程">四、SFCTR 执行流程</h1>
<p>执行流程<br>
<img src="https://DragonFive.github.io//post-images/1625142836322.png" alt="" loading="lazy"></p>
<ul>
<li>
<p>2-3行是 data loader 部分，有个虚拟Sparse Id OP，对batch Sparse ID 去重后形成 global_id，对于batch 中每个实例有个virtual_id，可以找到其对应的global_id ，跟XDL 的unique 操作很像。使用global_id, 各个gpu在同步的时候数据量就会少很多。</p>
</li>
<li>
<p>4-7行是 Host-Manager 部分，负责在主存中报错embedding参数（存得下吗？），使用mixcache把working parameters 放到 gpu 的cache buffer中。mixcache 还更新gpu cache buffer， 检查下一个batch需要哪些embedding ，预测哪些embedding未来一段时间不需要，在buffer满的时候进行pull, 发送数据到gpu，并对每个特征在gpu设置一个local_id (?)</p>
</li>
<li>
<p>9-15行是GPU部分，包括embedding查表，前向反向和参数更新</p>
</li>
</ul>
<p>host 和 gpu 是生产者与消费者模式</p>
<h1 id="五-一些对我们有用的实验">五、一些对我们有用的实验：</h1>
<h2 id="1环境">（1）环境</h2>
<p>GPU 集群使用 InfiniBand 连接，4台GPU服务器通过100Gb  RDMA提速</p>
<p>Intel Xeon Gold-5118 CPUs with 18 cores (36 threads), 8 Tesla V100 GPUs with 32 GB HBM，1GB内存。GPU之间PCI连接<br>
使用 Criteo-TB 数据库，使用filter构造10GB和100GB两个数据集，因为包括了优化器的信息，33×4B×80=10GB，所以实际parameter table是embedding table的3倍，所以实际上是 30GB和300GB的模型参数。</p>
<p>使用 DeepFM 模型，与 hugectr与ps mxnet对比</p>
<h2 id="2框架对比实验">（2）框架对比实验</h2>
<p>基于VSI OP，混合缓存机制，和三级pipeline，在10GB数据上SFCTR 在4机32卡上的吞吐量是psmxnet的1.6倍，hugectr的5.6倍，100GB 数据上是1.3倍和6.9倍<br>
如果GPU卡只有8个，hugectr在100GB数据上根本无法训练</p>
<figure data-type="image" tabindex="2"><img src="https://DragonFive.github.io//post-images/1625143001706.png" alt="" loading="lazy"></figure>
<h2 id="3vsi-op">（3）vsi op</h2>
<p>Host-gpu 数据传输量减少 94% ，g pu-gpu数据量减少88%</p>
<figure data-type="image" tabindex="3"><img src="https://DragonFive.github.io//post-images/1625143009606.png" alt="" loading="lazy"></figure>
<h2 id="4mixcache">（4）mixcache</h2>
<p>Cache 大小对传数据的影响<br>
2GB的cache可以把数据传输推迟到1000步之后<br>
如果cache 大小比较大，batch中要传输的数据的比例就会小，因为可以存更多高频特征<br>
12%（2GB）, 27%（0.5GB） and 29%（0.25GB）</p>
<figure data-type="image" tabindex="4"><img src="https://DragonFive.github.io//post-images/1625143020154.png" alt="" loading="lazy"></figure>
<h2 id="53级pipeline">（5）3级pipeline</h2>
<p>GPU-Worker 训练时间在pipeline中占比最高<br>
一个节点跑100GB数据，使用pipeline需要 75 s，不用pipeline就需要150s</p>
<h1 id="六-总结与思考">六、总结与思考</h1>
<p>文章写的通俗易懂，很有条理，related worker 也总结了很多训练的经验，工作很有实用性。<br>
作者提到未来的工作有两个方向</p>
<ol>
<li>提升通信效率，（使用all2all）</li>
<li>调查提升收敛速度的方法</li>
</ol>
<p>思考借鉴意义</p>
<ol>
<li>
<p>SFCTR相当于把图完全放在GPU中执行，没有进行图的分隔，所以实现起来更容易一些。CPU只是一个ps的存储和更新后落盘以及dataloader</p>
</li>
<li>
<p>CPU内存1T，而实验中的数据最大的为300GB，所以可以放在CPU内存中，其实我们的模型大小似乎也在几百GB，如果可以放在worker内存中，就没有必要单独弄一个ps server；<br>
如果模型超过1T，也可以融合AIBox的做法，使用SSD做cpu mem的缓存</p>
</li>
<li>
<p>pipeline 和 vsiop 其实 XDL 都有，只缺了缓存机制，但XDL如果不动ps这一块，参数的更新其实是在ps 上完成的，所以 ps 的 push 也会继续有延迟，参数预取只能解决pull 的问题，</p>
</li>
<li>
<p>但如果都在本地更新，那不同worker之间参数同步就会比较麻烦，所以缓存预取、更新后缓存失效再回传的机制必然依赖多机间 RDMA  单机allreduce 同步通信技术，ps存在的意义不大</p>
</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[业界CTR深度学习框架的一些新的进展 ]]></title>
        <id>https://DragonFive.github.io/post/ye-jie-ctr-shen-du-xue-xi-kuang-jia-de-yi-xie-xin-de-jin-zhan/</id>
        <link href="https://DragonFive.github.io/post/ye-jie-ctr-shen-du-xue-xi-kuang-jia-de-yi-xie-xin-de-jin-zhan/">
        </link>
        <updated>2021-05-31T13:03:49.000Z</updated>
        <content type="html"><![CDATA[<p>为了充分利用GPU的能力和高速带宽<br>
英伟达的 hugeCtr https://github.com/NVIDIA/HugeCTR 和 脸书 的 DLRM<br>
<a href="https://arxiv.org/abs/1906.00091">【CoRR2019】Deep Learning Recommendation Model for Personalization and Recommendation Systems</a><br>
把emb参数分成不同的份放在GPU HMB中，需要需要昂贵的GPU，不实用。</p>
<p>腾讯的DES<br>
<a href="https://arxiv.org/abs/1909.04823">Distributed Equivalent Substitution Training for Large-Scale Recommender Systems</a><br>
和 百度的 HierPs<br>
<a href="https://arxiv.org/abs/2003.05622">Distributed Hierarchical GPU Parameter Server for Massive Scale Deep Learning Ads System</a><br>
使用主存保存emb ，DES采用 field-aware 分片策略来reduce(减少or规约)GPU间的数据通信，但没有进行主存和GPU之间通信优化。<br>
HierPS使用大batch策略来在gpu中缓存使用大参数，以此减少传输延时。</p>
<p>Tensorflow, MxNet 和 PyTorch 并不能很好的支持大规模embedding的训练:</p>
<ul>
<li>PyTorch 中没有官方支持的ps</li>
<li>Mxnet 支持的模型大小因为实现问题受到了限制</li>
<li>tensorflow 使用它的ps后吞吐会严重下降</li>
</ul>
<p>为了提升tensorflow, money, pytorch较差的分布式性能，百度的horovod<br>
<a href="https://arxiv.org/abs/1802.05799">Horovod: fast and easy distributed deeplearninginTensorFlow</a><br>
和字节的byteps<br>
<a href="https://arxiv.org/abs/1807.00311">Product-based Neural Networks for User Response Prediction over Multi-field Categorical Data</a><br>
都支持不同的平台:</p>
<ul>
<li>horovod 使用百度的Ring-AllReduce实现来加速dense模型的训练</li>
<li>byteps 通过调度优先来在不同的层加速同步参数，优化不同层的顺序来在反向传播和前向计算的时候同步参数</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[百度的《AIBox: CTR Prediction Model Training on a Single Node》]]></title>
        <id>https://DragonFive.github.io/post/bai-du-de-lesslessaibox-ctr-prediction-model-training-on-a-single-nodegreatergreater/</id>
        <link href="https://DragonFive.github.io/post/bai-du-de-lesslessaibox-ctr-prediction-model-training-on-a-single-nodegreatergreater/">
        </link>
        <updated>2021-03-11T12:43:07.000Z</updated>
        <content type="html"><![CDATA[<p>AIBox 是百度提出的训练框架，论文 AIBox: CTR Prediction Model Training on a Single Node 进行了相关介绍。</p>
<h1 id="一-aibox-的技术创新及优势">一、AIBox 的技术创新及优势</h1>
<p>创新点与动机：<br>
AIBox 的核心想法就是想在一台机器上用GPU加速训练，但是参数实在太大了，所以就把计算密集的模型运算部分（joint learning）放在GPU完成，把取embedding 部分放在cpu部分完成(embedding learning)，这就是AiBox 的第一个创新：把网络切分为两部分。</p>
<p>但即便主存用了1TB 的内存，embedding 还是太大了，10^12 个key，每个key 的 weight 即使用8个字节存，key用8个字节存，也需要1.6TB , 所以论文提出了第二个创新：把embedding 存在SSD上，同时为了降低延迟和减少写操作对ssd寿命的影响，建立了二级缓存机制。</p>
<p>为了提高速度，AIBOX使用了流水线，把从hdfs 读数据(socket IO)，从SSD查Embedding （SSD io） 和 cpu+gpu 计算组成3阶段的 pipeline</p>
<p>优势：<br>
AIBOX不存在像分布式系统普遍存在的网络通信开销问题，然后在系统稳定性方面AIBOX与具有数千台机器的分布式集群相比更加稳定不会轻易宕机，而且在同步开销方面AIBOX只是涉及到一些内存锁和GPU片之间的少量通信。</p>
<h1 id="二-关于网络结构切分">二、关于网络结构切分</h1>
<p>the first module focuses on the embedding learning with high-dimensional &amp; sparse features and the second module is for joint learning with dense features resulted from the first module.</p>
<p>The embedding learning is processed on CPUs to help learn low dimensional dense embedding representations.</p>
<p>By transferring the learned embedding vectors from CPUs to GPUs, the computation-intensive joint learning module can make full use of the powerful GPUs for CTR prediction.</p>
<p>CPU 部分把数据从稀疏特征转化成 embedding （embedding learning），然后把embedding 传到 GPU，GPU进行一轮训练 (joint learning)</p>
<p>论文这部分讲了一些网络的设计细节，但这块感觉跟 AIBox本身没什么关系，论文写到：</p>
<p>把第一隐含层和最后一层隐含层的结果合并起来，第一层包含了low-level 的与输入信息最相关的feature，最后一层包含了high-level 的最抽象和有用的信息。这样会得到更准确的CTR预估结果。</p>
<p>训练两阶段(cpu+gpu)，梯度更新也是两阶段(gpu+cpu)。</p>
<h1 id="三-aibox-架构划分">三、AIBox 架构划分</h1>
<p><strong>架构：</strong><br>
分为三部分：CPU、GPU和 sparse table buff</p>
<ul>
<li>
<p>cpu模块：协调调度和embedding学习<br>
从hdfs读数据(一个pass)，向Sparse Table模块查embedding，然后发给GPU<br>
拿到gpu传来的梯度，更新sparse table<br>
定期save ckpt 到 hdfs</p>
</li>
<li>
<p>sparse table：把10^12 的离散特征的数据存储到ssd上的kv系统里<br>
内存中的key hash 索引存了特征到文件的映射关系，<br>
in-memory cache strategy 构造cache 和 buffer 来减少延迟</p>
</li>
<li>
<p>gpu模块：联合学习<br>
cpu传来的 embedding 被放入 HBMs 中，然后被fed 给 dense 联合学习网络<br>
emb通过pci-e总线进行传输<br>
一个CUDA stream进行数据传输，另一个cuda stream 进行学习计算<br>
HBMs如同片上ps一样工作，<br>
每个pass 在每个gpu 上计算新的参数，各gpu通过NVLink进行同步</p>
</li>
</ul>
<p><strong>3阶段流水线：network, SSDs and CPUs + GPUs</strong></p>
<figure data-type="image" tabindex="1"><img src="https://DragonFive.github.io//post-images/1625143570973.png" alt="" loading="lazy"></figure>
<figure data-type="image" tabindex="2"><img src="https://DragonFive.github.io//post-images/1625143575338.png" alt="" loading="lazy"></figure>
<h1 id="四-sparsetable-架构">四、sparseTable 架构</h1>
<p>由两部分构成：key hash index and bi-level cache management</p>
<h2 id="一key-hash-index">（一）key hash index</h2>
<p>Key Hash Index 存的是 10^12 个 feature key 到 ssd 文件的映射关系，直接每个key 存一个文件需要1.6TB大小的内存，放不下。</p>
<p>通过对key 取模进行分组建立 group 与 file 的对应关系，放在内存中。</p>
<p>group(key) → key mod 1012/m. We set m = ⌊BLOCK/(8 + sizeof(value))⌋,其中Block 是每次从ssd取数据的最小单元。</p>
<p>hash函数可以通过预训练一个模型来最大化feature 共现，把共现的feature 分到同样的桶里面。</p>
<h2 id="二二级缓存机制">（二）二级缓存机制</h2>
<p>ssd 的延迟是内存的1000倍，ssd是微秒级别延迟，内存是纳秒级别延迟</p>
<p>在一个 pass of mini batch 中只有1%的参数会被用到，所以我们可以用in-memory cache 来存储高频访问的hot parameters</p>
<p>SSD有物理性能限制：每个存储单元只能被写入（擦除）数千次，cache机制可以作为参数缓存，来减少更新参数对SSD使用寿命的影响</p>
<p>使用两个分离的链表进行拉链来提升探测性能。对每个ssd文件使用Bloom filter来减少不必要的读取。</p>
<p><strong>第一级缓存</strong></p>
<p>使用 si =hash1(g_id) 来算出一个 cache slot 槽，对应一个ssd 文件，对于参数并未进行真正初始化，而是在第一次访问到参数的时候，先用 bloom filter 探测key 是否在 slot 集合里，如果不在就不用读取这个文件，而是直接使用默认值，以此来减少不必要的ssd读取。</p>
<p><strong>二级缓存</strong></p>
<p>hash2(g_id, bucket)</p>
<p>对 一级的槽进行分桶bucket，来使得拉的链比较短。bucket 参数通过调节可以权衡空间和探测效率</p>
<p><strong>两条拉链</strong></p>
<ul>
<li>LRU 链用于保存最近访问过的key，以此来减少探测次数</li>
<li>LFU链按访问频次来保存key，用于缓存管理，只有当LFU满了需要删除低频key时，相应的数据才会写回到ssd上面</li>
</ul>
<p>由于经常有链条中的节点进行增删，所以使用线程池以Slab memory allocation mechanism机制 进行管理。</p>
<figure data-type="image" tabindex="3"><img src="https://DragonFive.github.io//post-images/1625143705399.png" alt="" loading="lazy"></figure>
<p><strong>文件管理系统</strong><br>
batch产生的小文件对于先有的文件系统有很大的压力，把许多小文件组成一个组来创建较少的文件。小文件的名字由大文件的名字加上offset构成，保存在第一级cache slot 中</p>
<p>监控文件系统的大小，合并访问量少的小问题，当model_size 达到最大冗余度的时候删掉访问少的文件，MAX_REPLICATION=SSD capacity ∗ (85% + overprovisioning)/model size<br>
<img src="https://DragonFive.github.io//post-images/1625143710125.png" alt="" loading="lazy"></p>
<h1 id="五-实验部分">五、实验部分</h1>
<p><strong>实验</strong><br>
AIBox 8 个GPU， 服务器级别的cpu, 1T 内存，Raid 0 nvme ssd<br>
MPI集群方式用75个计算节点</p>
<ul>
<li>
<p>AIBox 的硬件和维护费用比集群训练方式少 10%，执行时间多25%</p>
</li>
<li>
<p>AIBox 的auc 比集群方式稍好，可能是因为AIBox 这种单节点的方式，同步参数频率更高</p>
</li>
</ul>
<figure data-type="image" tabindex="4"><img src="https://DragonFive.github.io//post-images/1625143749656.png" alt="" loading="lazy"></figure>
<p>六、总结与一些细节问题：<br>
论文介绍了 AIBox 架构的一些细节方面，借助一系列系统设计方案如缓存机制来解决问题，通过这些并不是很fashion的技术合并，论文实现了集中式训练的技术突破，这种通过技术积累然后撬动难题的解决问题的方式值得我们学习。</p>
<p>但是还有一些细节没有讲清楚：</p>
<ol>
<li>
<p>AIBox 有几个worker 进行工作，他们是数据并行，还是使用同样的数据进行训练（文中提到AIBox 会在每个pass of mini batch 进行同步，所以应该不是一个worker 在参与训练）</p>
</li>
<li>
<p>AIBox 使用集中的训练方式，那如果这台机器挂掉，是不是根本没有办法进行恢复，只能另找一个机器从 ckpt 训练</p>
</li>
<li>
<p>文章没有介绍使用的具体计算引擎 (怀疑跟 horovod 接近）</p>
</li>
<li>
<p>同样文章没有介绍参数同步的细节，没有相关 all_reduce 的介绍（可以是使用了一个开源的框架，而这部分论文没有进行改进，所以没有做深入介绍）</p>
</li>
<li>
<p>文章开头提到使用 in-HBM ps 来减少数据传输，但是后面没有详细进行介绍</p>
</li>
</ol>
<p>总体上感觉这篇论文实用性强，但是细节介绍得不多</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[tensorflow2.x 分布式训练]]></title>
        <id>https://DragonFive.github.io/post/tensorflow2x-fen-bu-shi-xun-lian/</id>
        <link href="https://DragonFive.github.io/post/tensorflow2x-fen-bu-shi-xun-lian/">
        </link>
        <updated>2020-06-24T07:41:26.000Z</updated>
        <content type="html"><![CDATA[<blockquote>
<p>去年五月份总了一些 tensorflow 1.x 分布式训练的一些知识（<a href="https://dragonfive.github.io/post/tensorflow-1x-de-fen-bu-shi-xun-lian/">tensorflow 1.x 的分布式训练</a>）。最近总结了一些 tf2.x 的分布式训练相关知识。</p>
</blockquote>
<h1 id="tf2x-分布式训练策略">tf2.x 分布式训练策略</h1>
<p>TensorFlow 的 tf.distribute 模块下包含有一系列分布式训练策略，它们都是基于数据并行模式实现的。有些策略目前还在 experimental 模块下，表示它们是实验性质的策略，未来可能会发生变动。</p>
<p>训练分布式模型，只需要将原先的模型代码置于distribution Strategy的Scope()下，即可。</p>
<pre><code class="language-py">import tensorflow as tf
data_train, _ = tf.keras.datasets.mnist.load_data()
dataset = tf.data.Dataset.from_tensor_slices(data_train) # 该处可接收numpy数组
dataset = dataset.shuffle(buffer_size=60000) # 该处要大于data_train的长度
dataset = dataset.batch(32)
 
mirrored_strategy = tf.distribution.MirroredStrategy()
 
# mirrored_strategy策略: 在每一个gpu上训练一个模型，每次更新时需要汇总所有gpu上的梯度。
with mirrored_strategy.scope():
  model = tf.keras.Sequential([...])
# tf 2.0中，所有的optimizer都在tf.keras.optimizer下
model.compile(optimizer=tf.keras.optimizer.adam(lr=...))， 
              loss = &quot;sparse_categorical_crossentropy&quot;,
              metrics = ['accuracy'])
model.fit(dataset, epoch=5)
</code></pre>
<h1 id="单机多卡训练">单机多卡训练</h1>
<h2 id="mirrored">Mirrored</h2>
<p>MirroredStrategy 是一种单机的同步的分布式训练策略。它支持在一台机器的多个 GPU 之间进行分布式训练，它会在每个 GPU 上创建一个模型副本，模型中的每个变量 (Variables) 都会进行镜像复制并放置到相应的 GPU 上，这些变量被称作镜像变量 (MirroredVariable)。</p>
<p>MirroredStrategy 策略通过 AllReduce 算法使得所有镜像变量在每个 GPU 之间保持同步更新， AllReduce 算法默认使用英伟达的 NcclAllReduce ，也可以通过 cross_device_ops 参数修改为其他的 AllReduce 算法，如 HierarchicalCopyAllReduce 。</p>
<p>MirroredStrategy 策略会自动使用所有能被 TensorFlow 发现的 GPU 来做分布式训练，如果只想使用部分的 GPU 则可以通过 devices 参数来指定。</p>
<p>MirroredStrategy 实例的创建代码如下所示：</p>
<pre><code class="language-py">mirrored_strategy = tf.distribute.MirroredStrategy(
    devices=[&quot;/gpu:0&quot;, &quot;/gpu:1&quot;],
    cross_device_ops=tf.distribute.HierarchicalCopyAllReduce(),
)

</code></pre>
<p>如果 TensorFlow 没有发现 GPU 则默认会退化为使用 CPU 来进行训练。 MirroredStrategy 的典型使用场景为单机多 GPU 。</p>
<p><strong>MirroredStrategy 的步骤如下：</strong></p>
<ul>
<li>
<p>训练开始前，该策略在所有 N 个计算设备上均各复制一份完整的模型；</p>
</li>
<li>
<p>每次训练传入一个批次的数据时，将数据分成 N 份，分别传入 N 个计算设备（即数据并行）；</p>
</li>
<li>
<p>N 个计算设备使用本地变量（镜像变量）分别计算自己所获得的部分数据的梯度；</p>
</li>
<li>
<p>使用分布式计算的 All-reduce 操作，在计算设备间高效交换梯度数据并进行求和，使得最终每个设备都有了所有设备的梯度之和；</p>
</li>
<li>
<p>使用梯度求和的结果更新本地变量（镜像变量）；</p>
</li>
<li>
<p>当所有设备均更新本地变量后，进行下一轮训练（即该并行策略是同步的）。</p>
</li>
</ul>
<h2 id="centralstorage">CentralStorage</h2>
<p>CentralStorageStrategy 也是一种单机的同步的分布式训练策略。但与 MirroredStrategy 策略不同的是，它会将模型的所有变量保存在 CPU 内存上，而不是通过镜像复制的方式保存在每个 GPU 上，所有的计算操作则会在每个 GPU 上以同样的方式执行。</p>
<p>如果机器只有一个 GPU ， 那么所有的变量和计算操作都会放在该 GPU 上。在对 CPU 上的变量进行更新前，该策略会先将所有 GPU 副本的上的变量梯度进行聚合，然后应用到 CPU 变量更新中。</p>
<p>CentralStorageStrategy 实例的创建代码如下所示：</p>
<pre><code class="language-py">central_storage_strategy = tf.distribute.experimental.CentralStorageStrategy()
</code></pre>
<p>CentralStorageStrategy 策略在 CPU 与 GPU 通信代价远低于 GPU 与 GPU 之间的通信代价时，较为适用，基本上很少会有这种情况出现。</p>
<h1 id="多机训练策略">多机训练策略</h1>
<h2 id="multiworkermirroredstrategy">MultiWorkerMirroredStrategy</h2>
<p>MultiWorkerMirroredStrategy 策略因为要涉及到多个 worker 节点之间的通信交互，因此每个 worker 节点需要提前获知集群中各节点配置信息以便在变量更新时使用。</p>
<p>TensorFlow 中定义集群配置信息的标准方式是使用 TF_CONFIG 环境变量来实现的，该环境变量定义了集群中所有节点的配置信息，包括所有 worker 节点的网络地址，当前 worker 节点的索引 (index) 以及当前 worker 节点的角色 (type)。</p>
<p>示例如下:</p>
<pre><code class="language-python">os.environ['TF_CONFIG'] = json.dumps({
    'cluster': {
        'worker': [&quot;localhost:20000&quot;, &quot;localhost:20001&quot;]
    },
    'task': {'type': 'worker', 'index': 0}
})
</code></pre>
<p>TF_CONFIG 由 cluster 和 task 两部分组成：</p>
<p>cluster 说明了整个多机集群的结构和每台机器的网络地址（IP + 端口号）。对于每一台机器，cluster 的值都是相同的；</p>
<p>task 说明了当前机器的角色。例如， {'type': 'worker', 'index': 0} 说明当前机器是 cluster 中的第 0 个 worker（即 localhost:20000 ）。每一台机器的 task 值都需要针对当前主机进行分别的设置。</p>
<p>以上内容设置完成后，在所有的机器上逐个运行训练代码即可。先运行的代码在尚未与其他主机连接时会进入监听状态，待整个集群的连接建立完毕后，所有的机器即会同时开始训练。</p>
<p>MultiWorkerMirroredStrategy 策略与 MirroredStrategy 策略很相似，可以理解为是 MirroredStrategy 策略的多机的同步的分布式训练版本，它也会在每一台机器上创建所有变量的副本。</p>
<p>多个 worker 节点之间使用 AllReduce 算法来保持模型变量的同步更新， TensorFlow 里将这一操作称为 CollectiveOps。 CollectiveOps 会在 TensorFlow 模型运行时自动根据硬件，网络拓扑以及张量的大小来自动选择合适的 AllReduce 算法来进行网络通信以完成变量更新。</p>
<p>MultiWorkerMirroredStrategy 策略目前有两种可供选择的 CollectiveOps 。 一种为 CollectiveCommunication.RING ，它使用 gRPC 作为通信层实现了基于环的 AllReduce 操作。 另一种为 CollectiveCommunication.NCCL， 它使用了英伟达的 NCCL 库来实现 AllReduce 操作。在实际使用中，可以基于自己的运行环境选择合适的 CollectiveOps，或者使用 CollectiveCommunication.AUTO 交由 TensorFlow 运行时自行选择。</p>
<p>MultiWorkerMirroredStrategy 实例的创建代码如下所示:</p>
<pre><code class="language-python">multiworker_strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(
    tf.distribute.experimental.CollectiveCommunication.RING)
</code></pre>
<p>如果所有 worker 节点都不包含 GPU ，则该策略会退化为使用 CPU 在多个 worker 节点间进行分布式训练。如果集群中的 worker 节点数量只有一个则该策略会退化为 MirroredStrategy 策略。</p>
<h2 id="parameterserverstrategy">ParameterServerStrategy</h2>
<p>ParameterServerStrategy 是一种多机的异步的分布式训练策略。所以它也需要提前指定 TF_CONFIG 环境变量信息，与 MultiWorkerMirroredStrategy 策略不同的是集群中的节点不全是 worker ，有一部分节点会被指定为 ps 用来存储变量信息。模型的每一个变量都会存储在一个 ps 节点上，所有的计算操作会在所有的 worker 节点上以同样的方式执行。 ParameterServerStrategy 实例的创建代码如下所示：</p>
<pre><code class="language-python">ps_strategy = tf.distribute.experimental.ParameterServerStrategy()
</code></pre>
<h1 id="分布式集群定义">分布式集群定义</h1>
<p>一个典型的 TF_CONFIG 环境变量的值如下所示：</p>
<pre><code class="language-json">{
  &quot;cluster&quot;: {
    &quot;chief&quot;: [&quot;host1:port&quot;],
    &quot;worker&quot;: [&quot;host2:port&quot;, &quot;host3:port&quot;],
    &quot;ps&quot;: [&quot;host4:port&quot;],
    &quot;evaluator&quot;: [&quot;host5:port&quot;]
  },
  &quot;task&quot;: {
    &quot;type&quot;: &quot;worker&quot;,
    &quot;index&quot;: 0
  }
}

</code></pre>
<p>chief 节点的作用和 worker 节点大致相同，不过它还会做一些额外的工作，比如保存检查点文件 (checkpoints) 以及为 Tensorboard 记录日志文件等，如果不指定 cheif 节点，则默认会以 worker 列表中的第一个节点作为 chief 节点； worker 节点用来执行训练操作； ps 节点用来存储变量，只有在使用 ParameterServerStrategy 训练策略时才需要指定； evaluator 用来执行交叉验证操作，一般也是在使用 ParameterServerStrategy 策略时才会指定。</p>
<p>注意所有节点的 TF_CONFIG 环境变量中的 cluster 信息都是相同的，不同的地方在于 task 部分，而且所有角色 (task type) 的 index 必须从 0 开始，因为 TensorFlow 会根据该 index 从 cluster 下相应角色的列表中读取节点信息。</p>
<p>TF_CONFIG 环境变量可以写入到系统的环境变量中，但前提是该物理节点上只会同时启动一个集群节点实例，在大多数情况下，我们会在 python 程序中通过 os.environ[&quot;TF_CONFIG&quot;] 来指定集群的信息以实现按需创建，TensorFlow 运行时会自动解析其中的信息并启动训练任务。</p>
<h1 id="tf-集群分布式训练的难点">TF 集群分布式训练的难点</h1>
<p>集群分布式训练的难点在于每个节点的 TF_CONFIG 环境变量的构建，因为我们不能在每次训练时都去手动指定 ip 和端口（还需确定该端口是否被占用），一两个节点还可以忍受，可如果同时运行多个训练任务，并且每个任务都会使用几十个集群节点，那么手动构造这个环境变量的工作量是巨大的。</p>
<p>我们需要找到一种自动构建 TF_CONFIG 环境变量的方法，一些分布式训练框架可以为我们排忧解难。比如阿里的 x-deeplearning。</p>
<h1 id="参考资料">参考资料</h1>
<p><a href="https://tf.wiki/zh_hans/appendix/distributed.html">TensorFlow分布式训练 — 简单粗暴 TensorFlow 2 0.4 beta 文档</a></p>
<p><a href="https://juejin.cn/post/6885151250124374023">TensorFlow 篇 | TensorFlow 2.x 分布式训练概览</a></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[tensorflow 1.x 的分布式训练]]></title>
        <id>https://DragonFive.github.io/post/tensorflow-1x-de-fen-bu-shi-xun-lian/</id>
        <link href="https://DragonFive.github.io/post/tensorflow-1x-de-fen-bu-shi-xun-lian/">
        </link>
        <updated>2019-05-16T07:04:42.000Z</updated>
        <content type="html"><![CDATA[<h1 id="模型并行">模型并行</h1>
<p>将模型部署到很多设备上（设备可能分布在不同机器上）运行，由于模型分割开的各个部分之间有相互依赖关系，因此计算效率不高。所以在模型大小不算太大的情况下一般不使用模型并行。</p>
<h1 id="数据并行">数据并行</h1>
<p>相比较模型并行，数据并行方式能够支持更大的训练规模，提供更好的扩展性，因此数据并行是深度学习最常采用的分布式训练策略。</p>
<blockquote>
<p>in-graph replication和between-graph replication 都用于数据并行。<br>
所谓 replication，指的是各个task，replication的对象是模型。<br>
在使用in-graph replication方式时，只有一个client进程（可以在参与训练的CPU或GPU上任选一个task来运行这个client，参与计算的其它tasks不运行这个client）来创建模型（即tf.Graph）及模型的参数（那些tf.Variables，比如权重W和偏置b）。由于参数（W和b）是共享的，该client指定把参数放在/job:ps，即parameter server上（比如 /job:ps/task:0/cpu:0）。模型的计算部分（前向传播，后向传播，loss和梯度计算，等等）也由该client进程定义好，然后client进程把这个计算部分分配到各个GPU device上（这个过程就相当于在各个GPU中复制模型），分配的方式类似函数调用，但每次调用都指定了设备（即 /job:worker/task:0/gpu:0，/job:worker/task:1/gpu:0，等等）。调用时，模型的参数（即W和b）被当作函数的参数输入给不同tasks（通常运行在不同GPU上）运行的模型，以保证这些参数确实是共享的。<br>
如果用between-graph replication方式，则每个task都运行自己的client进程用于创建模型和参数，并将参数pin到parameter server上（比如 /job:ps/task:0/cpu:0），然后各自独立地执行该模型。注意，每个task创建的模型必须一模一样，这很容易做到，因为只要每个task里的这部分代码都一样就行了。问题是，这些task各自创建并pin到parameter server上的模型参数是同样的吗？问这个问题是因为我们现在跑的是数据并行，而模型的参数及其更新都必须由parameter server统一处理。回答是，只要各task使用同样的parameter server设备名（比如都用 /job:ps/task:0/cpu:0）和同样的变量名（那些tf.Variable定义的变量，比如权重和偏置变量)， 那么在默认的情况下，它们被分配在parameter server的相同的存储里。</p>
</blockquote>
<p>由于in-graph replication的性能不好，现在基本上只使用between-graph replication了。</p>
<h1 id="参数更新方式">参数更新方式</h1>
<p>数据并行参数更新方式可以是同步的（synchronous），也可以是异步的（asynchronous）。</p>
<p>百度的综述<a href="https://arxiv.org/abs/2003.05622">Distributed Hierarchical GPU Parameter Server for Massive Scale Deep Learning Ads Systems</a> 介绍了三种同步模式：</p>
<ul>
<li>
<p>BSP(bulk sync parallel)，严格对所有worker的更新进行同步</p>
</li>
<li>
<p>SSP(stale sync parallel)，对快worker 进行同步</p>
</li>
<li>
<p>ASP（async parallel）, 不同步gradient</p>
</li>
</ul>
<p>后两种方式虽然提升了训练效率，但是降低了模型性能</p>
<p>XDL使用的是ASP。在tensorflow中异步训练是默认的并行训练模式。</p>
<h2 id="异步训练">异步训练</h2>
<p>异步训练中，各个设备完成一个mini-batch训练之后，不需要等待其它节点，直接去更新模型的参数。异步训练总体会训练速度会快很多，但是异步训练的一个很严重的问题是梯度失效问题（stale gradients），刚开始所有设备采用相同的参数来训练，但是异步情况下，某个设备完成一步训练后，可能发现模型参数已经被其它设备更新过了，此时这个设备计算出的梯度就过期了。由于梯度失效问题，异步训练可能陷入次优解。</p>
<h2 id="同步训练">同步训练</h2>
<p>所谓同步指的是所有的设备都是采用相同的模型参数来训练，等待所有设备的mini-batch训练完成后，收集它们的梯度后执行模型的一次参数更新。</p>
<p>Tensorflow提供了tf.train.SyncReplicasOptimizer类用于执行同步训练。把异步训练改造成同步训练只需要两步：</p>
<p>在原来的Optimizer上封装SyncReplicasOptimizer，将参数更新改为同步模式；</p>
<pre><code class="language-python">optimizer = tf.train.SyncReplicasOptimizer(optimizer, replicas_to_aggregate=num_workers)
</code></pre>
<p>在MonitoredTrainingSession或者EstimatorSpec的hook中增加sync_replicas_hook：</p>
<pre><code class="language-python"> sync_replicas_hook = optimizer.make_session_run_hook(is_chief, num_tokens=0)
</code></pre>
<p>同步训练需要各个设备的计算能力要均衡，而且要求集群的通信也要均衡，慢worker会拖慢整体进度。</p>
<h1 id="tensorflow-1-分布式架构">tensorflow 1 分布式架构</h1>
<p>2017年2月百度在PaddlePaddle平台上首次引入了ring-allreduce的架构，随后将其提交到tensorflow的contrib package中。同年8月，Uber为tensorflow平台开源了一个更加易用和高效的ring allreduce分布式训练库Horovod。<br>
最后，tensorflow官方终于也在1.11版本中支持了allreduce的分布式训练策略CollectiveAllReduceStrategy，其跟estimator配合使用非常方便，只需要构造tf.estimator.RunConfig 对象时传入CollectiveAllReduceStrategy参数即可。</p>
<p>关于 ring-allreduce 之前总结在 <a href="https://dragonfive.github.io/post/fen-bu-shi-jia-gou-ring-all-reduce-suan-fa/">分布式架构：ring all-reduce算法</a>。</p>
<h2 id="使用-tensorflow-estimator-api-来编写分布式训练代码">使用 TensorFlow Estimator API 来编写分布式训练代码</h2>
<p>要让tensorflow分布式运行，首先我们需要定义一个由参与分布式计算的机器组成的集群，如下：</p>
<pre><code class="language-py">cluster = {'chief': ['host0:2222'], 'ps': ['host1:2222', 'host2:2222'], 'worker': ['host3:2222', 'host4:2222', 'host5:2222']}
</code></pre>
<p>集群中一般有多个worker，需要指定其中一个worker为主节点（cheif），chief节点会执行一些额外的工作，比如模型导出之类的。在PS分布式架构环境中，还需要定义ps节点。</p>
<p>要运行分布式Estimator模型，只需要设置好TF_CONFIG环境变量即可，可参考如下代码：</p>
<pre><code class="language-py"># Example of non-chief node:
os.environ['TF_CONFIG'] = json.dumps( {'cluster': cluster, 'task': {'type': 'worker', 'index': 1}})

# Example of chief node:
os.environ['TF_CONFIG'] = json.dumps( {'cluster': cluster, 'task': {'type': 'chief', 'index': 0}}) 

# Example of evaluator node (evaluator is not part of training cluster) 
os.environ['TF_CONFIG'] = json.dumps( {'cluster': cluster, 'task': {'type': 'evaluator', 'index': 0}})
</code></pre>
<p>定义好上述环境变量后，调用tf.estimator.train_and_evaluate即可开始分布式训练和评估，其他部分的代码跟开发单机的程序是一样的，可以参考下面的资料：<br>
<a href="https://zhuanlan.zhihu.com/p/41473323">构建分布式Tensorflow模型系列:Estimator - 知乎</a></p>
<h1 id="参考资料">参考资料</h1>
<p><a href="https://zhuanlan.zhihu.com/p/56991108">一文说清楚Tensorflow分布式训练必备知识 - 知乎</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/60474307">什么是in-graph replication和between-graph replication? - 知乎</a></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[分布式架构：ring all-reduce算法]]></title>
        <id>https://DragonFive.github.io/post/fen-bu-shi-jia-gou-ring-all-reduce-suan-fa/</id>
        <link href="https://DragonFive.github.io/post/fen-bu-shi-jia-gou-ring-all-reduce-suan-fa/">
        </link>
        <updated>2019-03-22T07:13:20.000Z</updated>
        <content type="html"><![CDATA[<blockquote>
<p>注：本文内容多参考自 <a href="https://zhuanlan.zhihu.com/p/56991108">一文说清楚Tensorflow分布式训练必备知识 - 知乎</a></p>
</blockquote>
<p>PS架构中，当worker数量较多时，ps节点的网络带宽将成为系统的瓶颈。</p>
<p>AllReduce 架构是指不带有参数服务器的分布式集群架构。在该架构中，集群中的所有节点都作为 worker 来执行计算操作，该架构会在每个 batch 训练完成后使用 AllReduce 算法在所有 worker 节点间进行模型变量的同步更新。</p>
<p>目前应用于深度学习的 AllReduce 算法有多种，如 Ring AllReduce 以及 NCCL 等</p>
<p>传统的同步更新方法（各个gpu卡算好梯度，求和算平均的方式），在融合梯度时，会产生巨大的通信数据量，这种通信压力往往在模型参数量很大时，显得很明显。因此我们需要找到一种方法，来解决同步更新的网络瓶颈问题。其中最具代表性的一种方法就是：ring all-reduce。</p>
<p>Ring AllReduce架构中各个设备都是worker，没有中心节点来聚合所有worker计算的梯度。Ring AllReduce算法将 device 放置在一个逻辑环路（logical ring）中。每个 device 从上行的device 接收数据，并向下行的 deivce 发送数据，因此可以充分利用每个 device 的上下行带宽。</p>
<figure data-type="image" tabindex="1"><img src="https://DragonFive.github.io//post-images/1625216429335.jpeg" alt="" loading="lazy"></figure>
<p>梯度融合过程分为两阶段：</p>
<ol>
<li>
<p>Scatter Reduce：在这个 Scatter Reduce阶段，GPU 会逐步交换彼此的梯度并融合，最后每个 GPU 都会包含完整融合梯度的一部分</p>
</li>
<li>
<p>Allgather：GPU 会逐步交换彼此不完整的融合梯度，最后所有 GPU 都会得到完整的融合梯度</p>
</li>
</ol>
<p>使用 Ring Allreduce 算法进行某个稠密梯度的平均值的基本过程如下：</p>
<p>将每个设备上的梯度 tensor 切分成长度大致相等的 num_devices 个分片；</p>
<p>ScatterReduce 阶段：通过 num_devices - 1 轮通信和相加，在每个 device 上都计算出一个 tensor 分片的和；</p>
<p>AllGather 阶段：通过 num_devices - 1 轮通信和覆盖，将上个阶段计算出的每个 tensor 分片的和广播到其他 device；</p>
<p>在每个设备上合并分片，得到梯度和，然后除以 num_devices，得到平均梯度；</p>
<p>以 4 个 device上的梯度求和过程为例：</p>
<p>ScatterReduce 阶段：</p>
<figure data-type="image" tabindex="2"><img src="https://flomo.oss-cn-shanghai.aliyuncs.com/file/2021-07-02/32821/5db30ccec04cc0cb08d547fd89e42022.png" alt="图片来自知乎-杨旭东" loading="lazy"></figure>
<p>经过 num_devices - 1 轮后，每个 device 上都有一个 tensor 分片进得到了这个分片各个 device 上的和；</p>
<p>AllGather 阶段：</p>
<figure data-type="image" tabindex="3"><img src="https://flomo.oss-cn-shanghai.aliyuncs.com/file/2021-07-02/32821/05493e10065ea1d444da13f1f354798a.png" alt="图片来自知乎-杨旭东" loading="lazy"></figure>
<p>经过 num_devices - 1 轮后，每个 device 上都每个 tensor 分片都得到了这个分片各个 device 上的和；</p>
<p>相比PS架构，Ring Allreduce架构是带宽优化的，因为集群中每个节点的带宽都被充分利用。此外，在深度学习训练过程中，计算梯度采用BP算法，其特点是后面层的梯度先被计算，而前面层的梯度慢于前面层，Ring-allreduce架构可以充分利用这个特点，在前面层梯度计算的同时进行后面层梯度的传递，从而进一步减少训练时间。Ring Allreduce的训练速度基本上线性正比于GPUs数目（worker数）。</p>
<p>通信代价分析：每个 GPU 在Scatter Reduce 阶段，接收 N-1 次数据，N 是 GPU 数量；每个 GPU 在allgather 阶段，接收 N-1 次 数据；每个 GPU 每次发送 K/N 大小数据块，K 是总数据大小；所以，Data Transferred=2(N−1)*K/N ，随着 GPU 数量 N 增加，总传输量恒定。也就是理论上，随着gpu数量的增加，ring all-reduce有线性加速能力。</p>
<h2 id="参考资料">参考资料</h2>
<p><a href="https://zhuanlan.zhihu.com/p/69797852">浅谈Tensorflow分布式架构：ring all-reduce算法 - 知乎</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/56991108">一文说清楚Tensorflow分布式训练必备知识 - 知乎</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/34172340">【第一期】AI Talk：TensorFlow 分布式训练的线性加速实践 - 知乎</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/79030485">腾讯机智团队分享--AllReduce算法的前世今生 - 知乎</a></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[深度学习框架的并行优化方法小结]]></title>
        <id>https://DragonFive.github.io/post/deeplearning-parrel/</id>
        <link href="https://DragonFive.github.io/post/deeplearning-parrel/">
        </link>
        <updated>2018-08-11T08:17:26.000Z</updated>
        <content type="html"><![CDATA[<p>title: 深度学习框架的并行优化方法小结</p>
<p>categories:</p>
<ul>
<li>深度学习<br>
tags:</li>
<li>deeplearning</li>
<li>mpi</li>
<li>caffe</li>
</ul>
<p>目前的深度学习领域就是海量的数据加上大量的数学运算，所以计算量相当的大，训练一个模型跑上十天半个月啥的是常事。那此时分布式的意义就出现了，既然一张GPU卡跑得太慢就来两张，一台机器跑得太慢就用多台机器。</p>
<p><strong>数据并行</strong></p>
<figure data-type="image" tabindex="1"><img src="https://www.github.com/DragonFive/CVBasicOp/raw/master/1505026360037.jpg" alt="数据并行" loading="lazy"></figure>
<p>每一个节点（或者叫进程）都有一份模型，然后各个节点取不同的数据，通常是一个batch_size，然后各自完成前向和后向的计算得到梯度，这些进行训练的进程我们成为<strong>worker</strong>，除了worker，还有<strong>参数服务器</strong>，简称ps server，这些worker会把各自计算得到的梯度送到ps server，然后由ps server来进行update操作，然后把update后的模型再传回各个节点。因为在这种并行模式中，被划分的是数据，所以这种并行方式叫<strong>数据并行</strong>。</p>
<p>数据并行有<strong>同步模式和异步模式</strong>之分。同步模式中，所有训练程序同时训练一个批次的训练数据，完成后经过同步，再同时交换参数。参数交换完成后所有的训练程序就有了共同的新模型作为起点，再训练下一个批次。而异步模式中，训练程序完成一个批次的训练数据，立即和参数服务器交换参数，不考虑其他训练程序的状态。异步模式中一个训练程序的最新结果不会立刻体现在其他训练程序中，直到他们进行下次参数交换。</p>
<p><a href="http://blog.csdn.net/xsc_c/article/details/42420167"> 卷积神经网络的并行化模型</a></p>
<h1 id="parameter-server">parameter server</h1>
<p>limu的parameter server， MSRA的adam和google的tensorflow。</p>
<p><a href="https://www.zhihu.com/question/26998075">最近比较火的parameter server是什么？</a></p>
<p><a href="http://www.cs.cmu.edu/~muli/file/ps.pdf">李沐：Parameter Server for Distributed Machine Learning</a></p>
<p>参数服务器是个编程框架，用于方便分布式并行程序的编写，其中重点是对大规模参数的分布式存储和协同的支持。</p>
<p>参数服务器就类似于MapReduce，是大规模机器学习在不断使用过程中，抽象出来的框架之一。重点支持的就是<strong>参数的分布式</strong>，毕竟巨大的模型其实就是巨大的参数。</p>
<h2 id="架构">架构：</h2>
<p>集群中的节点可以分为<strong>计算节点和参数服务节点</strong>两种。其中，计算节点负责对分配到自己本地的训练数据（块）计算学习，并更新对应的参数；参数服务节点采用分布式存储的方式，各自存储全局参数的一部分，并作为服务方接受计算节点的参数查询和更新请求。简而言之吧，计算节点负责干活和更新参数，参数服务节点则负责存储参数。</p>
<h2 id="冗余和恢复">冗余和恢复：</h2>
<p>类似MapReduce，每个参数在参数服务器的集群中都在多个不同节点上备份（<strong>3个</strong>也是极好的），这样当出现节点失效时，冗余的参数依旧能够保证服务的有效性。当有新的节点插入时，把原先失效节点的参数从冗余参数那边复制过来，失效节点的接班人就加入队伍了。</p>
<h2 id="并行计算">并行计算：</h2>
<p>并行计算这部分主要在计算节点上进行。 类似于MapReduce，分配任务时，会将数据拆分给每个worker节点。参数服务器在开始学习前，也会把大规模的训练数据拆分到每个计算节点上。单个计算节点就对本地数据进行学习就可以了。学习完毕再把参数的更新梯度上传给对应的参数服务节点进行更新。</p>
<h2 id="流程">流程</h2>
<p>1.分发训练数据 -&gt; 节点1 节点2   节点3   ... 节点i  ... 节点N<br>
2.节点i 学习过程：遍历本地的训练数据，统计所有需要的参数(key)向分布式的参数服务器查询需要的参数（注意，本地数据对应到的参数只是全局参数的一小部分）得到查询到的参数值，用于模型的本地训练一轮训练完毕，得到所有对应参数的更新，将更新上传给参数服务器<br>
3.参数服务器更新参数过程：参数服务器得到计算节点传过来的局部更新，<strong>汇总后更新本地数据</strong></p>
<h1 id="并行程序">并行程序</h1>
<h2 id="并行实现实现方式">并行实现实现方式：</h2>
<ol>
<li>任务并行：将任务分配带若干计算核上;</li>
<li><strong>数据并行</strong>：将数据进行分割，然后由不同的计算核进行处理，<strong>每个核在规模相当的数据集上大致采用相同的操作</strong>。这不由使我想到了<strong>CAFFE中的对GPU的运用来实现并行训练</strong>的思路，就是将数据集进行分割，每个GPU并行处理各自对应的数据集。</li>
</ol>
<p>多指令多数据流又分为分布式内存系统和共享内存系统。<br>
<strong>分布式内存系统</strong>：<br>
每个处理器由独立的内存，通过<strong>消息传递函数</strong>来通信。<br>
共享式内存系统：<br>
多个处理器能访问内存系统中的相同内存，通过共享内存进行通信。<br>
<strong>MPI</strong>就是用来在分布式系统中为各处理器进行消息传递的API。</p>
<p>各个核能够直接访问自己的内存，而运行在不同核之间的进程需要交换内存数据的时候，只能通过消息传递API来实现。消息传递的API至少要提供一个发送函数和接收函数。**进程之间通过它们的序号（rank）**进行识别。</p>
<h2 id="并行程序的流程">并行程序的流程</h2>
<p>a、任务或者<strong>数据划分</strong>，就是要识别出任务中可以进行并行执行的部分。<br>
b、不同任务之间的<strong>通信</strong>;<br>
c、<strong>聚合</strong>，将任务和通信进行集合，聚合成更大的任务;<br>
d、<strong>分配</strong>，将聚合的任务分配到进程或线程中。</p>
<p>1、MPI是进程级别的，通过通信在进程之间进行消息传递。<br>
2、编程模型复杂：<br>
a、需要进行任务划分;<br>
b、通信延迟和负载不均衡;通信延迟很好理解，负载不均衡是因为分布式的系统，每个处理的任务量不同？待进一步的解释 ；<br>
c、可靠性差，一个进程出错，整个程序崩溃。第一感觉就是这简直是MPI的命门。在分布式系统中某一个进程出错是很容易的，为MPI的命运担忧。</p>
<h1 id="通信函数">通信函数</h1>
<h2 id="一般函数">一般函数</h2>
<pre><code class="language-cpp">int MPI_Send (void *buf, int count, MPI_Datatype datatype,int dest, int tag,MPI_Comm comm)
</code></pre>
<p>参数buf为发送缓冲区；count为发送的数据个数；datatype为发送的数据类型；dest为消息的目的地址(进程号)，其取值范围为0到np－1间的整数(np代表通信器comm中的进程数) 或MPI_PROC_NULL；tag为消息标签，其取值范围为0到MPI_TAG_UB间的整数；<strong>comm为通信器</strong></p>
<pre><code class="language-cpp">mpi_recv:接收信息   MPI_Probe：预测一下消息的size
</code></pre>
<h2 id="mpi聚合通信">mpi聚合通信</h2>
<p>collective communication。聚合通信是在通信子中的所有的进程都参与的通信方式。</p>
<h3 id="同步-mpi_barrier">同步 MPI_Barrier</h3>
<p>MPI_Barrier就是这样的一个函数，他确保除非所有的进程同时调用，否则他不会允许任何进程通过这个节点<br>
对于所有的进程来说，聚合通信必然包含了一个<strong>同步点</strong>。也就是说所有的进程必须在他们又一次执行新动作之前都到达某个点。这跟GPU中线程同步的概念很相似，很好理解。</p>
<h3 id="广播">广播</h3>
<p>广播机制：<br>
一个进程将相同的数据发送给通信子中所有的进程。该机制最主要的应用是将输入数据发送给并行程序，或者将<strong>配置参数</strong>发送给所有的进程</p>
<pre><code class="language-cpp">MPI_Bcast(
    void* data,//数据
    int count,//数据个数
    MPI_Datatype datatype,
    int root,//根进程编号
    MPI_Comm communicator)
</code></pre>
<h3 id="mpi_scatter-数据分发">MPI_Scatter 数据分发</h3>
<p>MPI_Scatter与MPI_Bcast非常相似，都是<strong>一对多</strong>的通信方式，不同的是后者的<strong>0号进程</strong>将相同的信息发送给所有的进程，而前者则是将一段array 的不同部分发送给所有的进程</p>
<figure data-type="image" tabindex="2"><img src="https://www.github.com/DragonFive/CVBasicOp/raw/master/1502761049076.jpg" alt="scatter与bcast的区别" loading="lazy"></figure>
<pre><code class="language-cpp">MPI_Scatter(
    void* send_data,//存储在0号进程的数据，array
    int send_count,//具体需要给每个进程发送的数据的个数
    //如果send_count为1，那么每个进程接收1个数据；如果为2，那么每个进程接收2个数据
    MPI_Datatype send_datatype,//发送数据的类型
    void* recv_data,//接收缓存，缓存 recv_count个数据
    int recv_count,
    MPI_Datatype recv_datatype,
    int root,//root进程的编号
    MPI_Comm communicator)
</code></pre>
<p>通常send_count等于array的元素个数除以进程个数。</p>
<h3 id="mpi_gather">MPI_Gather</h3>
<p>MPI_Gather和MPI_scatter刚好相反，他的作用是从所有的进程中将每个进程的数据集中到根进程中，<strong>同样根据进程的编号对array元素排序</strong></p>
<figure data-type="image" tabindex="3"><img src="https://www.github.com/DragonFive/CVBasicOp/raw/master/1502761558789.jpg" alt="mpi_gather" loading="lazy"></figure>
<pre><code class="language-cpp">MPI_Gather(
    void* send_data,
    int send_count,
    MPI_Datatype send_datatype,
    void* recv_data,
    int recv_count,//注意该参数表示的是从单个进程接收的数据个数，不是总数
    MPI_Datatype recv_datatype,
    int root,
    MPI_Comm communicator)
</code></pre>
<h3 id="mpi_allgather-多对多通信">MPI_Allgather 多对多通信</h3>
<p>当数据分布在所有的进程中时，MPI_Allgather将所有的数据聚合到每个进程中。</p>
<figure data-type="image" tabindex="4"><img src="https://www.github.com/DragonFive/CVBasicOp/raw/master/1502761637900.jpg" alt="mpi_Allgather" loading="lazy"></figure>
<h2 id="数据归约-reduce">数据归约 Reduce</h2>
<p>Reduce——规约是来自函数式编程的一个经典概念。数据规约包含通过一个函数将一批数据分成较小的一批数据。比如将一个数组的元素通过加法函数规约为一个数字。</p>
<h3 id="mpi_reduce">mpi_reduce</h3>
<p>与MPI_Gather类似，MPI_Reduce在每个进程上都有一组输入元素，并将<strong>一个输出元素数组返回给根进程</strong>。 输出元素包含被规约的结果。</p>
<pre><code class="language-cpp">MPI_Reduce(
    void* send_data,
    void* recv_data,
    int count,
    MPI_Datatype datatype,
    MPI_Op op,
    int root,
    MPI_Comm communicator)
</code></pre>
<blockquote>
<p>send_data参数指向的是每个进程想要规约的datatype类型的元素数组。<br>
recv_data仅与根进程相关。<br>
recv_data数组包含规约的结果，并具有sizeof（datatype）* count的大小的内存。<br>
op参数是要应用于数据的操作。</p>
</blockquote>
<p>mpi支持的操作有</p>
<blockquote>
<p>MPI_MAX - 返回最大值.<br>
MPI_MIN - 返回最小值.<br>
MPI_SUM -元素和.<br>
MPI_PROD - 元素乘积.<br>
MPI_LAND - 逻辑与.<br>
MPI_LOR - 逻辑或<br>
MPI_BAND -按位与<br>
MPI_BOR - 按位或<br>
MPI_MAXLOC - 返回最大值和拥有该值的进程编号<br>
MPI_MINLOC - 返回最小值和拥有该值的进程编号.```</p>
</blockquote>
<p>如果每个进程中的数组拥有两个元素，那么规约结果是对两个对位的元素进行规约的。</p>
<figure data-type="image" tabindex="5"><img src="https://www.github.com/DragonFive/CVBasicOp/raw/master/1502762764619.jpg" alt="两个元素的归约结果" loading="lazy"></figure>
<h3 id="mpi_allreduce">mpi_allReduce</h3>
<figure data-type="image" tabindex="6"><img src="https://www.github.com/DragonFive/CVBasicOp/raw/master/1502762804609.jpg" alt="归约后分发给所有的进程" loading="lazy"></figure>
<h1 id="parameter-server-2">parameter-server</h1>
<h1 id="cuda-c编程">CUDA C编程</h1>
<h2 id="cuda运行时函数">cuda运行时函数</h2>
<p>cuda运行时提供了丰富的函数，功能涉及设备管理、存储管理、数据传输、线程管理、流管理、事件管理、纹理管理、执行控制等。</p>
<h3 id="设备管理函数">设备管理函数</h3>
<p>函数声明一般这样</p>
<pre><code>extern __host__ cudaError_t CUDARTAPI 函数名(参数列表)
</code></pre>
<p><strong>cudaGetDeviceCount</strong><br>
获得计算能力大于等于1.0的GPU数量</p>
<pre><code class="language-cpp">int count;
cudaGetDeviceCount(&amp;count);
</code></pre>
<p><strong>cudaSetDevice</strong><br>
设置使用的GPU索引号，如果不设置默认使用0号GPU</p>
<pre><code class="language-cpp">int gpuid = 0;
cudaSetDevice(gpuid);

</code></pre>
<p><strong>cudaGetDevice</strong><br>
获得当前线程的GPU设备号</p>
<pre><code class="language-cpp">int gpuid;
cudaGetDevice(&amp;gpuid);

</code></pre>
<p><strong>cudaSetValidDevices</strong></p>
<p>设置多个device,len表示签名设备号数组的长度;</p>
<pre><code class="language-cpp">cudaSetValidDevices(int &amp;device_arr, int len);
</code></pre>
<h3 id="存储管理函数">存储管理函数</h3>
<p><strong>cudaMalloc</strong></p>
<p>在GPU上分配大小为size的现行存储空间，起始地址为 *devPtr</p>
<pre><code class="language-cpp">cudaMalloc(void **devPtr,size_t size);

</code></pre>
<p><strong>cudaMallocPitch</strong></p>
<p>在GPU上分配大小为PitchxHight的逻辑2D线性存储空间，首地址为<code>*devPtr</code>, 其中Pitch是返回的width对齐后的存储空间的宽度</p>
<pre><code class="language-cpp">cudaMallocPitch(void **devPtr, size_t *pitch, size_t width, size_t height);
</code></pre>
<pre><code>devPtr[x] = devPtr[rowid*pitch+column]
</code></pre>
<p><strong>cudaFree</strong><br>
清空指定的GPU存储区域，可释放cudaMalloc和cudaMallocPitch分类的GPU存储区域</p>
<pre><code class="language-cpp">cudaFree(void *devPtr);

</code></pre>
<p><strong>cudaMemset</strong><br>
将GPU端的devPtr指针指向的count长度的存储空间赋值为value.</p>
<pre><code class="language-cpp">cudaMemset(void 8DevPTR， int value,size_t count);
</code></pre>
<p><strong>cudaHostAlloc</strong><br>
在主机端(CPU)根据flag值来分配页锁定存储,</p>
<pre><code class="language-cpp">cudaHostAlloc(void **pHost, size_t size, usigned int flags);
</code></pre>
<p>flags可以有四种取值</p>
<pre><code class="language-cpp">cudaHostAllocDefault   分配默认存储
cudaHostAllocPortable  分配的存储可以被cuda索引
cudaHostAllocMapped 分配的存储映射到GPU
。。。

</code></pre>
<h3 id="数据传输函数">数据传输函数</h3>
<p><strong>cudaMemcpy</strong></p>
<pre><code class="language-cpp">cudaMemcpy(void * dst, const void *src, size_t count, enum cudaMemcpyKind kind);
</code></pre>
<p>主机(cpu内存)与设备间的数据传输函数，源地址是<code>*src</code>，目标地址是<code>*dst</code>,传输长度为<code>count</code>,kind指定了传输的方向，kind可选值域如下：</p>
<pre><code class="language-cpp">cudaMemcpyHostToHost = 0;
cudaMemcpyHostToDevice = 0;
cudaMemcpyDeviceToHost = 0;
cudaMemcpyDeviceToDevice = 0;
</code></pre>
<p>还有其它的形式</p>
<h3 id="线程管理函数">线程管理函数</h3>
<p><strong>cudaThreadSynchronize</strong></p>
<p>CPU与GPU之间的同步函数，保证该函数前的CPU和GPU上的任务均执行完成，并在该函数位置汇合。一般是CPU在该函数处等待GPU函数执行完。</p>
<pre><code class="language-cpp">cudaThreadSynchronize(void);
</code></pre>
<h1 id="reference">reference</h1>
<p>《GPU编程与优化》——方民权</p>
<h1 id="reference-2">reference</h1>
<p><a href="http://blog.csdn.net/sinat_22336563/article/details/69486937">MPI学习笔记之并行程序概述</a></p>
<p><a href="http://blog.csdn.net/xsc_c/article/details/42420167"> 卷积神经网络的并行化模型</a></p>
<p><a href="https://www.zhihu.com/search?type=content&amp;q=parameter+server">知乎 parameter server</a></p>
<p><a href="http://blog.csdn.net/xbinworld/article/details/74781605">分布式机器学习系统笔记（一）——模型并行，数据并行，参数平均，ASGD</a></p>
<p><a href="http://djt.qq.com/article/view/1245">深度学习及并行化实现概述</a></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[基于知识迁移的深度神经网络压缩方法研究]]></title>
        <id>https://DragonFive.github.io/post/networker-thransfer/</id>
        <link href="https://DragonFive.github.io/post/networker-thransfer/">
        </link>
        <updated>2018-05-20T03:40:48.000Z</updated>
        <content type="html"><![CDATA[<hr>
<p>title: 基于知识迁移的深度神经网络压缩方法研究<br>
date: 2018/5/20 12:04:12<br>
tags:</p>
<ul>
<li>神经网络压缩</li>
<li>深度学习</li>
<li>神经网络</li>
</ul>
<hr>
<p>我的毕设题目是基于知识迁移的深度神经网络压缩方法研究，由于本文涉及实验室的后续研究与项目开发，暂时删除该内容，等待时机合适再公开</p>
<p>——待续</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[mxnet gluon ]]></title>
        <id>https://DragonFive.github.io/post/gluon-study/</id>
        <link href="https://DragonFive.github.io/post/gluon-study/">
        </link>
        <updated>2018-03-22T03:39:08.000Z</updated>
        <content type="html"><![CDATA[<hr>
<p>title: gluon学习笔记<br>
date: 2018/3/22 12:04:12<br>
categories:</p>
<ul>
<li>深度学习<br>
tags:</li>
<li>目标检测</li>
<li>深度学习</li>
<li>神经网络</li>
</ul>
<hr>
<h1 id="学到的新知识">学到的新知识</h1>
<h2 id="bn放在relu后面">bn放在relu后面</h2>
<p><a href="http://minibatch.net/2017/06/11/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87-Batch-Normalization/">BN应该放在relu后</a></p>
<p><a href="https://mp.weixin.qq.com/s/xJromD5Q30KlRhB_kM4kfA">用于分类、检测和分割的移动网络 MobileNetV2</a></p>
<p><a href="https://www.zhihu.com/question/265709710">如何评价MobileNetV2</a></p>
<h2 id="卷积核的数量">卷积核的数量</h2>
<p><a href="http://zh.gluon.ai/chapter_convolutional-neural-networks/cnn-scratch.html">卷积神经网络 — 从0开始</a></p>
<p>当输入数据有多个通道的时候，每个通道会有对应的权重，然后会对每个通道做卷积之后在通道之间求和。所以当输出只有一个的时候，卷积的channel数目和data的channel数目是一样的。</p>
<p>当输出需要多通道时，每个输出通道有对应权重，然后每个通道上做卷积。所以当输入有n个channel，输出有h个channel时，卷积核channel数目为n * h，每个输出channel对应一个bias ,卷积核的维度为(h,n,w,h)</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>c</mi><mi>o</mi><mi>n</mi><mi>v</mi><mo>(</mo><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi><mo separator="true">,</mo><mi>w</mi><mo separator="true">,</mo><mi>b</mi><mo>)</mo><mo>[</mo><mo>:</mo><mo separator="true">,</mo><mi>i</mi><mo separator="true">,</mo><mo>:</mo><mo separator="true">,</mo><mo>:</mo><mo>]</mo><mo>=</mo><mi>c</mi><mi>o</mi><mi>n</mi><mi>v</mi><mo>(</mo><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi><mo separator="true">,</mo><mi>w</mi><mo>[</mo><mi>i</mi><mo separator="true">,</mo><mo>:</mo><mo separator="true">,</mo><mo>:</mo><mo separator="true">,</mo><mo>:</mo><mo>]</mo><mo separator="true">,</mo><mi>b</mi><mo>[</mo><mi>i</mi><mo>]</mo><mo>)</mo></mrow><annotation encoding="application/x-tex">conv(data, w, b)[:,i,:,:] = conv(data, w[i,:,:,:], b[i])
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">c</span><span class="mord mathdefault">o</span><span class="mord mathdefault">n</span><span class="mord mathdefault" style="margin-right:0.03588em;">v</span><span class="mopen">(</span><span class="mord mathdefault">d</span><span class="mord mathdefault">a</span><span class="mord mathdefault">t</span><span class="mord mathdefault">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">b</span><span class="mclose">)</span><span class="mopen">[</span><span class="mrel">:</span></span><span class="base"><span class="strut" style="height:0.85396em;vertical-align:-0.19444em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">i</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">:</span></span><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">:</span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mclose">]</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">c</span><span class="mord mathdefault">o</span><span class="mord mathdefault">n</span><span class="mord mathdefault" style="margin-right:0.03588em;">v</span><span class="mopen">(</span><span class="mord mathdefault">d</span><span class="mord mathdefault">a</span><span class="mord mathdefault">t</span><span class="mord mathdefault">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mopen">[</span><span class="mord mathdefault">i</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">:</span></span><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">:</span></span><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">:</span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mclose">]</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">b</span><span class="mopen">[</span><span class="mord mathdefault">i</span><span class="mclose">]</span><span class="mclose">)</span></span></span></span></span></p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">Γ</mi><mo>(</mo><mi>z</mi><mo>)</mo><mo>=</mo><msubsup><mo>∫</mo><mn>0</mn><mi mathvariant="normal">∞</mi></msubsup><msup><mi>t</mi><mrow><mi>z</mi><mo>−</mo><mn>1</mn></mrow></msup><msup><mi>e</mi><mrow><mo>−</mo><mi>t</mi></mrow></msup><mi>d</mi><mi>t</mi><mtext> </mtext><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\Gamma(z) = \int_0^\infty t^{z-1}e^{-t}dt\,.
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">Γ</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.04398em;">z</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.326242em;vertical-align:-0.9119499999999999em;"></span><span class="mop"><span class="mop op-symbol large-op" style="margin-right:0.44445em;position:relative;top:-0.0011249999999999316em;">∫</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.414292em;"><span style="top:-1.7880500000000001em;margin-left:-0.44445em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span><span style="top:-3.8129000000000004em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">∞</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.9119499999999999em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">t</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.864108em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.04398em;">z</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.843556em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathdefault mtight">t</span></span></span></span></span></span></span></span></span><span class="mord mathdefault">d</span><span class="mord mathdefault">t</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">.</span></span></span></span></span></p>
<p>123</p>
<figure data-type="image" tabindex="1"><img src="https://www.github.com/DragonFive/CVBasicOp/raw/master/1513949196873.jpg" alt="inception v1" loading="lazy"></figure>
<figure data-type="image" tabindex="2"><img src="https://www.github.com/DragonFive/CVBasicOp/raw/master/1514013854016.jpg" alt="residual" loading="lazy"></figure>
<figure data-type="image" tabindex="3"><img src="https://www.github.com/DragonFive/CVBasicOp/raw/master/1514012389756.jpg" alt="resnet各种结构" loading="lazy"></figure>
<h1 id="gluon语法">gluon语法</h1>
<h2 id="nnblock与nnsequential的嵌套使用">nn.Block与nn.sequential的嵌套使用</h2>
<pre><code class="language-python">class RecMLP(nn.Block):
    def __init__(self, **kwargs):
        super(RecMLP, self).__init__(**kwargs)
        self.net = nn.Sequential()
        with self.name_scope():
            self.net.add(nn.Dense(256, activation=&quot;relu&quot;))
            self.net.add(nn.Dense(128, activation=&quot;relu&quot;))
            self.dense = nn.Dense(64)

    def forward(self, x):
        return nd.relu(self.dense(self.net(x)))

rec_mlp = nn.Sequential()
rec_mlp.add(RecMLP())
rec_mlp.add(nn.Dense(10))
print(rec_mlp)
</code></pre>
<h2 id="初始化与参数访问">初始化与参数访问</h2>
<pre><code class="language-python">from mxnet import init
params.initialize(init=init.Normal(sigma=0.02), force_reinit=True)
print(net[0].weight.data(), net[0].bias.data())
</code></pre>
<p>我们也可以通过collect_params来访问Block里面所有的参数（这个会包括所有的子Block）。它会返回一个名字到对应Parameter的dict。</p>
<p>也可以自定义各层的初始化方法，没有自定义的按照net.initialize里面的方法进行定义</p>
<pre><code class="language-python">from mxnet.gluon import nn
from mxnet import nd
from mxnet import init

def get_net():
    net = nn.Sequential()
    with net.name_scope():
        net.add(nn.Dense(4,activation=&quot;relu&quot;))#,weight_initializer=init.Xavier()))
        net.add(nn.Dense(2,weight_initializer=init.Zero(),bias_initializer=init.Zero()) )
    return net

x = nd.random.uniform(shape=(3,5))
net = get_net()
net.initialize(init.One())
net(x)
print(net[1].weight.data
</code></pre>
<h2 id="gpu访问">GPU访问</h2>
<ol>
<li>删除cpu版本mxnet</li>
</ol>
<pre><code class="language-bash">pip uninstall mxnet
</code></pre>
<ol start="2">
<li>更新GPU版本mxnet</li>
</ol>
<pre><code class="language-bash">pip install -U --pre mxnet-cu80
</code></pre>
<ol start="3">
<li>查看版本号</li>
</ol>
<pre><code class="language-python">import pip
for pkg in ['mxnet', 'mxnet-cu75', 'mxnet-cu80']:
    pip.main(['show', pkg])
</code></pre>
<h2 id="使用jupyter的相关插件">使用jupyter的相关插件</h2>
<ol>
<li>notedown插件<br>
可以在jupyter 中查看markdown文件</li>
<li>nb_conda<br>
是conda的插件，可以在jupyter里面修改python内核版本</li>
</ol>
<h2 id="优化方法">优化方法</h2>
<p><strong>momentum</strong><br>
gluon.Trainer的learning_rate属性和set_learning_rate函数可以随意调整学习率。</p>
<pre><code class="language-python">trainer = gluon.Trainer(net.collect_params(), 'sgd',
                            {'learning_rate': lr, 'momentum': mom})
</code></pre>
<p><strong>adagrad</strong><br>
Adagrad是一个在迭代过程中不断自我调整学习率，并让模型参数中每个元素都使用不同学习率的优化算法。</p>
<pre><code class="language-python">    trainer = gluon.Trainer(net.collect_params(), 'adagrad',
                            {'learning_rate': lr})
</code></pre>
<p><strong>Adam</strong></p>
<pre><code class="language-python">trainer = gluon.Trainer(net.collect_params(), 'adam',
                            {'learning_rate': lr})

</code></pre>
<p>通过以上分析, 理论上可以说, 在数据比较稀疏的时候, adaptive 的方法能得到更好的效果, 例如, adagrad, adadelta, rmsprop, adam 等. 在数据稀疏的情况下, adam 方法也会比 rmsprop 方法收敛的结果要好一些, 所以, 通常在没有其它更好的理由的前框下, 我会选用 adam 方法, 可以比较快地得到一个预估结果. 但是, 在论文中, 我们看到的大部分还是最原始的 mini-batch 的 SGD 方法. 因为马鞍面的存在等问题, SGD 方法有时候较难收敛. 另外, SGD 对于参数的初始化要求也比较高. 所以, 如果要是想快速收敛的话, 建议使用 adam 这类 adaptive 的方法</p>
<h2 id="延迟执行">延迟执行</h2>
<p>延后执行使得系统有更多空间来做性能优化。但我们推荐每个批量里至少有一个同步函数，例如对损失函数进行评估，来避免将过多任务同时丢进后端系统。</p>
<pre><code class="language-python">from mxnet import autograd

mem = get_mem()

total_loss = 0
for x, y in get_data():
    with autograd.record():
        L = loss(y, net(x))
    total_loss += L.sum().asscalar()
    L.backward()
    trainer.step(x.shape[0])

nd.waitall()
print('Increased memory %f MB' % (get_mem() - mem))

</code></pre>
<h2 id="多gpu训练">多GPU训练</h2>
<pre><code class="language-python">ctx = [gpu(i) for i in range(num_gpus)]
data_list = gluon.utils.split_and_load(data, ctx)
label_list = gluon.utils.split_and_load(label, ctx)



</code></pre>
<h2 id="fintune-微调">fintune 微调</h2>
<p><a href="https://fiercex.github.io/post/gluon_features_fine/">gluon微调</a></p>
<h1 id="一些可以重复使用的代码">一些可以重复使用的代码</h1>
<h2 id="读取数据">读取数据</h2>
<pre><code class="language-python">from mxnet import gluon
from mxnet import ndarray as nd

def transform(data, label):
    return data.astype('float32')/255, label.astype('float32')
mnist_train = gluon.data.vision.FashionMNIST(train=True, transform=transform)
mnist_test = gluon.data.vision.FashionMNIST(train=False, transform=transform)

</code></pre>
<h2 id="计算精度">计算精度</h2>
<pre><code class="language-python">def accuracy(output, label):
    return nd.mean(output.argmax(axis=1)==label).asscalar()


</code></pre>
<p>我们先使用Flatten层将输入数据转成 batch_size x ? 的矩阵，然后输入到10个输出节点的全连接层。照例我们不需要制定每层输入的大小，gluon会做自动推导。</p>
<h2 id="激活函数">激活函数</h2>
<p><strong>sigmoid</strong></p>
<pre><code class="language-python">from mxnet import nd
def softmax(X):
    exp = nd.exp(X)
    # 假设exp是矩阵，这里对行进行求和，并要求保留axis 1，
    # 就是返回 (nrows, 1) 形状的矩阵
    partition = exp.sum(axis=1, keepdims=True)
    return exp / partition


</code></pre>
<p><strong>relu</strong></p>
<pre><code class="language-python">def relu(X):
    return nd.maximum(X, 0)

</code></pre>
<h2 id="损失函数">损失函数</h2>
<p><strong>平方误差</strong></p>
<pre><code class="language-python">square_loss = gluon.loss.L2Loss()


</code></pre>
<pre><code class="language-python">def square_loss(yhat, y):
    # 注意这里我们把y变形成yhat的形状来避免矩阵形状的自动转换
    return (yhat - y.reshape(yhat.shape)) ** 2
 

</code></pre>
<p><strong>交叉熵损失</strong></p>
<pre><code class="language-python">def cross_entropy(yhat, y):
    return - nd.pick(nd.log(yhat), y)

</code></pre>
<pre><code class="language-python">softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()

</code></pre>
<h2 id="取一个batch_size的代码">取一个batch_size的代码</h2>
<p><strong>scratch版本</strong></p>
<pre><code class="language-python">import random
batch_size = 1
def data_iter(num_examples):
    idx = list(range(num_examples))
    random.shuffle(idx)
    for i in range(0, num_examples, batch_size):
        j = nd.array(idx[i:min(i+batch_size,num_examples)])
        yield X.take(j), y.take(j)

</code></pre>
<p><strong>gluon版本</strong></p>
<pre><code class="language-python">batch_size = 1
dataset_train = gluon.data.ArrayDataset(X_train, y_train)
data_iter_train = gluon.data.DataLoader(dataset_train, batch_size, shuffle=True)



</code></pre>
<h2 id="初始化权值">初始化权值</h2>
<p><strong>scratch版本</strong></p>
<pre><code class="language-python">def get_params():
    w = nd.random.normal(shape=(num_inputs, 1))*0.1
    b = nd.zeros((1,))
    for param in (w, b):
        param.attach_grad()
    return (w, b)

</code></pre>
<p><strong>gluon版本</strong></p>
<pre><code class="language-python">net.initialize()


net.collect_params().initialize(mx.init.Normal(sigma=1))

</code></pre>
<h2 id="sgd">SGD</h2>
<p><strong>scratch版本</strong></p>
<pre><code class="language-python">def SGD(params, lr):
    for param in params:
        param[:] = param - lr * param.grad


</code></pre>
<p>L2正则</p>
<pre><code class="language-python">def L2_penalty(w, b):
    return ((w**2).sum() + b**2) / 2

</code></pre>
<p><strong>gluon版本</strong></p>
<pre><code>    trainer = gluon.Trainer(net.collect_params(), 'sgd', {
        'learning_rate': learning_rate, 'wd': weight_decay})

</code></pre>
<p>这里的weight_decay表明这里添加了L2正则，正则化<br>
w = w -lr * grad - wd * w</p>
<h2 id="训练过程">训练过程</h2>
<p><strong>scratch版本</strong></p>
<pre><code class="language-python">    for e in range(epochs):        
        for data, label in data_iter(num_train):
            with autograd.record():
                output = net(data, lambd, *params)
                loss = square_loss(
                    output, label) + lambd * L2_penalty(*params)
            loss.backward()
            SGD(params, learning_rate)
        train_loss.append(test(params, X_train, y_train))
        test_loss.append(test(params, X_test, y_test))

</code></pre>
<p><strong>gluon版本</strong></p>
<pre><code class="language-python">    for e in range(epochs):        
        for data, label in data_iter_train:
            with autograd.record():
                output = net(data)
                loss = square_loss(output, label)
            loss.backward()
            trainer.step(batch_size)            
        train_loss.append(test(net, X_train, y_train))
        test_loss.append(test(net, X_test, y_test))


</code></pre>
<pre><code class="language-python">%matplotlib inline
import matplotlib as mpl
mpl.rcParams['figure.dpi']= 120
import matplotlib.pyplot as plt

def train(X_train, X_test, y_train, y_test):
    # 线性回归模型
    net = gluon.nn.Sequential()
    with net.name_scope():
        net.add(gluon.nn.Dense(1))
    net.initialize()
    # 设一些默认参数
    learning_rate = 0.01
    epochs = 100
    batch_size = min(10, y_train.shape[0])
    dataset_train = gluon.data.ArrayDataset(X_train, y_train)
    data_iter_train = gluon.data.DataLoader(
        dataset_train, batch_size, shuffle=True)
    # 默认SGD和均方误差
    trainer = gluon.Trainer(net.collect_params(), 'sgd', {
        'learning_rate': learning_rate})
    square_loss = gluon.loss.L2Loss()
    # 保存训练和测试损失
    train_loss = []
    test_loss = []
    for e in range(epochs):
        for data, label in data_iter_train:
            with autograd.record():
                output = net(data)
                loss = square_loss(output, label)
            loss.backward()
            trainer.step(batch_size)
        train_loss.append(square_loss(
            net(X_train), y_train).mean().asscalar())
        test_loss.append(square_loss(
            net(X_test), y_test).mean().asscalar())
    # 打印结果
    plt.plot(train_loss)
    plt.plot(test_loss)
    plt.legend(['train','test'])
    plt.show()
    return ('learned weight', net[0].weight.data(),
            'learned bias', net[0].bias.data())

</code></pre>
<p>最终版</p>
<pre><code class="language-python">def train(train_data, test_data, net, loss, trainer, ctx, num_epochs, print_batches=None):
    &quot;&quot;&quot;Train a network&quot;&quot;&quot;
    print(&quot;Start training on &quot;, ctx)
    if isinstance(ctx, mx.Context):
        ctx = [ctx]
    for epoch in range(num_epochs):
        train_loss, train_acc, n, m = 0.0, 0.0, 0.0, 0.0
        if isinstance(train_data, mx.io.MXDataIter):
            train_data.reset()
        start = time()
        for i, batch in enumerate(train_data):
            data, label, batch_size = _get_batch(batch, ctx)
            losses = []
            with autograd.record():
                outputs = [net(X) for X in data]
                losses = [loss(yhat, y) for yhat, y in zip(outputs, label)]
            for l in losses:
                l.backward()
            train_acc += sum([(yhat.argmax(axis=1)==y).sum().asscalar()
                              for yhat, y in zip(outputs, label)])
            train_loss += sum([l.sum().asscalar() for l in losses])
            trainer.step(batch_size)
            n += batch_size
            m += sum([y.size for y in label])
            if print_batches and (i+1) % print_batches == 0:
                print(&quot;Batch %d. Loss: %f, Train acc %f&quot; % (
                    n, train_loss/n, train_acc/m
                ))

        test_acc = evaluate_accuracy(test_data, net, ctx)
        print(&quot;Epoch %d. Loss: %.3f, Train acc %.2f, Test acc %.2f, Time %.1f sec&quot; % (
            epoch, train_loss/n, train_acc/m, test_acc, time() - start
        ))

</code></pre>
<h1 id="reference">reference</h1>
<p><a href="https://zhuanlan.zhihu.com/p/28867241">从零开始码一个皮卡丘检测器</a></p>
<p><a href="http://blog.csdn.net/jesse_mx/article/details/53606897">图片标注工具</a></p>
<p><a href="http://blog.csdn.net/u014696921/article/details/56877979"> mxnet 使用自己的图片数据训练CNN模型</a></p>
<p><a href="https://mxnet.incubator.apache.org/api/python/image.html#Image">mxnet image API</a></p>
<p><a href="https://mxnet.incubator.apache.org/how_to/recordio.html?highlight=recordio">Create a Dataset Using RecordIO</a></p>
<p><a href="http://blog.csdn.net/muyouhang/article/details/77727381">基于MXNet gluon 的SSD模型训练</a></p>
<p><a href="https://groups.google.com/a/continuum.io/forum/m/#!topic/anaconda/RuSpZVPEio8">解决conda与ipython notebook的python版本问题</a></p>
<p><a href="http://blog.csdn.net/sunshine_in_moon/article/details/51434908">神经网络计算参数量的方法</a></p>
<p><a href="https://www.jianshu.com/p/c56a37093cfa">神经网络计算特征图的大小的方法</a></p>
<p><a href="http://minibatch.net/2017/06/11/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87-Batch-Normalization/">BN应该放在relu后</a></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[c++ 的关键字回顾]]></title>
        <id>https://DragonFive.github.io/post/cpp-keyword/</id>
        <link href="https://DragonFive.github.io/post/cpp-keyword/">
        </link>
        <updated>2018-03-03T04:15:07.000Z</updated>
        <content type="html"><![CDATA[<hr>
<p>Title: c++ 的关键字回顾<br>
date: 2018/3/03 17:38:58<br>
categories:</p>
<ul>
<li>编程<br>
tags:</li>
<li>cpp</li>
<li>关键字</li>
</ul>
<hr>
<p>2013年的时候写了一些学c++时候的经验<a href="https://blog.csdn.net/zhzz2012/article/details/46346115">由底层和逻辑深入剖析c++系列</a>，<br>
两年前写了一篇<a href="https://zhuanlan.zhihu.com/p/21930436">c++11新特性详解</a>，<br>
去年总结了一些c++的一些使用技巧 <a href="https://mxl.bitcron.com/post/engineering/cpp_new_feature">c++使用7年后的经验总结</a>。<br>
现在看来需要经常复习使用，才能更好地掌握c++这门编程语言。</p>
<h1 id="关键字">关键字</h1>
<h2 id="static与extern-c">static与extern &quot;C&quot;</h2>
<p><strong>区别</strong><br>
这两个是不能同时使用的一对词。</p>
<ol>
<li>
<p>static 修饰的名字只能在当前模块中使用，且不能被extern所修饰</p>
</li>
<li>
<p>extern &quot;C&quot; 表示修饰的名字来自其它模块，且要按C语言的方式进行编译和链接<br>
<strong>extern &quot;C&quot;从问题中学习</strong></p>
</li>
<li>
<p>既然c++是C的超集，为什么还要用C语言的方式编译和链接？<br>
&gt; 因为可能编写c语言的人给你的不是源代码。而是编译之后的.so文件</p>
</li>
<li>
<p>extern &quot;C&quot; 与 extern 是什么关系</p>
<blockquote>
<p>extern &quot;C&quot;包含双重含义，从字面上可以知道，首先，被它修饰的目标是&quot;extern&quot;的；其次，被它修饰的目标代码是&quot;C&quot;的。extern 告诉编译器，其申明的函数和变量可以在本模块或其他模块中使用。</p>
</blockquote>
</li>
<li>
<p>什么是按C语言的方式进行编译和链接</p>
<blockquote>
<p>函数被C++编译后在符号库中的名字是与C语言不同的；C++编译后的函数需要加上参数的类型才能唯一标定重载后的函数，而加上extern &quot;C&quot;后，是为了向编译器指明这段代码按照C语言的方式进行编译和链接。比如对于<code>int foo(int x, int y)</code>C++会编译为类似<code>__foo_int_int_</code>的形式，而c语言则会编译为<code>__foo__</code>的形式。</p>
</blockquote>
</li>
<li>
<p>在c语言中想要C++类里面的东西怎么办</p>
<blockquote>
<p><a href="https://www.cnblogs.com/Yogurshine/p/3913073.html">C代码中如何调用C++ C++中如何调用C</a><br>
<a href="https://blog.csdn.net/caspiansea/article/details/9676153">如何用C语言封装 C++的类，在 C里面使用</a></p>
</blockquote>
</li>
</ol>
<p><strong>extern &quot;C&quot;从例子中学</strong><br>
1.修饰单个句子</p>
<pre><code class="language-CPP">extern &quot;C&quot; double sqrt(double);
</code></pre>
<ol start="2">
<li>修饰复合句子</li>
</ol>
<pre><code class="language-C">extern &quot;C&quot;
 {
      double sqrt(double);
      int min(int, int);
  }
</code></pre>
<p>3.包含include头文件，相当于头文件中的声明都加了 extern &quot;C&quot;</p>
<pre><code class="language-C">extern &quot;C&quot;
{
     #include &lt;cmath&gt;
}
</code></pre>
<p>4.在C语言的一些标准头文件里经常有这样的表示</p>
<pre><code class="language-CPP">#ifdef  __cplusplus
extern &quot;C&quot; {
#endif
……// (C函数声明)
#ifdef  __cplusplus
}
#endif
</code></pre>
<p><strong>参考资料</strong><br>
<a href="https://blog.csdn.net/jiqiren007/article/details/5933599">extern C的作用详解</a></p>
<h2 id="const关键字">const关键字</h2>
<p><strong>常量指针与指针常量</strong></p>
<ol>
<li>区分方法：从右向左读 <code>char * const A</code>, A是一个不可变的指针，指向的是char数据，<code>char const * B</code> , 表示B是一个指针，这个指针指向char常量；</li>
<li>对于常量指针，不能通过该指针来改变所指的内容(可以通过其它方式修改)<br>
<strong>常量对象与常量成员函数</strong><br>
一个类的常量对象只能调用该类的常量成员函数，因为常量成员函数不能修改对象的成员变量，当然可以在一个成员变量前加上mutable关键字，这样常量成员函数就能修改它了。 <code>void func() const</code></li>
</ol>
<p><strong>常量成员变量与类常量</strong><br>
常量成员变量是说，它是属于对象的不可变的变量，所以初始化只能在构造函数的初始化列表里。const数据成员只在某个对象生存期内是常量，而对于整个类而言却是可变的。因为类可以创建多个对象，不同的对象其const数据成员的值可以不同。所以不能在类声明中初始化const数据成员，因为类的对象未被创建时，编译器不知道const 数据成员的值是什么。</p>
<p>要想建立在整个类中都恒定的常量，应该用类中的枚举常量来实现。</p>
<pre><code class="language-CPP">class A
{
 enum {size1=100, size2 = 200 };
 int array1[size1];
 int array2[size2];
}
</code></pre>
<p><strong>const修饰函数返回值</strong><br>
一般用const修饰返回值为对象本身（非引用和指针）的情况多用于二目操作符重载函数并产生新对象的时候。 防止产生的临时对象被赋值。<br>
比如两个复数的乘法</p>
<pre><code class="language-CPP">const Rational operator*(const Rational&amp; lhs, const Rational&amp; rhs) 
{ 
 return Rational(lhs.numerator() * rhs.numerator(), 
 lhs.denominator() * rhs.denominator()); 
}
</code></pre>
<p>这样做可以预防出现<code>(a*b) = c</code>的情况。<br>
<strong>引用传递的返回值不要用const修饰</strong><br>
在类的本地操作符（=，&lt;&lt;等）重载函数中，函数返回值常采用“引用传递”，目的是为了实现链式表达。</p>
<pre><code class="language-CPP">class A
{
 A &amp;operate = (const A &amp;other);  //赋值函数
}
A a,b,c;              //a,b,c为A的对象
a=b=c;            //正常
(a=b)=c;          //不正常，但是合法
</code></pre>
<p>若赋值函数的返回值加const修饰，那么该返回值的内容不允许修改。所以一般赋值函数都不会这样设置。<br>
<strong>C语言与CPP中const的区别</strong></p>
<ol>
<li>C++中的const正常情况下是看成编译期的常量,编译器并不为const分配空间,只是在编译的时候将期值保存在名字表中,并在适当的时候折合在代码中，而c语言认为是不变的变量，在编译期不知道值。</li>
</ol>
<p>C++中,是否为const分配空间要看具体情况.如果加上关键字extern或者取const变量地址,则编译器就要为const分配存储空间，下面的代码在c++中会通过，而c语言不会通过</p>
<pre><code class="language-CPP">const int a=10;
int b[a];
</code></pre>
<p>2.C++中,const默认使用内部连接，定义时必须初始化(类中的成员变量除外)，或者使用extern修饰. 而C中使用外部连接，可以只声明不初始化<code>const int size;</code></p>
<p><strong>顶层const与底层const</strong><br>
是用const修饰时，如果修饰的是定义的变量就是顶层const，如果修饰的是定义的变量指向的对象那就是底层const。<br>
值得注意的是顶层const在初始化和赋值的时候，等号左右两边的对象是否const并无影响。<br>
而底层const赋值的时候，可以把非常量赋值给指向常量对象的地址，却不可以把常量初始化给指向非常量对象的地址。</p>
<p><strong>constexpr变量</strong><br>
c++11标准规定，允许将变量声明为constexpr类型，以便由编译器来验证变量的值是否是一个常量表达式</p>
<p><strong>使用const的建议</strong></p>
<ol>
<li>要大胆的使用const，这将给你带来无尽的益处，但前提是你必须搞清楚原委；</li>
<li>在参数中使用const应该使用引用或指针，而不是一般的对象实例；</li>
<li>不要轻易的将函数的返回值类型定为const;</li>
<li>除了重载操作符外一般不要将返回值类型定为对某个对象的const引用;</li>
</ol>
<blockquote>
<p>非 const 变量默认为 extern。要使 const 变量能够在其他的文件中访问，必须地指定它为 extern——《cpp primer》</p>
</blockquote>
<p><strong>参考资料</strong><br>
<a href="https://www.cnblogs.com/yc_sunniwell/archive/2010/07/14/1777416.html">c/c++中的const关键字</a><br>
《c++ primer 第五版》</p>
]]></content>
    </entry>
</feed>