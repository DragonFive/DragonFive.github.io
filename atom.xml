<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://DragonFive.github.io/</id>
    <title>dragon</title>
    <updated>2021-07-03T09:02:45.950Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://DragonFive.github.io/"/>
    <link rel="self" href="https://DragonFive.github.io/atom.xml"/>
    <subtitle>Code is cheap, show me the theory</subtitle>
    <logo>https://DragonFive.github.io/images/avatar.png</logo>
    <icon>https://DragonFive.github.io/favicon.ico</icon>
    <rights>All rights reserved 2021, dragon</rights>
    <entry>
        <title type="html"><![CDATA[华为的《ScaleFreeCTR:a MixCache-based distributed training system for CTR》]]></title>
        <id>https://DragonFive.github.io/post/hua-wei-de-scalefreectr/</id>
        <link href="https://DragonFive.github.io/post/hua-wei-de-scalefreectr/">
        </link>
        <updated>2021-06-15T12:23:03.000Z</updated>
        <content type="html"><![CDATA[<p>华为诺亚方舟提出了SFCTR <a href="https://arxiv.org/pdf/2104.08542.pdf">ScaleFreeCTR: a MixCache-based distributed training system for CTR</a>，scalefree 可能是因为数据规模对训练吞吐没有影响，后面实验部分有具体的数据。</p>
<h1 id="一-动机与主要创新">一、动机与主要创新</h1>
<p>现有的分布式CTR训练框架使用CPU内存来保存和更新参数，使用gpu 进行前向和反向计算（也有用CPU的），会有两个瓶颈</p>
<ol>
<li>
<p>CPU 和 gpu 之间的pull 和 push 操作有一定的延迟</p>
</li>
<li>
<p>cpu 进行参数的同步和更新比较慢</p>
</li>
</ol>
<p>分布式训练的关键在于</p>
<ol>
<li>关键在于减少host_gpu之间的延迟</li>
<li>减少host-gpu 及gpu之间的数据传输量也很重要</li>
</ol>
<p>推荐中的参数有两个特点：</p>
<ol>
<li>实际 working parameters 比较少，sparse 参数和 MLP参数都很少</li>
<li>sparse 特征符合幂律分布，小部分特征被高频访问</li>
</ol>
<p>根据两个特点，可以有两个方法</p>
<ol>
<li>使用缓存机制减少 host-gpu 延迟</li>
<li>通过重组batch数据来减少参数传输量(unique?)</li>
</ol>
<p>由此提出了 SFCTR:<br>
在CPU中通过 虚拟sparse id op 来减少host-gpu 和gpu-gpu 的数据传输量，使用 mixcache 实验特征预取来减少传输延迟，使用3级pipeline 来减少整体训练时长。</p>
<p>系统将会在MindSpore 上开源，现在看似乎还没有开源。</p>
<h1 id="二-相关工作">二、相关工作</h1>
<h2 id="一论文介绍的跟sfctr无关但挺有用的经验知识">（一）论文介绍的跟SFCTR无关，但挺有用的经验知识</h2>
<p>为了提高训练效率，有两种通用的做法：</p>
<ol>
<li>增量学习（batch训练的补充，用最近的数据更新模型）</li>
<li>分布式训练（使用额外的训练资源）</li>
</ol>
<p>CTR模型稀疏部分参数量太大，所以不能使用reduce 数据并行，大多数考虑用了模型并行。<br>
模型并行解决方案ps 架构的局限性：<br>
Ps server 保存并同步参数，worker执行前向和反向计算，</p>
<ul>
<li>worker pull and push from ps</li>
<li>Ps 从worker接收梯度之后进行同步<br>
分布式训练包括两个阶段：计算和参数同步。</li>
</ul>
<p>百度的综述 <a href="https://arxiv.org/abs/2003.05622">Distributed Hierarchical GPU Parameter Server for Massive Scale Deep Learning Ads Systems</a> 介绍了三种同步模式：</p>
<ul>
<li>BSP(bulk sync parallel)，严格对所有worker的更新进行同步</li>
<li>SSP(stale sync parallel)，对快worker 进行同步</li>
<li>ASP（async parallel）, 不同步gradient</li>
</ul>
<p>后两种方式虽然提升了训练效率，但是降低了模型性能。SFCTR 使用的是BSP，XDL使用的是ASP。</p>
<h1 id="三-sfctr-架构">三、SFCTR 架构</h1>
<p>SFCTR 由三部分构成</p>
<ul>
<li>Data-Loader, 提出虚拟sparse id op 来减少batch中重复的特征emb(unique?)</li>
<li>Host-Manager, 使用混合缓存策略来减少host-gpu延迟，MixCache 的管理器部分在 CPU 的内存中，MixCache 的缓冲区在 GPU 的 HBM 中</li>
<li>GPU-WORKER<br>
<img src="https://DragonFive.github.io//post-images/1625142600332.png" alt="" loading="lazy"></li>
</ul>
<p><strong>3级pipeline</strong><br>
把 Data-Loader,Host-Manager 和gpu worker 中，三阶段资源不同 Disk, CPU and GPU在三个不同的线程里完成</p>
<figure data-type="image" tabindex="1"><img src="https://DragonFive.github.io//post-images/1625142730082.png" alt="" loading="lazy"></figure>
<h2 id="1data-loader">（1）data loader</h2>
<p>sparse id op 来减少batch中重复的特征emb，就是xdl中的unique。减少host-gpu 和gpu-gpu 的数据传输量。</p>
<h2 id="2host-manager">（2）HOST-MANAGER</h2>
<p>MixCache用来减少延迟，在每个GPU的HBM 上申请一个cache buffer，使用modulo 哈希方法对 working parameters 进行分组，放在不同的GPU 上，embedding参数在data loader执行完VSI op 之后检查哪些参数 gpu 上没有就把那些参数传输到gpu 上。</p>
<p>当cache满了之后，满足两种情况的emb 会回传到host。</p>
<ol>
<li>参数完成了更新</li>
<li>下个batch不需要这个参数</li>
</ol>
<h2 id="3gpu-worker">（3）GPU-WORKER</h2>
<p>不同的GPU保存了不同的参数，所以前向和反向的时候都需要同步。<br>
前向传播，每个worekr从其它worker拿到batch所需要的参数，使用all-reduce 通信方式，一个gpu只需要跟另外两个gpu通信两次，首先通过gather_cache ，从cache buffer 中获得local common emb, 因为global_id 顺序一致，所以可以做all_reduce同步, 然后通过all_reduce 或者 global common emb，最后通过vis 算出来自己worker需要执行的batch emb<br>
<img src="https://DragonFive.github.io//post-images/1625142803471.png" alt="" loading="lazy"></p>
<p>梯度更新<br>
<img src="https://DragonFive.github.io//post-images/1625142812224.png" alt="" loading="lazy"></p>
<h1 id="四-sfctr-执行流程">四、SFCTR 执行流程</h1>
<p>执行流程<br>
<img src="https://DragonFive.github.io//post-images/1625142836322.png" alt="" loading="lazy"></p>
<ul>
<li>
<p>2-3行是 data loader 部分，有个虚拟Sparse Id OP，对batch Sparse ID 去重后形成 global_id，对于batch 中每个实例有个virtual_id，可以找到其对应的global_id ，跟XDL 的unique 操作很像。使用global_id, 各个gpu在同步的时候数据量就会少很多。</p>
</li>
<li>
<p>4-7行是 Host-Manager 部分，负责在主存中报错embedding参数（存得下吗？），使用mixcache把working parameters 放到 gpu 的cache buffer中。mixcache 还更新gpu cache buffer， 检查下一个batch需要哪些embedding ，预测哪些embedding未来一段时间不需要，在buffer满的时候进行pull, 发送数据到gpu，并对每个特征在gpu设置一个local_id (?)</p>
</li>
<li>
<p>9-15行是GPU部分，包括embedding查表，前向反向和参数更新</p>
</li>
</ul>
<p>host 和 gpu 是生产者与消费者模式</p>
<h1 id="五-一些对我们有用的实验">五、一些对我们有用的实验：</h1>
<h2 id="1环境">（1）环境</h2>
<p>GPU 集群使用 InfiniBand 连接，4台GPU服务器通过100Gb  RDMA提速</p>
<p>Intel Xeon Gold-5118 CPUs with 18 cores (36 threads), 8 Tesla V100 GPUs with 32 GB HBM，1GB内存。GPU之间PCI连接<br>
使用 Criteo-TB 数据库，使用filter构造10GB和100GB两个数据集，因为包括了优化器的信息，33×4B×80=10GB，所以实际parameter table是embedding table的3倍，所以实际上是 30GB和300GB的模型参数。</p>
<p>使用 DeepFM 模型，与 hugectr与ps mxnet对比</p>
<h2 id="2框架对比实验">（2）框架对比实验</h2>
<p>基于VSI OP，混合缓存机制，和三级pipeline，在10GB数据上SFCTR 在4机32卡上的吞吐量是psmxnet的1.6倍，hugectr的5.6倍，100GB 数据上是1.3倍和6.9倍<br>
如果GPU卡只有8个，hugectr在100GB数据上根本无法训练</p>
<figure data-type="image" tabindex="2"><img src="https://DragonFive.github.io//post-images/1625143001706.png" alt="" loading="lazy"></figure>
<h2 id="3vsi-op">（3）vsi op</h2>
<p>Host-gpu 数据传输量减少 94% ，g pu-gpu数据量减少88%</p>
<figure data-type="image" tabindex="3"><img src="https://DragonFive.github.io//post-images/1625143009606.png" alt="" loading="lazy"></figure>
<h2 id="4mixcache">（4）mixcache</h2>
<p>Cache 大小对传数据的影响<br>
2GB的cache可以把数据传输推迟到1000步之后<br>
如果cache 大小比较大，batch中要传输的数据的比例就会小，因为可以存更多高频特征<br>
12%（2GB）, 27%（0.5GB） and 29%（0.25GB）</p>
<figure data-type="image" tabindex="4"><img src="https://DragonFive.github.io//post-images/1625143020154.png" alt="" loading="lazy"></figure>
<h2 id="53级pipeline">（5）3级pipeline</h2>
<p>GPU-Worker 训练时间在pipeline中占比最高<br>
一个节点跑100GB数据，使用pipeline需要 75 s，不用pipeline就需要150s</p>
<h1 id="六-总结与思考">六、总结与思考</h1>
<p>文章写的通俗易懂，很有条理，related worker 也总结了很多训练的经验，工作很有实用性。<br>
作者提到未来的工作有两个方向</p>
<ol>
<li>提升通信效率，（使用all2all）</li>
<li>调查提升收敛速度的方法</li>
</ol>
<p>思考借鉴意义</p>
<ol>
<li>
<p>SFCTR相当于把图完全放在GPU中执行，没有进行图的分隔，所以实现起来更容易一些。CPU只是一个ps的存储和更新后落盘以及dataloader</p>
</li>
<li>
<p>CPU内存1T，而实验中的数据最大的为300GB，所以可以放在CPU内存中，其实我们的模型大小似乎也在几百GB，如果可以放在worker内存中，就没有必要单独弄一个ps server；<br>
如果模型超过1T，也可以融合AIBox的做法，使用SSD做cpu mem的缓存</p>
</li>
<li>
<p>pipeline 和 vsiop 其实 XDL 都有，只缺了缓存机制，但XDL如果不动ps这一块，参数的更新其实是在ps 上完成的，所以 ps 的 push 也会继续有延迟，参数预取只能解决pull 的问题，</p>
</li>
<li>
<p>但如果都在本地更新，那不同worker之间参数同步就会比较麻烦，所以缓存预取、更新后缓存失效再回传的机制必然依赖多机间 RDMA  单机allreduce 同步通信技术，ps存在的意义不大</p>
</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[业界CTR深度学习框架的一些新的进展 ]]></title>
        <id>https://DragonFive.github.io/post/ye-jie-ctr-shen-du-xue-xi-kuang-jia-de-yi-xie-xin-de-jin-zhan/</id>
        <link href="https://DragonFive.github.io/post/ye-jie-ctr-shen-du-xue-xi-kuang-jia-de-yi-xie-xin-de-jin-zhan/">
        </link>
        <updated>2021-05-31T13:03:49.000Z</updated>
        <content type="html"><![CDATA[<p>为了充分利用GPU的能力和高速带宽<br>
英伟达的 hugeCtr https://github.com/NVIDIA/HugeCTR 和 脸书 的 DLRM<br>
<a href="https://arxiv.org/abs/1906.00091">【CoRR2019】Deep Learning Recommendation Model for Personalization and Recommendation Systems</a><br>
把emb参数分成不同的份放在GPU HMB中，需要需要昂贵的GPU，不实用。</p>
<p>腾讯的DES<br>
<a href="https://arxiv.org/abs/1909.04823">Distributed Equivalent Substitution Training for Large-Scale Recommender Systems</a><br>
和 百度的 HierPs<br>
<a href="https://arxiv.org/abs/2003.05622">Distributed Hierarchical GPU Parameter Server for Massive Scale Deep Learning Ads System</a><br>
使用主存保存emb ，DES采用 field-aware 分片策略来reduce(减少or规约)GPU间的数据通信，但没有进行主存和GPU之间通信优化。<br>
HierPS使用大batch策略来在gpu中缓存使用大参数，以此减少传输延时。</p>
<p>Tensorflow, MxNet 和 PyTorch 并不能很好的支持大规模embedding的训练:</p>
<ul>
<li>PyTorch 中没有官方支持的ps</li>
<li>Mxnet 支持的模型大小因为实现问题受到了限制</li>
<li>tensorflow 使用它的ps后吞吐会严重下降</li>
</ul>
<p>为了提升tensorflow, mxnet, pytorch较差的分布式性能，uber 的horovod<br>
<a href="https://arxiv.org/abs/1802.05799">Horovod: fast and easy distributed deeplearninginTensorFlow</a><br>
和字节的byteps<br>
<a href="https://arxiv.org/abs/1807.00311">Product-based Neural Networks for User Response Prediction over Multi-field Categorical Data</a><br>
都支持不同的平台:</p>
<ul>
<li>horovod 使用Ring-AllReduce实现来加速dense模型的训练</li>
<li>byteps 通过调度优先来在不同的层加速同步参数，优化不同层的顺序来在反向传播和前向计算的时候同步参数</li>
</ul>
<p>之前写过一篇关于 horovod 的知识总结：<a href="https://dragonfive.github.io/post/uber-de-horovod/">uber的Horovod | dragon</a></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[百度的《AIBox: CTR Prediction Model Training on a Single Node》]]></title>
        <id>https://DragonFive.github.io/post/bai-du-de-lesslessaibox-ctr-prediction-model-training-on-a-single-nodegreatergreater/</id>
        <link href="https://DragonFive.github.io/post/bai-du-de-lesslessaibox-ctr-prediction-model-training-on-a-single-nodegreatergreater/">
        </link>
        <updated>2021-03-11T12:43:07.000Z</updated>
        <content type="html"><![CDATA[<p>AIBox 是百度提出的训练框架，论文 AIBox: CTR Prediction Model Training on a Single Node 进行了相关介绍。</p>
<h1 id="一-aibox-的技术创新及优势">一、AIBox 的技术创新及优势</h1>
<p>创新点与动机：<br>
AIBox 的核心想法就是想在一台机器上用GPU加速训练，但是参数实在太大了，所以就把计算密集的模型运算部分（joint learning）放在GPU完成，把取embedding 部分放在cpu部分完成(embedding learning)，这就是AiBox 的第一个创新：把网络切分为两部分。</p>
<p>但即便主存用了1TB 的内存，embedding 还是太大了，10^12 个key，每个key 的 weight 即使用8个字节存，key用8个字节存，也需要1.6TB , 所以论文提出了第二个创新：把embedding 存在SSD上，同时为了降低延迟和减少写操作对ssd寿命的影响，建立了二级缓存机制。</p>
<p>为了提高速度，AIBOX使用了流水线，把从hdfs 读数据(socket IO)，从SSD查Embedding （SSD io） 和 cpu+gpu 计算组成3阶段的 pipeline</p>
<p>优势：<br>
AIBOX不存在像分布式系统普遍存在的网络通信开销问题，然后在系统稳定性方面AIBOX与具有数千台机器的分布式集群相比更加稳定不会轻易宕机，而且在同步开销方面AIBOX只是涉及到一些内存锁和GPU片之间的少量通信。</p>
<h1 id="二-关于网络结构切分">二、关于网络结构切分</h1>
<p>the first module focuses on the embedding learning with high-dimensional &amp; sparse features and the second module is for joint learning with dense features resulted from the first module.</p>
<p>The embedding learning is processed on CPUs to help learn low dimensional dense embedding representations.</p>
<p>By transferring the learned embedding vectors from CPUs to GPUs, the computation-intensive joint learning module can make full use of the powerful GPUs for CTR prediction.</p>
<p>CPU 部分把数据从稀疏特征转化成 embedding （embedding learning），然后把embedding 传到 GPU，GPU进行一轮训练 (joint learning)</p>
<p>论文这部分讲了一些网络的设计细节，但这块感觉跟 AIBox本身没什么关系，论文写到：</p>
<p>把第一隐含层和最后一层隐含层的结果合并起来，第一层包含了low-level 的与输入信息最相关的feature，最后一层包含了high-level 的最抽象和有用的信息。这样会得到更准确的CTR预估结果。</p>
<p>训练两阶段(cpu+gpu)，梯度更新也是两阶段(gpu+cpu)。</p>
<h1 id="三-aibox-架构划分">三、AIBox 架构划分</h1>
<p><strong>架构：</strong><br>
分为三部分：CPU、GPU和 sparse table buff</p>
<ul>
<li>
<p>cpu模块：协调调度和embedding学习<br>
从hdfs读数据(一个pass)，向Sparse Table模块查embedding，然后发给GPU<br>
拿到gpu传来的梯度，更新sparse table<br>
定期save ckpt 到 hdfs</p>
</li>
<li>
<p>sparse table：把10^12 的离散特征的数据存储到ssd上的kv系统里<br>
内存中的key hash 索引存了特征到文件的映射关系，<br>
in-memory cache strategy 构造cache 和 buffer 来减少延迟</p>
</li>
<li>
<p>gpu模块：联合学习<br>
cpu传来的 embedding 被放入 HBMs 中，然后被fed 给 dense 联合学习网络<br>
emb通过pci-e总线进行传输<br>
一个CUDA stream进行数据传输，另一个cuda stream 进行学习计算<br>
HBMs如同片上ps一样工作，<br>
每个pass 在每个gpu 上计算新的参数，各gpu通过NVLink进行同步</p>
</li>
</ul>
<p><strong>3阶段流水线：network, SSDs and CPUs + GPUs</strong></p>
<figure data-type="image" tabindex="1"><img src="https://DragonFive.github.io//post-images/1625143570973.png" alt="" loading="lazy"></figure>
<figure data-type="image" tabindex="2"><img src="https://DragonFive.github.io//post-images/1625143575338.png" alt="" loading="lazy"></figure>
<h1 id="四-sparsetable-架构">四、sparseTable 架构</h1>
<p>由两部分构成：key hash index and bi-level cache management</p>
<h2 id="一key-hash-index">（一）key hash index</h2>
<p>Key Hash Index 存的是 10^12 个 feature key 到 ssd 文件的映射关系，直接每个key 存一个文件需要1.6TB大小的内存，放不下。</p>
<p>通过对key 取模进行分组建立 group 与 file 的对应关系，放在内存中。</p>
<p>group(key) → key mod 1012/m. We set m = ⌊BLOCK/(8 + sizeof(value))⌋,其中Block 是每次从ssd取数据的最小单元。</p>
<p>hash函数可以通过预训练一个模型来最大化feature 共现，把共现的feature 分到同样的桶里面。</p>
<h2 id="二二级缓存机制">（二）二级缓存机制</h2>
<p>ssd 的延迟是内存的1000倍，ssd是微秒级别延迟，内存是纳秒级别延迟</p>
<p>在一个 pass of mini batch 中只有1%的参数会被用到，所以我们可以用in-memory cache 来存储高频访问的hot parameters</p>
<p>SSD有物理性能限制：每个存储单元只能被写入（擦除）数千次，cache机制可以作为参数缓存，来减少更新参数对SSD使用寿命的影响</p>
<p>使用两个分离的链表进行拉链来提升探测性能。对每个ssd文件使用Bloom filter来减少不必要的读取。</p>
<p><strong>第一级缓存</strong></p>
<p>使用 si =hash1(g_id) 来算出一个 cache slot 槽，对应一个ssd 文件，对于参数并未进行真正初始化，而是在第一次访问到参数的时候，先用 bloom filter 探测key 是否在 slot 集合里，如果不在就不用读取这个文件，而是直接使用默认值，以此来减少不必要的ssd读取。</p>
<p><strong>二级缓存</strong></p>
<p>hash2(g_id, bucket)</p>
<p>对 一级的槽进行分桶bucket，来使得拉的链比较短。bucket 参数通过调节可以权衡空间和探测效率</p>
<p><strong>两条拉链</strong></p>
<ul>
<li>LRU 链用于保存最近访问过的key，以此来减少探测次数</li>
<li>LFU链按访问频次来保存key，用于缓存管理，只有当LFU满了需要删除低频key时，相应的数据才会写回到ssd上面</li>
</ul>
<p>由于经常有链条中的节点进行增删，所以使用线程池以Slab memory allocation mechanism机制 进行管理。</p>
<figure data-type="image" tabindex="3"><img src="https://DragonFive.github.io//post-images/1625143705399.png" alt="" loading="lazy"></figure>
<p><strong>文件管理系统</strong><br>
batch产生的小文件对于先有的文件系统有很大的压力，把许多小文件组成一个组来创建较少的文件。小文件的名字由大文件的名字加上offset构成，保存在第一级cache slot 中</p>
<p>监控文件系统的大小，合并访问量少的小问题，当model_size 达到最大冗余度的时候删掉访问少的文件，MAX_REPLICATION=SSD capacity ∗ (85% + overprovisioning)/model size<br>
<img src="https://DragonFive.github.io//post-images/1625143710125.png" alt="" loading="lazy"></p>
<h1 id="五-实验部分">五、实验部分</h1>
<p><strong>实验</strong><br>
AIBox 8 个GPU， 服务器级别的cpu, 1T 内存，Raid 0 nvme ssd<br>
MPI集群方式用75个计算节点</p>
<ul>
<li>
<p>AIBox 的硬件和维护费用比集群训练方式少 10%，执行时间多25%</p>
</li>
<li>
<p>AIBox 的auc 比集群方式稍好，可能是因为AIBox 这种单节点的方式，同步参数频率更高</p>
</li>
</ul>
<figure data-type="image" tabindex="4"><img src="https://DragonFive.github.io//post-images/1625143749656.png" alt="" loading="lazy"></figure>
<p>六、总结与一些细节问题：<br>
论文介绍了 AIBox 架构的一些细节方面，借助一系列系统设计方案如缓存机制来解决问题，通过这些并不是很fashion的技术合并，论文实现了集中式训练的技术突破，这种通过技术积累然后撬动难题的解决问题的方式值得我们学习。</p>
<p>但是还有一些细节没有讲清楚：</p>
<ol>
<li>
<p>AIBox 有几个worker 进行工作，他们是数据并行，还是使用同样的数据进行训练（文中提到AIBox 会在每个pass of mini batch 进行同步，所以应该不是一个worker 在参与训练）</p>
</li>
<li>
<p>AIBox 使用集中的训练方式，那如果这台机器挂掉，是不是根本没有办法进行恢复，只能另找一个机器从 ckpt 训练</p>
</li>
<li>
<p>文章没有介绍使用的具体计算引擎 (怀疑跟 horovod 接近）</p>
</li>
<li>
<p>同样文章没有介绍参数同步的细节，没有相关 all_reduce 的介绍（可以是使用了一个开源的框架，而这部分论文没有进行改进，所以没有做深入介绍）</p>
</li>
<li>
<p>文章开头提到使用 in-HBM ps 来减少数据传输，但是后面没有详细进行介绍</p>
</li>
</ol>
<p>总体上感觉这篇论文实用性强，但是细节介绍得不多</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[tensorflow2.x 分布式训练]]></title>
        <id>https://DragonFive.github.io/post/tensorflow2x-fen-bu-shi-xun-lian/</id>
        <link href="https://DragonFive.github.io/post/tensorflow2x-fen-bu-shi-xun-lian/">
        </link>
        <updated>2020-06-24T07:41:26.000Z</updated>
        <content type="html"><![CDATA[<blockquote>
<p>去年总结了一些 tensorflow 1.x 分布式训练的一些知识（<a href="https://dragonfive.github.io/post/tensorflow-1x-de-fen-bu-shi-xun-lian/">tensorflow 1.x 的分布式训练</a>）。最近总结了一些 tf2.x 的分布式训练相关知识。</p>
</blockquote>
<h1 id="tf2x-分布式训练策略">tf2.x 分布式训练策略</h1>
<p>TensorFlow 的 tf.distribute 模块下包含有一系列分布式训练策略，它们都是基于数据并行模式实现的。有些策略目前还在 experimental 模块下，表示它们是实验性质的策略，未来可能会发生变动。</p>
<p>训练分布式模型，只需要将原先的模型代码置于distribution Strategy的Scope()下，即可。</p>
<pre><code class="language-py">import tensorflow as tf
data_train, _ = tf.keras.datasets.mnist.load_data()
dataset = tf.data.Dataset.from_tensor_slices(data_train) # 该处可接收numpy数组
dataset = dataset.shuffle(buffer_size=60000) # 该处要大于data_train的长度
dataset = dataset.batch(32)
 
mirrored_strategy = tf.distribution.MirroredStrategy()
 
# mirrored_strategy策略: 在每一个gpu上训练一个模型，每次更新时需要汇总所有gpu上的梯度。
with mirrored_strategy.scope():
  model = tf.keras.Sequential([...])
# tf 2.0中，所有的optimizer都在tf.keras.optimizer下
model.compile(optimizer=tf.keras.optimizer.adam(lr=...))， 
              loss = &quot;sparse_categorical_crossentropy&quot;,
              metrics = ['accuracy'])
model.fit(dataset, epoch=5)
</code></pre>
<h1 id="单机多卡训练">单机多卡训练</h1>
<h2 id="mirrored">Mirrored</h2>
<p>MirroredStrategy 是一种单机的同步的分布式训练策略。它支持在一台机器的多个 GPU 之间进行分布式训练，它会在每个 GPU 上创建一个模型副本，模型中的每个变量 (Variables) 都会进行镜像复制并放置到相应的 GPU 上，这些变量被称作镜像变量 (MirroredVariable)。</p>
<p>MirroredStrategy 策略通过 AllReduce 算法使得所有镜像变量在每个 GPU 之间保持同步更新， AllReduce 算法默认使用英伟达的 NcclAllReduce ，也可以通过 cross_device_ops 参数修改为其他的 AllReduce 算法，如 HierarchicalCopyAllReduce 。</p>
<p>MirroredStrategy 策略会自动使用所有能被 TensorFlow 发现的 GPU 来做分布式训练，如果只想使用部分的 GPU 则可以通过 devices 参数来指定。</p>
<p>MirroredStrategy 实例的创建代码如下所示：</p>
<pre><code class="language-py">mirrored_strategy = tf.distribute.MirroredStrategy(
    devices=[&quot;/gpu:0&quot;, &quot;/gpu:1&quot;],
    cross_device_ops=tf.distribute.HierarchicalCopyAllReduce(),
)

</code></pre>
<p>如果 TensorFlow 没有发现 GPU 则默认会退化为使用 CPU 来进行训练。 MirroredStrategy 的典型使用场景为单机多 GPU 。</p>
<p><strong>MirroredStrategy 的步骤如下：</strong></p>
<ul>
<li>
<p>训练开始前，该策略在所有 N 个计算设备上均各复制一份完整的模型；</p>
</li>
<li>
<p>每次训练传入一个批次的数据时，将数据分成 N 份，分别传入 N 个计算设备（即数据并行）；</p>
</li>
<li>
<p>N 个计算设备使用本地变量（镜像变量）分别计算自己所获得的部分数据的梯度；</p>
</li>
<li>
<p>使用分布式计算的 All-reduce 操作，在计算设备间高效交换梯度数据并进行求和，使得最终每个设备都有了所有设备的梯度之和；</p>
</li>
<li>
<p>使用梯度求和的结果更新本地变量（镜像变量）；</p>
</li>
<li>
<p>当所有设备均更新本地变量后，进行下一轮训练（即该并行策略是同步的）。</p>
</li>
</ul>
<h2 id="centralstorage">CentralStorage</h2>
<p>CentralStorageStrategy 也是一种单机的同步的分布式训练策略。但与 MirroredStrategy 策略不同的是，它会将模型的所有变量保存在 CPU 内存上，而不是通过镜像复制的方式保存在每个 GPU 上，所有的计算操作则会在每个 GPU 上以同样的方式执行。</p>
<p>如果机器只有一个 GPU ， 那么所有的变量和计算操作都会放在该 GPU 上。在对 CPU 上的变量进行更新前，该策略会先将所有 GPU 副本的上的变量梯度进行聚合，然后应用到 CPU 变量更新中。</p>
<p>CentralStorageStrategy 实例的创建代码如下所示：</p>
<pre><code class="language-py">central_storage_strategy = tf.distribute.experimental.CentralStorageStrategy()
</code></pre>
<p>CentralStorageStrategy 策略在 CPU 与 GPU 通信代价远低于 GPU 与 GPU 之间的通信代价时，较为适用，基本上很少会有这种情况出现。</p>
<h1 id="多机训练策略">多机训练策略</h1>
<h2 id="multiworkermirroredstrategy">MultiWorkerMirroredStrategy</h2>
<p>MultiWorkerMirroredStrategy 策略因为要涉及到多个 worker 节点之间的通信交互，因此每个 worker 节点需要提前获知集群中各节点配置信息以便在变量更新时使用。</p>
<p>TensorFlow 中定义集群配置信息的标准方式是使用 TF_CONFIG 环境变量来实现的，该环境变量定义了集群中所有节点的配置信息，包括所有 worker 节点的网络地址，当前 worker 节点的索引 (index) 以及当前 worker 节点的角色 (type)。</p>
<p>示例如下:</p>
<pre><code class="language-python">os.environ['TF_CONFIG'] = json.dumps({
    'cluster': {
        'worker': [&quot;localhost:20000&quot;, &quot;localhost:20001&quot;]
    },
    'task': {'type': 'worker', 'index': 0}
})
</code></pre>
<p>TF_CONFIG 由 cluster 和 task 两部分组成：</p>
<p>cluster 说明了整个多机集群的结构和每台机器的网络地址（IP + 端口号）。对于每一台机器，cluster 的值都是相同的；</p>
<p>task 说明了当前机器的角色。例如， {'type': 'worker', 'index': 0} 说明当前机器是 cluster 中的第 0 个 worker（即 localhost:20000 ）。每一台机器的 task 值都需要针对当前主机进行分别的设置。</p>
<p>以上内容设置完成后，在所有的机器上逐个运行训练代码即可。先运行的代码在尚未与其他主机连接时会进入监听状态，待整个集群的连接建立完毕后，所有的机器即会同时开始训练。</p>
<p>MultiWorkerMirroredStrategy 策略与 MirroredStrategy 策略很相似，可以理解为是 MirroredStrategy 策略的多机的同步的分布式训练版本，它也会在每一台机器上创建所有变量的副本。</p>
<p>多个 worker 节点之间使用 AllReduce 算法来保持模型变量的同步更新， TensorFlow 里将这一操作称为 CollectiveOps。 CollectiveOps 会在 TensorFlow 模型运行时自动根据硬件，网络拓扑以及张量的大小来自动选择合适的 AllReduce 算法来进行网络通信以完成变量更新。</p>
<p>MultiWorkerMirroredStrategy 策略目前有两种可供选择的 CollectiveOps 。 一种为 CollectiveCommunication.RING ，它使用 gRPC 作为通信层实现了基于环的 AllReduce 操作。 另一种为 CollectiveCommunication.NCCL， 它使用了英伟达的 NCCL 库来实现 AllReduce 操作。在实际使用中，可以基于自己的运行环境选择合适的 CollectiveOps，或者使用 CollectiveCommunication.AUTO 交由 TensorFlow 运行时自行选择。</p>
<p>MultiWorkerMirroredStrategy 实例的创建代码如下所示:</p>
<pre><code class="language-python">multiworker_strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(
    tf.distribute.experimental.CollectiveCommunication.RING)
</code></pre>
<p>如果所有 worker 节点都不包含 GPU ，则该策略会退化为使用 CPU 在多个 worker 节点间进行分布式训练。如果集群中的 worker 节点数量只有一个则该策略会退化为 MirroredStrategy 策略。</p>
<h2 id="parameterserverstrategy">ParameterServerStrategy</h2>
<p>ParameterServerStrategy 是一种多机的异步的分布式训练策略。所以它也需要提前指定 TF_CONFIG 环境变量信息，与 MultiWorkerMirroredStrategy 策略不同的是集群中的节点不全是 worker ，有一部分节点会被指定为 ps 用来存储变量信息。模型的每一个变量都会存储在一个 ps 节点上，所有的计算操作会在所有的 worker 节点上以同样的方式执行。 ParameterServerStrategy 实例的创建代码如下所示：</p>
<pre><code class="language-python">ps_strategy = tf.distribute.experimental.ParameterServerStrategy()
</code></pre>
<h1 id="分布式集群定义">分布式集群定义</h1>
<p>一个典型的 TF_CONFIG 环境变量的值如下所示：</p>
<pre><code class="language-json">{
  &quot;cluster&quot;: {
    &quot;chief&quot;: [&quot;host1:port&quot;],
    &quot;worker&quot;: [&quot;host2:port&quot;, &quot;host3:port&quot;],
    &quot;ps&quot;: [&quot;host4:port&quot;],
    &quot;evaluator&quot;: [&quot;host5:port&quot;]
  },
  &quot;task&quot;: {
    &quot;type&quot;: &quot;worker&quot;,
    &quot;index&quot;: 0
  }
}

</code></pre>
<p>chief 节点的作用和 worker 节点大致相同，不过它还会做一些额外的工作，比如保存检查点文件 (checkpoints) 以及为 Tensorboard 记录日志文件等，如果不指定 cheif 节点，则默认会以 worker 列表中的第一个节点作为 chief 节点； worker 节点用来执行训练操作； ps 节点用来存储变量，只有在使用 ParameterServerStrategy 训练策略时才需要指定； evaluator 用来执行交叉验证操作，一般也是在使用 ParameterServerStrategy 策略时才会指定。</p>
<p>注意所有节点的 TF_CONFIG 环境变量中的 cluster 信息都是相同的，不同的地方在于 task 部分，而且所有角色 (task type) 的 index 必须从 0 开始，因为 TensorFlow 会根据该 index 从 cluster 下相应角色的列表中读取节点信息。</p>
<p>TF_CONFIG 环境变量可以写入到系统的环境变量中，但前提是该物理节点上只会同时启动一个集群节点实例，在大多数情况下，我们会在 python 程序中通过 os.environ[&quot;TF_CONFIG&quot;] 来指定集群的信息以实现按需创建，TensorFlow 运行时会自动解析其中的信息并启动训练任务。</p>
<h1 id="tf-集群分布式训练的难点">TF 集群分布式训练的难点</h1>
<p>集群分布式训练的难点在于每个节点的 TF_CONFIG 环境变量的构建，因为我们不能在每次训练时都去手动指定 ip 和端口（还需确定该端口是否被占用），一两个节点还可以忍受，可如果同时运行多个训练任务，并且每个任务都会使用几十个集群节点，那么手动构造这个环境变量的工作量是巨大的。</p>
<p>我们需要找到一种自动构建 TF_CONFIG 环境变量的方法，一些分布式训练框架可以为我们排忧解难。比如阿里的 x-deeplearning。</p>
<h1 id="参考资料">参考资料</h1>
<p><a href="https://tf.wiki/zh_hans/appendix/distributed.html">TensorFlow分布式训练 — 简单粗暴 TensorFlow 2 0.4 beta 文档</a></p>
<p><a href="https://juejin.cn/post/6885151250124374023">TensorFlow 篇 | TensorFlow 2.x 分布式训练概览</a></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[uber的Horovod]]></title>
        <id>https://DragonFive.github.io/post/uber-de-horovod/</id>
        <link href="https://DragonFive.github.io/post/uber-de-horovod/">
        </link>
        <updated>2020-05-14T09:03:21.000Z</updated>
        <content type="html"><![CDATA[<p>uber的Horovod 发表在 <a href="https://arxiv.org/abs/1802.05799">Horovod: fast and easy distributed deeplearninginTensorFlow</a>。</p>
<p>horovod 提供的各种框架的支持可以让 horovod 比较好的在各个框架的基础上使用，他支持 tensorflow/keras/mxnet/pytorch，MPI 的实现也有很多，比如 OpenMPI 还有 Nvidia 的 NCCL，还有 facebook 的 gloo，他们都实现了一种并行计算的通信和计算方式。</p>
<h1 id="用法">用法</h1>
<p>horovod追求以尽可能小的代码侵入性。<br>
<img src="https://DragonFive.github.io//post-images/1625217840093.png" alt="" loading="lazy"></p>
<p>在用户已经构建的代码上，只需要插入三段很短的代码即可，Horovod易用性甚好。因为只要用户的代码没问题，Horovod这三段植入不会让你的程序break。</p>
<ul>
<li>
<p>hvd.init()</p>
</li>
<li>
<p>创建horovod的优化器，即DistributedOptimizer，将旧的优化器封装起来</p>
</li>
<li>
<p>创建horovod的初始化hook，即BroadcastGlobalVariablesHook，将master的初始化值广播给其他worker</p>
</li>
</ul>
<p>hvd.init()这个函数。用户的这一句话，启动了Horovod的所有轮询进程及资源管理过程，下图描述了hvd.init()的宏观调用栈，核心就是background thread上启动的BackgroundThreadLoop()函数，它将常驻在进程中并不断轮询，直到程序完全结束。</p>
<figure data-type="image" tabindex="1"><img src="https://DragonFive.github.io//post-images/1625218453054.png" alt="" loading="lazy"></figure>
<p>Horovod借助BackgroundThreadLoop()函数对RunLoopOnce()函数做无限循环调用。<br>
若某份gradients已经产生，何时做AllReduce才能不死锁？显然，不可能见到一份gradients就马上做，因为这有概率会陷入死锁。正确答案应该是：</p>
<blockquote>
<p>“当该份gradients在所有的worker上均已经产出时，才能统一发动AllReduce”</p>
</blockquote>
<p>此时，不会有worker因为在等待其他某个worker没有产出该份gradients而进入无限等待的情况。那么就需要有一种机制，能够观察每份gradients在每个worker上的产出情况。</p>
<p>实际上，上述过程其实就是Horovod的做法。BackgroundThreadLoop为什么一直要轮询？就是要不断地做通知，计数等管理工作。因此，rank 0又被称为——Coordinator。等到真正需要做AllReduce时，RunLoopOnce会调用PerformOperation发动通信过程。</p>
<h1 id="参考资料">参考资料</h1>
<p><a href="https://ggaaooppeenngg.github.io/zh-CN/2019/08/30/horovod-%E5%AE%9E%E7%8E%B0%E5%88%86%E6%9E%90/">horovod 实现分析 | ggaaooppeenngg</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/332825987">Horovod 源码分析 - 知乎</a></p>
<p><a href="https://mp.weixin.qq.com/s/7c7Q0P3g3IEL_r4BU2ZxRg">Horovod架构剖析——解密最成功的第三方DL分布式训练框架</a></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[tensorflow 1.x 的分布式训练]]></title>
        <id>https://DragonFive.github.io/post/tensorflow-1x-de-fen-bu-shi-xun-lian/</id>
        <link href="https://DragonFive.github.io/post/tensorflow-1x-de-fen-bu-shi-xun-lian/">
        </link>
        <updated>2019-09-05T07:04:42.000Z</updated>
        <content type="html"><![CDATA[<h1 id="模型并行">模型并行</h1>
<p>将模型部署到很多设备上（设备可能分布在不同机器上）运行，由于模型分割开的各个部分之间有相互依赖关系，因此计算效率不高。所以在模型大小不算太大的情况下一般不使用模型并行。</p>
<h1 id="数据并行">数据并行</h1>
<p>相比较模型并行，数据并行方式能够支持更大的训练规模，提供更好的扩展性，因此数据并行是深度学习最常采用的分布式训练策略。</p>
<blockquote>
<p>in-graph replication和between-graph replication 都用于数据并行。<br>
所谓 replication，指的是各个task，replication的对象是模型。<br>
在使用in-graph replication方式时，只有一个client进程（可以在参与训练的CPU或GPU上任选一个task来运行这个client，参与计算的其它tasks不运行这个client）来创建模型（即tf.Graph）及模型的参数（那些tf.Variables，比如权重W和偏置b）。由于参数（W和b）是共享的，该client指定把参数放在/job:ps，即parameter server上（比如 /job:ps/task:0/cpu:0）。模型的计算部分（前向传播，后向传播，loss和梯度计算，等等）也由该client进程定义好，然后client进程把这个计算部分分配到各个GPU device上（这个过程就相当于在各个GPU中复制模型），分配的方式类似函数调用，但每次调用都指定了设备（即 /job:worker/task:0/gpu:0，/job:worker/task:1/gpu:0，等等）。调用时，模型的参数（即W和b）被当作函数的参数输入给不同tasks（通常运行在不同GPU上）运行的模型，以保证这些参数确实是共享的。<br>
如果用between-graph replication方式，则每个task都运行自己的client进程用于创建模型和参数，并将参数pin到parameter server上（比如 /job:ps/task:0/cpu:0），然后各自独立地执行该模型。注意，每个task创建的模型必须一模一样，这很容易做到，因为只要每个task里的这部分代码都一样就行了。问题是，这些task各自创建并pin到parameter server上的模型参数是同样的吗？问这个问题是因为我们现在跑的是数据并行，而模型的参数及其更新都必须由parameter server统一处理。回答是，只要各task使用同样的parameter server设备名（比如都用 /job:ps/task:0/cpu:0）和同样的变量名（那些tf.Variable定义的变量，比如权重和偏置变量)， 那么在默认的情况下，它们被分配在parameter server的相同的存储里。</p>
</blockquote>
<p>由于in-graph replication的性能不好，现在基本上只使用between-graph replication了。</p>
<h1 id="参数更新方式">参数更新方式</h1>
<p>数据并行参数更新方式可以是同步的（synchronous），也可以是异步的（asynchronous）。</p>
<p>百度的综述<a href="https://arxiv.org/abs/2003.05622">Distributed Hierarchical GPU Parameter Server for Massive Scale Deep Learning Ads Systems</a> 介绍了三种同步模式：</p>
<ul>
<li>
<p>BSP(bulk sync parallel)，严格对所有worker的更新进行同步</p>
</li>
<li>
<p>SSP(stale sync parallel)，对快worker 进行同步</p>
</li>
<li>
<p>ASP（async parallel）, 不同步gradient</p>
</li>
</ul>
<p>后两种方式虽然提升了训练效率，但是降低了模型性能</p>
<p>XDL使用的是ASP。在tensorflow中异步训练是默认的并行训练模式。</p>
<h2 id="异步训练">异步训练</h2>
<p>异步训练中，各个设备完成一个mini-batch训练之后，不需要等待其它节点，直接去更新模型的参数。异步训练总体会训练速度会快很多，但是异步训练的一个很严重的问题是梯度失效问题（stale gradients），刚开始所有设备采用相同的参数来训练，但是异步情况下，某个设备完成一步训练后，可能发现模型参数已经被其它设备更新过了，此时这个设备计算出的梯度就过期了。由于梯度失效问题，异步训练可能陷入次优解。</p>
<h2 id="同步训练">同步训练</h2>
<p>所谓同步指的是所有的设备都是采用相同的模型参数来训练，等待所有设备的mini-batch训练完成后，收集它们的梯度后执行模型的一次参数更新。</p>
<p>Tensorflow提供了tf.train.SyncReplicasOptimizer类用于执行同步训练。把异步训练改造成同步训练只需要两步：</p>
<p>在原来的Optimizer上封装SyncReplicasOptimizer，将参数更新改为同步模式；</p>
<pre><code class="language-python">optimizer = tf.train.SyncReplicasOptimizer(optimizer, replicas_to_aggregate=num_workers)
</code></pre>
<p>在MonitoredTrainingSession或者EstimatorSpec的hook中增加sync_replicas_hook：</p>
<pre><code class="language-python"> sync_replicas_hook = optimizer.make_session_run_hook(is_chief, num_tokens=0)
</code></pre>
<p>同步训练需要各个设备的计算能力要均衡，而且要求集群的通信也要均衡，慢worker会拖慢整体进度。</p>
<h1 id="tensorflow-1-分布式架构">tensorflow 1 分布式架构</h1>
<p>2017年2月百度在PaddlePaddle平台上首次引入了ring-allreduce的架构，随后将其提交到tensorflow的contrib package中。同年8月，Uber为tensorflow平台开源了一个更加易用和高效的ring allreduce分布式训练库Horovod。<br>
最后，tensorflow官方终于也在1.11版本中支持了allreduce的分布式训练策略CollectiveAllReduceStrategy，其跟estimator配合使用非常方便，只需要构造tf.estimator.RunConfig 对象时传入CollectiveAllReduceStrategy参数即可。</p>
<p>关于 ring-allreduce 之前总结在 <a href="https://dragonfive.github.io/post/fen-bu-shi-jia-gou-ring-all-reduce-suan-fa/">分布式架构：ring all-reduce算法</a>。</p>
<h2 id="使用-tensorflow-estimator-api-来编写分布式训练代码">使用 TensorFlow Estimator API 来编写分布式训练代码</h2>
<p>要让tensorflow分布式运行，首先我们需要定义一个由参与分布式计算的机器组成的集群，如下：</p>
<pre><code class="language-py">cluster = {'chief': ['host0:2222'], 'ps': ['host1:2222', 'host2:2222'], 'worker': ['host3:2222', 'host4:2222', 'host5:2222']}
</code></pre>
<p>集群中一般有多个worker，需要指定其中一个worker为主节点（cheif），chief节点会执行一些额外的工作，比如模型导出之类的。在PS分布式架构环境中，还需要定义ps节点。</p>
<p>要运行分布式Estimator模型，只需要设置好TF_CONFIG环境变量即可，可参考如下代码：</p>
<pre><code class="language-py"># Example of non-chief node:
os.environ['TF_CONFIG'] = json.dumps( {'cluster': cluster, 'task': {'type': 'worker', 'index': 1}})

# Example of chief node:
os.environ['TF_CONFIG'] = json.dumps( {'cluster': cluster, 'task': {'type': 'chief', 'index': 0}}) 

# Example of evaluator node (evaluator is not part of training cluster) 
os.environ['TF_CONFIG'] = json.dumps( {'cluster': cluster, 'task': {'type': 'evaluator', 'index': 0}})
</code></pre>
<p>定义好上述环境变量后，调用tf.estimator.train_and_evaluate即可开始分布式训练和评估，其他部分的代码跟开发单机的程序是一样的，可以参考下面的资料：<br>
<a href="https://zhuanlan.zhihu.com/p/41473323">构建分布式Tensorflow模型系列:Estimator - 知乎</a></p>
<h1 id="参考资料">参考资料</h1>
<p><a href="https://zhuanlan.zhihu.com/p/56991108">一文说清楚Tensorflow分布式训练必备知识 - 知乎</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/60474307">什么是in-graph replication和between-graph replication? - 知乎</a></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[TF的OP与Tensor]]></title>
        <id>https://DragonFive.github.io/post/tf-de-op-yu-tensor/</id>
        <link href="https://DragonFive.github.io/post/tf-de-op-yu-tensor/">
        </link>
        <updated>2019-04-18T01:42:54.000Z</updated>
        <content type="html"><![CDATA[<blockquote>
<p>最近在读《TensorFlow 内核剖析》这本书，作者<a href="https://www.jianshu.com/u/49d1f3b7049e">刘光聪</a>。上篇记录于 <a href="http://localhost:4000/post/tf-de-session-yu-graph/">TF的session 与graph | dragon</a></p>
</blockquote>
<p>Graph包含两大成员，节点和边。节点即为计算算子Operation，边则为计算数据Tensor。由起始节点Source出发，按照Graph的拓扑顺序，依次执行节点的计算，即可完成整图的计算，最后结束于终止节点Sink，并输出计算结果。边用来表示计算的数据，它经过上游节点计算后得到，然后传递给下游节点进行运算。本文讲解Graph的边Tensor，以及TensorFlow中的变量。</p>
<h1 id="op前端">Op前端</h1>
<p>Python前端中，Operation表示Graph的节点，Tensor表示Graph的边。Operation包含OpDef和NodeDef两个主要成员变量。其中OpDef描述了op的静态属性信息，例如op入参列表，出参列表等。而NodeDef则描述op的动态属性信息，例如op运行的设备信息，用户给op设置的name等。</p>
<pre><code class="language-python">@tf_export(&quot;Operation&quot;)
class Operation(object):
  def __init__(self,
               node_def,
               g,
               inputs=None,
               output_types=None,
               control_inputs=None,
               input_types=None,
               original_op=None,
               op_def=None):
     # graph引用，通过它可以拿到Operation所注册到的Graph
     self._graph = g
    
    # inputs
    if inputs is None:
      inputs = []

    #  input types
    if input_types is None:
      input_types = [i.dtype.base_dtype for i in inputs]

    # control_input_ops
    control_input_ops = []
    
    # node_def和op_def是两个最关键的成员
    if not self._graph._c_graph:
      self._inputs_val = list(inputs)  # Defensive copy.
      self._input_types_val = input_types
      self._control_inputs_val = control_input_ops
      
      # NodeDef，深复制
      self._node_def_val = copy.deepcopy(node_def)
        
      # OpDef
      self._op_def_val = op_def
      
    # outputs输出
    self._outputs = [
        Tensor(self, i, output_type)
        for i, output_type in enumerate(output_types)
    ]
</code></pre>
<p>成员函数，通过成员函数我们可以拿到Operation的两大成员，即OpDef和NodeDef。</p>
<pre><code class="language-python">  @property
  def name(self):
    # Operation的name，注意要嵌套name_scope
	return self._node_def_val.name

  @property
  def _id(self):
    # Operation的唯一标示，id
    return self._id_value

  @property
  def device(self):
    # Operation的设备信息
    return self._node_def_val.device
    
  @property
  def graph(self):
    # graph引用
    return self._graph

  @property
  def node_def(self):
    # NodeDef成员，获取Operation的动态属性信息，例如Operation分配到的设备信息，Operation的name等
    return self._node_def_val

  @property
  def op_def(self):
    # OpDef，获取Operation的静态属性信息，例如Operation入参列表，出参列表等
    return self._op_def_val
</code></pre>
<h2 id="opdef">opDef</h2>
<p>在系统实现中，OP的元数据使用Protobuf格式的OpDef描述，实现前端与后端的数据交换，及其领域模型的统一。</p>
<figure data-type="image" tabindex="1"><img src="https://upload-images.jianshu.io/upload_images/2254249-d25671f7ef2fe90a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/309/format/webp" alt="" loading="lazy"></figure>
<h1 id="op-后端">op 后端</h1>
<p>C++后端中，Graph图也包含两部分，即边Edge和节点Node。同样，节点Node用来表示计算算子，而边Edge则表示计算数据或者Node间依赖关系。Node数据结构如下所示。</p>
<pre><code class="language-c++">class Node {
 public:
    // NodeDef,节点算子Operation的信息，比如op分配到哪个设备上了等，运行时有可能变化。
  	const NodeDef&amp; def() const;
    
    // OpDef, 节点算子Operation的元数据，不会变的。比如Operation的入参个数，名字等
  	const OpDef&amp; op_def() const;
 private:
  	// 输入边，传递数据给节点。可能有多条
  	EdgeSet in_edges_;

  	// 输出边，节点计算后得到的数据。可能有多条
  	EdgeSet out_edges_;
}

</code></pre>
<p>节点Node中包含的主要数据有输入边和输出边的集合，从而能够由Node找到跟他关联的所有边。Node中还包含NodeDef和OpDef两个成员。NodeDef表示节点算子的动态属性，创建Node时会new一个NodeDef对象。OpDef表示节点算子的静态属性，运行时不会变，创建Node时不需要new OpDef，只需要从OpDef仓库中取出即可。因为元信息是确定的，比如Operation的入参列表，出参列表等。</p>
<h1 id="python-tensor">python Tensor</h1>
<p>Tensor作为Graph的边，使得节点Operation之间建立了连接。上游源节点Operation经过计算得到数据Tensor，然后传递给下游目标节点，是一个典型的生产者-消费者关系。下面来看Tensor的数据结构:</p>
<pre><code class="language-python">@tf_export(&quot;Tensor&quot;)
class Tensor(_TensorLike):
  def __init__(self, op, value_index, dtype):
    # 源节点，tensor的生产者，会计算得到tensor
    self._op = op

    # tensor在源节点的输出边集合中的索引。源节点可能会有多条输出边
    # 利用op和value_index即可唯一确定tensor。
    self._value_index = value_index

    # tensor中保存的数据的数据类型
    self._dtype = dtypes.as_dtype(dtype)

    # tensor的shape，可以得到张量的rank，维度等信息
    self._shape_val = tensor_shape.unknown_shape()

    # 目标节点列表，tensor的消费者，会使用该tensor来进行计算
    self._consumers = []

    #
    self._handle_data = None
    self._id = uid()
</code></pre>
<p>Tensor中主要包含两类信息，一个是Graph结构信息，如边的源节点和目标节点。另一个则是它所保存的数据信息，例如数据类型，shape等。</p>
<h1 id="c-端-edge">C++ 端 EDGE</h1>
<p>边Edge为算子所需要的数据，或者代表节点间的依赖关系。这一点和Python中的定义相似。边Edge的持有它的源节点和目标节点的指针，从而将两个节点连接起来。下面看Edge类的定义。</p>
<pre><code class="language-c++">class Edge {
   private:
      Edge() {}

      friend class EdgeSetTest;
      friend class Graph;
      // 源节点, 边的数据就来源于源节点的计算。源节点是边的生产者
      Node* src_;

      // 目标节点，边的数据提供给目标节点进行计算。目标节点是边的消费者
      Node* dst_;

      // 边id，也就是边的标识符
      int id_;

      // 表示当前边为源节点的第src_output_条边。源节点可能会有多条输出边
      int src_output_;

      // 表示当前边为目标节点的第dst_input_条边。目标节点可能会有多条输入边。
      int dst_input_;
};
</code></pre>
<p>Edge既可以承载tensor数据，提供给节点Operation进行运算，也可以用来表示节点之间有依赖关系。对于表示节点依赖的边，其src_output_, dst_input_均为-1，此时边不承载任何数据。</p>
<h1 id="tf-varible">TF varible</h1>
<h2 id="tf常量-constant">TF常量 Constant</h2>
<p>TensorFlow的常量constant，最终包装成了一个Tensor。通过tf.constant(10)，返回一个Tensor对象。</p>
<pre><code class="language-python">@tf_export(&quot;constant&quot;)
def constant(value, dtype=None, shape=None, name=&quot;Const&quot;, verify_shape=False):
  # 算子注册到默认Graph中
  g = ops.get_default_graph()
    
  # 对常量值value的处理
  tensor_value = attr_value_pb2.AttrValue()
  tensor_value.tensor.CopyFrom(
      tensor_util.make_tensor_proto(
          value, dtype=dtype, shape=shape, verify_shape=verify_shape))

  # 对常量值的类型dtype进行处理
  dtype_value = attr_value_pb2.AttrValue(type=tensor_value.tensor.dtype)

  # 构造并注册类型为“Const”的算子到Graph中，从算子的outputs中取出输出的tensor。
  const_tensor = g.create_op(
      &quot;Const&quot;, [], [dtype_value.type],
      attrs={&quot;value&quot;: tensor_value,
             &quot;dtype&quot;: dtype_value},
      name=name).outputs[0]
  return const_tensor
</code></pre>
<p>tf.constant的过程为:</p>
<ol>
<li>获取默认graph</li>
<li>对常量值value和常量值的类型dtype进行处理</li>
<li>构造并注册类型为“Const”的算子到默认graph中，从算子的outputs中取出输出的tensor。</li>
</ol>
<p>在TF1.x中，此时只是图的构造过程，tensor并未承载数据，仅表示Operation输出的一个符号句柄。经过tensor.eval()或session.run()后，才会启动graph的执行，并得到数据。</p>
<h2 id="tf-变量-variable">TF 变量 Variable</h2>
<p>通过tf.Variable()构造一个变量，代码如下：</p>
<pre><code class="language-python">@tf_export(&quot;Variable&quot;)
class Variable(object):
  def __init__(self,
               initial_value=None,
               trainable=True,
               collections=None,
               validate_shape=True,
               caching_device=None,
               name=None,
               variable_def=None,
               dtype=None,
               expected_shape=None,
               import_scope=None,
               constraint=None):
</code></pre>
<p><strong>参数：</strong></p>
<ul>
<li>
<p>initial_value: initial_value：Tensor或可转换为Tensor的Python对象，它是Variable的初始值。除非validate_shape设置为False，否则初始值必须具有指定的形状；也可以是一个可调用的，没有参数，在调用时返回初始值。在这种情况下，必须指定dtype。 （请注意，init_ops.py中的初始化函数必须首先绑定到形状才能在此处使用。）</p>
</li>
<li>
<p>trainable：是否可以训练，如果为false，则训练时不会改变，如果为True，则会默认将变量添加到图形集合GraphKeys.TRAINABLE_VARIABLES中。此集合用于Optimizer类优化的的默认变量列表【可为optimizer指定其他的变量集合】，可就是要训练的变量列表。</p>
</li>
<li>
<p>collections：变量要加入哪个集合中，有全局变量集合、本地变量集合、可训练变量集合等。默认[GraphKeys.GLOBAL_VARIABLES]加入全局变量集合中</p>
</li>
<li>
<p>validate_shape：如果为False，则允许使用未知形状的值初始化变量。如果为True，则默认为initial_value的形状必须已知。</p>
</li>
<li>
<p>name：变量的可选名称。默认为“Variable”并自动获取。</p>
</li>
<li>
<p>dtype：如果设置，则initial_value将转换为给定类型。如果为None，则保留数据类型（如果initial_value是Tensor），或者convert_to_tensor将决定。</p>
</li>
<li>
<p>expected_shape：TensorShape。如果设置，则initial_value应具有此形状。</p>
</li>
</ul>
<h3 id="初始化">初始化</h3>
<p>Variable可以接受一个tensor或者可以被包装为tensor的值，来作为初始值。事实上，Variable可以看做是Tensor的包装器，它重载了Tensor的几乎所有操作，是对Tensor的进一步封装。</p>
<p>初始化时将initial_value初始值赋予Variable内部持有的Tensor。通过运行变量的初始化器可以对变量进行初始化，也可以执行全局初始化器。如下</p>
<pre><code class="language-python">y = tf.Variable([5.3])

with tf.Session() as sess:
    initialization = tf.global_variables_initializer()
    print sess.run(initialization)
</code></pre>
<p>通过调用tf.global_variables_initializer()将变量的所有初始化器进行汇总，然后启动Session运行该OP。事实上，搜集所有全局变量的初始化器的OP是一个NoOp，即不存在输入，也不存在输出。所有变量的初始化器通过控制依赖边与该NoOp相连，保证所有的全局变量被初始化。</p>
<p>一般常用的参数包括初始化值和名称name(是该变量的唯一索引)，在使用变量之前必须要进行初始化，初始化的方式有三种：</p>
<ul>
<li>在会话中运行initializer操作。</li>
<li>从文件中恢复，如restore from checkpoint。</li>
<li>自己通过tf.assign()给变量附初值。</li>
</ul>
<h3 id="初始化依赖">初始化依赖</h3>
<p>如果一个变量初始化需要依赖于另外一个变量的初始值，则需要特殊地处理。例如，变量V的初始值依赖于W的初始值，可以通过W.initialized_value()指定。</p>
<pre><code class="language-python">W = tf.Variable(tf.zeros([784,10]), name='W')
V = tf.Variable(W.initialized_value(), name='V')
</code></pre>
<p>事实上，两者通过Identity衔接，并显式地添加了依赖控制边，保证W在V之前初始化。此处，存在两个Identity的OP，但职责不一样，它们分别完成初始化依赖和变量读取。</p>
<figure data-type="image" tabindex="2"><img src="https://upload-images.jianshu.io/upload_images/2254249-07fcc971de877958.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/340/format/webp" alt="" loading="lazy"></figure>
<h2 id="tfget_variable">tf.get_variable()</h2>
<pre><code class="language-python">get_variable(
    name,
    shape=None,
    dtype=None,
    initializer=None,
    regularizer=None,
    trainable=True,
    collections=None,
    caching_device=None,
    partitioner=None,
    validate_shape=True,
    use_resource=None,
    custom_getter=None,
    constraint=None
)
</code></pre>
<p><strong>参数：</strong></p>
<ul>
<li>
<p>name：新变量或现有变量的名称。</p>
</li>
<li>
<p>shape：新变量或现有变量的形状。</p>
</li>
<li>
<p>dtype：新变量或现有变量的类型（默认为DT_FLOAT）。</p>
</li>
<li>
<p>ininializer：如果创建了，则用它来初始化变量。</p>
</li>
<li>
<p>regularizer：A（Tensor - &gt; Tensor或None）函数;将它应用于新创建的变量的结果将添加到集合tf.GraphKeys.REGULARIZATION_LOSSES中，并可用于正则化。</p>
</li>
<li>
<p>trainable：如果为True，还将变量添加到图形集合GraphKeys.TRAINABLE_VARIABLES（参见tf.Variable）。</p>
</li>
<li>
<p>collections：要将变量添加到的图表集合列表。默认为[GraphKeys.GLOBAL_VARIABLES]（参见tf.Variable）。</p>
</li>
<li>
<p>validate_shape：如果为False，则允许使用未知形状的值初始化变量。如果为True，则默认为initial_value的形状必须已知。</p>
</li>
<li>
<p>use_resource：如果为False，则创建常规变量。如果为true，则使用定义良好的语义创建实验性ResourceVariable。默认为False（稍后将更改为True）。在Eager模式下，此参数始终强制为True。</p>
</li>
<li>
<p>custom_getter：Callable，它将第一个参数作为true getter，并允许覆盖内部get_variable方法。 custom_getter的签名应与此方法的签名相匹配，但最适合未来的版本将允许更改：def custom_getter（getter，* args，** kwargs）。也允许直接访问所有get_variable参数：def custom_getter（getter，name，* args，** kwargs）。一个简单的身份自定义getter只需创建具有修改名称的变量是：python def custom_getter（getter，name，* args，** kwargs）：return getter（name +'_suffix'，* args，** kwargs）。</p>
</li>
</ul>
<p>如果initializer初始化方法是None(默认值)，则会使用variable_scope()中定义的initializer，如果也为None，则默认使用glorot_uniform_initializer，也可以使用其他的tensor来初始化，value、和shape与此tensor相同。</p>
<p>正则化方法默认是None，如果不指定，只会使用variable_scope()中的正则化方式，如果也为None，则不使用正则化；</p>
<h2 id="两者区别">两者区别</h2>
<p>tf.get_variable()会检查当前命名空间下是否存在同样name的变量，可以方便共享变量。而tf.Variable每次都会新建一个变量。tf.get_variable()，要配合reuse和tf.variable_scope()使用，对于get_variable()来说，如果已经创建的变量对象，就把那个对象返回，如果没有创建变量对象的话，就创建一个新的。</p>
<pre><code class="language-python">import tensorflow as tf

with tf.variable_scope(&quot;scope1&quot;):
    w1 = tf.get_variable(&quot;w1&quot;, shape=[])
    w2 = tf.Variable(0.0, name=&quot;w2&quot;)
with tf.variable_scope(&quot;scope1&quot;, reuse=True):
    w1_p = tf.get_variable(&quot;w1&quot;, shape=[])
    w2_p = tf.Variable(1.0, name=&quot;w2&quot;)

print(w1 is w1_p, w2 is w2_p)
#输出
#True  False
</code></pre>
<h3 id="graphkeys-图分组">GraphKeys 图分组</h3>
<p>每个Operation节点都有一个特定的标签，从而实现节点的分类。相同标签的节点归为一类，放到同一个Collection中。标签是一个唯一的GraphKey，GraphKey被定义在类GraphKeys中，如下</p>
<pre><code class="language-python">@tf_export(&quot;GraphKeys&quot;)
class GraphKeys(object):
    GLOBAL_VARIABLES = &quot;variables&quot;
    QUEUE_RUNNERS = &quot;queue_runners&quot;
    SAVERS = &quot;savers&quot;
    WEIGHTS = &quot;weights&quot;
    BIASES = &quot;biases&quot;
    ACTIVATIONS = &quot;activations&quot;
    UPDATE_OPS = &quot;update_ops&quot;
    LOSSES = &quot;losses&quot;
    TRAIN_OP = &quot;train_op&quot;
    # 省略其他
</code></pre>
<h3 id="variable集合">Variable集合</h3>
<p>Variable被划分到不同的集合中，方便后续操作。常见的集合有</p>
<p>全局变量：全局变量可以在不同进程中共享，可运用在分布式环境中。变量默认会加入到全局变量集合中。通过tf.global_variables()可以查询全局变量集合。其op标示为GraphKeys.GLOBAL_VARIABLES</p>
<pre><code class="language-python">@tf_export(&quot;global_variables&quot;)
def global_variables(scope=None):
  return ops.get_collection(ops.GraphKeys.GLOBAL_VARIABLES, scope)
</code></pre>
<p>本地变量：运行在进程内的变量，不能跨进程共享。通常用来保存临时变量，如训练迭代次数epoches。通过tf.local_variables()可以查询本地变量集合。其op标示为GraphKeys.LOCAL_VARIABLES</p>
<pre><code class="language-python">@tf_export(&quot;local_variables&quot;)
def local_variables(scope=None):
	return ops.get_collection(ops.GraphKeys.LOCAL_VARIABLES, scope)
</code></pre>
<p>可训练变量：一般模型参数会放到可训练变量集合中，训练时，做这些变量会得到改变。不在这个集合中的变量则不会得到改变。默认会放到此集合中。通过tf.trainable_variables()可以查询。其op标示为<br>
GraphKeys.TRAINABLE_VARIABLES</p>
<pre><code class="language-python">@tf_export(&quot;trainable_variables&quot;)
def trainable_variables(scope=None):
  return ops.get_collection(ops.GraphKeys.TRAINABLE_VARIABLES, scope)
</code></pre>
<p>其他集合还有model_variables，moving_average_variables。</p>
<h2 id="tf-variable-的本质">TF variable 的本质</h2>
<p>Variable是一个特殊的OP，它拥有状态(Stateful)。如果从实现技术探究，Variable的Kernel实现直接持有一个Tensor实例，存在几个操作Variable的特殊OP，例如Assign, AssignAdd等。变量所持有的Tensor以引用的方式输入到Assign中，Assign根据初始值，就地修改Tensor内部的值，最后以引用的方式输出该Tensor。通过初始化器(Initializer)在初始化期间，将初始化值赋予Variable内部所持有Tensor，完成Variable的就地修改。如果要读取变量的值，则通过Identity恒等变化，直接输出变量所持有的Tensor。但时，Identity去除了Variable的引用标识，同时也避免了内存拷贝。</p>
<figure data-type="image" tabindex="3"><img src="https://upload-images.jianshu.io/upload_images/2254249-7c2cd1ef175994b6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/642/format/webp" alt="" loading="lazy"></figure>
<h1 id="参考资料">参考资料</h1>
<p><a href="https://www.jianshu.com/p/236335897b30">TensorFlow架构与设计：OP本质论 - 简书</a></p>
<p><a href="https://www.jianshu.com/p/bebcdfb74fb1">TensorFlow架构与设计：变量初始化 - 简书</a></p>
<p><a href="https://blog.csdn.net/u013510838/article/details/84141538">Tensorflow源码解析4 -- 图的节点 - Operation_谢杨易的博客-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/u013510838/article/details/84144238">Tensorflow源码解析5 -- 图的边 - Tensor_谢杨易的博客-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/TeFuirnever/article/details/89577480">tf.get_variable()和tf.Variable()的区别</a></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[分布式架构：ring all-reduce算法]]></title>
        <id>https://DragonFive.github.io/post/fen-bu-shi-jia-gou-ring-all-reduce-suan-fa/</id>
        <link href="https://DragonFive.github.io/post/fen-bu-shi-jia-gou-ring-all-reduce-suan-fa/">
        </link>
        <updated>2019-03-22T07:13:20.000Z</updated>
        <content type="html"><![CDATA[<blockquote>
<p>注：本文内容多参考自 <a href="https://zhuanlan.zhihu.com/p/56991108">一文说清楚Tensorflow分布式训练必备知识 - 知乎</a></p>
</blockquote>
<p>PS架构中，当worker数量较多时，ps节点的网络带宽将成为系统的瓶颈。</p>
<p>AllReduce 架构是指不带有参数服务器的分布式集群架构。在该架构中，集群中的所有节点都作为 worker 来执行计算操作，该架构会在每个 batch 训练完成后使用 AllReduce 算法在所有 worker 节点间进行模型变量的同步更新。</p>
<p>目前应用于深度学习的 AllReduce 算法有多种，如 Ring AllReduce 以及 NCCL 等</p>
<p>传统的同步更新方法（各个gpu卡算好梯度，求和算平均的方式），在融合梯度时，会产生巨大的通信数据量，这种通信压力往往在模型参数量很大时，显得很明显。因此我们需要找到一种方法，来解决同步更新的网络瓶颈问题。其中最具代表性的一种方法就是：ring all-reduce。</p>
<p>Ring AllReduce架构中各个设备都是worker，没有中心节点来聚合所有worker计算的梯度。Ring AllReduce算法将 device 放置在一个逻辑环路（logical ring）中。每个 device 从上行的device 接收数据，并向下行的 deivce 发送数据，因此可以充分利用每个 device 的上下行带宽。</p>
<figure data-type="image" tabindex="1"><img src="https://DragonFive.github.io//post-images/1625216429335.jpeg" alt="" loading="lazy"></figure>
<p>梯度融合过程分为两阶段：</p>
<ol>
<li>
<p>Scatter Reduce：在这个 Scatter Reduce阶段，GPU 会逐步交换彼此的梯度并融合，最后每个 GPU 都会包含完整融合梯度的一部分</p>
</li>
<li>
<p>Allgather：GPU 会逐步交换彼此不完整的融合梯度，最后所有 GPU 都会得到完整的融合梯度</p>
</li>
</ol>
<p>使用 Ring Allreduce 算法进行某个稠密梯度的平均值的基本过程如下：</p>
<p>将每个设备上的梯度 tensor 切分成长度大致相等的 num_devices 个分片；</p>
<p>ScatterReduce 阶段：通过 num_devices - 1 轮通信和相加，在每个 device 上都计算出一个 tensor 分片的和；</p>
<p>AllGather 阶段：通过 num_devices - 1 轮通信和覆盖，将上个阶段计算出的每个 tensor 分片的和广播到其他 device；</p>
<p>在每个设备上合并分片，得到梯度和，然后除以 num_devices，得到平均梯度；</p>
<p>以 4 个 device上的梯度求和过程为例：</p>
<p>ScatterReduce 阶段：</p>
<figure data-type="image" tabindex="2"><img src="https://flomo.oss-cn-shanghai.aliyuncs.com/file/2021-07-02/32821/5db30ccec04cc0cb08d547fd89e42022.png" alt="图片来自知乎-杨旭东" loading="lazy"></figure>
<p>经过 num_devices - 1 轮后，每个 device 上都有一个 tensor 分片进得到了这个分片各个 device 上的和；</p>
<p>AllGather 阶段：</p>
<figure data-type="image" tabindex="3"><img src="https://flomo.oss-cn-shanghai.aliyuncs.com/file/2021-07-02/32821/05493e10065ea1d444da13f1f354798a.png" alt="图片来自知乎-杨旭东" loading="lazy"></figure>
<p>经过 num_devices - 1 轮后，每个 device 上都每个 tensor 分片都得到了这个分片各个 device 上的和；</p>
<p>相比PS架构，Ring Allreduce架构是带宽优化的，因为集群中每个节点的带宽都被充分利用。此外，在深度学习训练过程中，计算梯度采用BP算法，其特点是后面层的梯度先被计算，而前面层的梯度慢于前面层，Ring-allreduce架构可以充分利用这个特点，在前面层梯度计算的同时进行后面层梯度的传递，从而进一步减少训练时间。Ring Allreduce的训练速度基本上线性正比于GPUs数目（worker数）。</p>
<p>通信代价分析：每个 GPU 在Scatter Reduce 阶段，接收 N-1 次数据，N 是 GPU 数量；每个 GPU 在allgather 阶段，接收 N-1 次 数据；每个 GPU 每次发送 K/N 大小数据块，K 是总数据大小；所以，Data Transferred=2(N−1)*K/N ，随着 GPU 数量 N 增加，总传输量恒定。也就是理论上，随着gpu数量的增加，ring all-reduce有线性加速能力。</p>
<h2 id="参考资料">参考资料</h2>
<p><a href="https://zhuanlan.zhihu.com/p/69797852">浅谈Tensorflow分布式架构：ring all-reduce算法 - 知乎</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/56991108">一文说清楚Tensorflow分布式训练必备知识 - 知乎</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/34172340">【第一期】AI Talk：TensorFlow 分布式训练的线性加速实践 - 知乎</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/79030485">腾讯机智团队分享--AllReduce算法的前世今生 - 知乎</a></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[[转自刘光聪]TensorFlow架构与设计：OP本质论]]></title>
        <id>https://DragonFive.github.io/post/tensorflow-jia-gou-yu-she-ji-op-ben-zhi-lun-liu-guang-cong/</id>
        <link href="https://DragonFive.github.io/post/tensorflow-jia-gou-yu-she-ji-op-ben-zhi-lun-liu-guang-cong/">
        </link>
        <updated>2019-03-05T03:59:51.000Z</updated>
        <content type="html"><![CDATA[<blockquote>
<blockquote>
<p>最近在读《TensorFlow 内核剖析》这本书，作者<a href="https://www.jianshu.com/u/49d1f3b7049e">刘光聪</a>。本篇文章摘自 <a href="https://www.jianshu.com/p/236335897b30">TensorFlow架构与设计：OP本质论 - 简书</a></p>
</blockquote>
</blockquote>
<h1 id="符号编程">符号编程</h1>
<p>TensorFlow的计算过程是一个延迟计算，是一种典型的基于符号的编程范式。从计算时间轴看，计算过程基本分为2个阶段：</p>
<ul>
<li>图构造期：负责计算图的构造；</li>
<li>图执行期：负责计算图的执行。</li>
</ul>
<p>其中，在系统初始化时，系统实现对所有OP进行扫描注册，并保存于OpRegistry之中。</p>
<h2 id="注册op">注册OP</h2>
<p>理论上，OP的注册发生在系统初始化阶段。后端系统，可以使用REGISTER_OP实用宏注册OP。前端系统，也存在类似的OP注册机制。</p>
<p>使用REGISTER_OP注册OP过程，实际上是一个REGISTER_OP描述到OpDef表示的翻译过程。OpDefBuilder通过链式调用Input, Output, Attr方法分别构造OP的输入、输出列表，及其属性列表。最后，通过调用Finalize成员函数，经过解析字符串表示，将其翻译为OpDef的内在表示，最后注册到OpRegistry之中。</p>
<figure data-type="image" tabindex="1"><img src="https://upload-images.jianshu.io/upload_images/2254249-66c28d1f68d97448.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/150/format/webp" alt="" loading="lazy"></figure>
<h1 id="参考资料">参考资料</h1>
<p><a href="https://www.jianshu.com/p/236335897b30">TensorFlow架构与设计：OP本质论 - 简书</a></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[TF的session 与graph]]></title>
        <id>https://DragonFive.github.io/post/tf-de-session-yu-graph/</id>
        <link href="https://DragonFive.github.io/post/tf-de-session-yu-graph/">
        </link>
        <updated>2019-01-30T09:54:33.000Z</updated>
        <content type="html"><![CDATA[<blockquote>
<p>最近在读《TensorFlow 内核剖析》这本书，作者刘光聪。有一些收获，记录一下。</p>
</blockquote>
<h1 id="tf的session">TF的Session</h1>
<p>Session是TensorFlow前后端连接的桥梁。用户利用session使得client能够与master的执行引擎建立连接，并通过session.run()来触发一次计算。它建立了一套上下文环境，封装了operation计算以及tensor求值的环境。</p>
<p>session之间采用共享graph的方式来提高运行效率。一个session只能运行一个graph实例，但一个graph可以运行在多个session中。创建session时如果不指定Graph实例，则会使用系统默认Graph。当session close时，默认 graph 引用计数减1。只有引用计数为0时，graph才会被回收。这种graph共享的方式，大大减少了graph创建和回收的资源消耗，优化了TensorFlow运行效率。</p>
<p>op运算和tensor求值时，如果没有指定运行在哪个session中，则会运行在默认session中。通过session.as_default()可以将自己设置为默认session。</p>
<pre><code class="language-python">operation.run()
tensor.eval()
</code></pre>
<p>实际执行的代码是</p>
<pre><code class="language-python">tf.get_default_session().run(operation)
tf.get_default_session().run(tensor)
</code></pre>
<h1 id="session-类型">Session 类型</h1>
<h2 id="前端-session">前端 Session</h2>
<figure data-type="image" tabindex="1"><img src="https://DragonFive.github.io//post-images/1625220300985.png" alt="" loading="lazy"></figure>
<p>分为普通Session和交互式InteractiveSession， 区别在于：</p>
<ul>
<li>
<p>InteractiveSession创建后，会将自己替换为默认session。使得之后operation.run()和tensor.eval()的执行通过这个默认session来进行。特别适合Python交互式环境。</p>
</li>
<li>
<p>InteractiveSession自带with上下文管理器。它在创建时和关闭时会调用上下文管理器的enter和exit方法，从而进行资源的申请和释放，避免内存泄漏问题。这同样很适合Python交互式环境。</p>
</li>
</ul>
<p>BaseSession基本包含了所有的会话实现逻辑。包括会话的整个生命周期，也就是创建 执行 关闭和销毁四个阶段。</p>
<p>BaseSession包含的主要成员变量有：</p>
<ul>
<li>graph引用</li>
<li>序列化的graph_def</li>
<li>要连接的tf引擎target</li>
<li>session配置信息config</li>
</ul>
<h2 id="后端session">后端Session</h2>
<p>后端master中，根据前端client调用tf.Session(target=’’, graph=None, config=None)时指定的target，来创建不同的Session。target为要连接的tf后端执行引擎，默认为空字符串。Session创建采用了抽象工厂模式，如果为空字符串，则创建本地DirectSession，如果以grpc://开头，则创建分布式GrpcSession。</p>
<p>DirectSession只能利用本地设备，将任务创建到本地的CPU GPU上。而GrpcSession则可以利用远端分布式设备，将任务创建到其他机器的CPU GPU上，然后通过grpc协议进行通信。</p>
<figure data-type="image" tabindex="2"><img src="https://DragonFive.github.io//post-images/1625220817848.png" alt="" loading="lazy"></figure>
<h1 id="session-生命周期">Session 生命周期</h1>
<p>Session作为前后端连接的桥梁，以及上下文运行环境，其生命周期尤其关键。大致分为4个阶段</p>
<ul>
<li>创建：通过tf.Session()创建session实例，进行系统资源分配，特别是graph引用计数加1</li>
<li>运行：通过session.run()触发计算的执行，client会将整图graph传递给master，由master进行执行</li>
<li>关闭：通过session.close()来关闭，会进行系统资源的回收，特别是graph引用计数减1.</li>
<li>销毁：Python垃圾回收器进行GC时，调用session.<strong>del</strong>()进行回收。</li>
</ul>
<h1 id="graph">graph</h1>
<p>可以显示创建Graph，并调用as_default()使他替换默认Graph。在该上下文管理器中创建的op都会注册到这个graph中。退出上下文管理器后，则恢复原来的默认graph。一般情况下，我们不用显式创建Graph，使用系统创建的那个默认Graph即可。</p>
<pre><code class="language-python">with tf.Graph().as_default() as g:
    print tf.get_default_graph() is g
    print tf.get_default_graph()

print tf.get_default_graph()
</code></pre>
<p>在上下文管理器中，当前线程的默认图被替换了，而退出上下文管理后，则恢复为了原来的默认图。</p>
<h1 id="graph-类型">graph 类型</h1>
<h1 id="前端graph-类型">前端graph 类型</h1>
<p>Python前端中，Graph的数据结构。Graph主要的成员变量是Operation和Tensor。Operation是Graph的节点，它代表了运算算子。Tensor是Graph的边，它代表了运算数据。</p>
<pre><code class="language-python">@tf_export(&quot;Graph&quot;)
class Graph(object):
    def __init__(self):
   	    # 加线程锁，使得注册op时，不会有其他线程注册op到graph中，从而保证共享graph是线程安全的
        self._lock = threading.Lock()
        
        # op相关数据。
        # 为graph的每个op分配一个id，通过id可以快速索引到相关op。故创建了_nodes_by_id字典
        self._nodes_by_id = dict()  # GUARDED_BY(self._lock)
        self._next_id_counter = 0  # GUARDED_BY(self._lock)
        # 同时也可以通过name来快速索引op，故创建了_nodes_by_name字典
        self._nodes_by_name = dict()  # GUARDED_BY(self._lock)
        self._version = 0  # GUARDED_BY(self._lock)
        
        # tensor相关数据。
        # 处理tensor的placeholder
        self._handle_feeders = {}
        # 处理tensor的read操作
        self._handle_readers = {}
        # 处理tensor的move操作
        self._handle_movers = {}
        # 处理tensor的delete操作
        self._handle_deleters = {}

</code></pre>
<p>graph 添加 op 是会保证线程安全的。</p>
<pre><code class="language-python">  def _add_op(self, op):
    # graph被设置为final后，就是只读的了，不能添加op了。
    self._check_not_finalized()
    
    # 保证共享graph的线程安全
    with self._lock:
      # 将op以id和name分别构建字典，添加到_nodes_by_id和_nodes_by_name字典中，方便后续快速索引
      self._nodes_by_id[op._id] = op
      self._nodes_by_name[op.name] = op
      self._version = max(self._version, op._id)

</code></pre>
<h2 id="name_scope">name_scope</h2>
<p>name_scope 节点命名空间<br>
使用name_scope对graph中的节点进行层次化管理，上下层之间通过斜杠分隔。</p>
<h1 id="后端graph">后端Graph</h1>
<h2 id="graph-2">Graph</h2>
<pre><code class="language-cpp">class Graph {
     private:
      // 所有已知的op计算函数的注册表
      FunctionLibraryDefinition ops_;

      // GraphDef版本号
      const std::unique_ptr&lt;VersionDef&gt; versions_;

      // 节点node列表，通过id来访问
      std::vector&lt;Node*&gt; nodes_;

      // node个数
      int64 num_nodes_ = 0;

      // 边edge列表，通过id来访问
      std::vector&lt;Edge*&gt; edges_;

      // graph中非空edge的数目
      int num_edges_ = 0;

      // 已分配了内存，但还没使用的node和edge
      std::vector&lt;Node*&gt; free_nodes_;
      std::vector&lt;Edge*&gt; free_edges_;
 }

</code></pre>
<p>后端中的Graph主要成员也是节点node和边edge。节点node为计算算子Operation，边为算子所需要的数据，或者代表节点间的依赖关系。这一点和Python中的定义相似。边Edge的持有它的源节点和目标节点的指针，从而将两个节点连接起来。</p>
<h2 id="edge">Edge</h2>
<pre><code class="language-cpp">class Edge {
     private:
      Edge() {}

      friend class EdgeSetTest;
      friend class Graph;
      // 源节点, 边的数据就来源于源节点的计算。源节点是边的生产者
      Node* src_;

      // 目标节点，边的数据提供给目标节点进行计算。目标节点是边的消费者
      Node* dst_;

      // 边id，也就是边的标识符
      int id_;

      // 表示当前边为源节点的第src_output_条边。源节点可能会有多条输出边
      int src_output_;

      // 表示当前边为目标节点的第dst_input_条边。目标节点可能会有多条输入边。
      int dst_input_;
};

</code></pre>
<p>Edge既可以承载tensor数据，提供给节点Operation进行运算，也可以用来表示节点之间有依赖关系。对于表示节点依赖的边，其src_output_, dst_input_均为-1，此时边不承载任何数据。</p>
<h2 id="node">Node</h2>
<pre><code class="language-cpp">class Node {
 public:
    // NodeDef,节点算子Operation的信息，比如op分配到哪个设备上了，op的名字等，运行时有可能变化。
  	const NodeDef&amp; def() const;
    
    // OpDef, 节点算子Operation的元数据，不会变的。比如Operation的入参列表，出参列表等
  	const OpDef&amp; op_def() const;
 private:
  	// 输入边，传递数据给节点。可能有多条
  	EdgeSet in_edges_;

  	// 输出边，节点计算后得到的数据。可能有多条
  	EdgeSet out_edges_;
}
</code></pre>
<p>创建Node时不需要new OpDef，只需要从OpDef仓库中取出即可。因为元信息是确定的，比如Operation的入参个数等。</p>
<p>由Node和Edge，即可以组成图Graph，通过任何节点和任何边，都可以遍历完整图。Graph执行计算时，按照拓扑结构，依次执行每个Node的op计算，最终即可得到输出结果。入度为0的节点，也就是依赖数据已经准备好的节点，可以并发执行，从而提高运行效率。</p>
<p>系统中存在默认的Graph，初始化Graph时，会添加一个Source节点和Sink节点。Source表示Graph的起始节点，Sink为终止节点。Source的id为0，Sink的id为1，其他节点id均大于1.</p>
<h1 id="参考资料">参考资料</h1>
<p><a href="https://blog.csdn.net/u013510838/article/details/84139986">Graph_谢杨易的博客-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/u013510838/article/details/84111031">Session_谢杨易的博客-CSDN博客</a></p>
]]></content>
    </entry>
</feed>