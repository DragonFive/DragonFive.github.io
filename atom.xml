<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://DragonFive.github.io/</id>
    <title>dragon</title>
    <updated>2021-09-04T11:45:41.956Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://DragonFive.github.io/"/>
    <link rel="self" href="https://DragonFive.github.io/atom.xml"/>
    <subtitle>邮箱(base64)：MTY5MDMwMjk2M0BxcS5jb20=
</subtitle>
    <logo>https://DragonFive.github.io/images/avatar.png</logo>
    <icon>https://DragonFive.github.io/favicon.ico</icon>
    <rights>All rights reserved 2021, dragon</rights>
    <entry>
        <title type="html"><![CDATA[快手bagua使用教程翻译与摘抄]]></title>
        <id>https://DragonFive.github.io/post/kuai-shou-bagua-shi-yong-jiao-cheng-fan-yi-yu-zhai-chao/</id>
        <link href="https://DragonFive.github.io/post/kuai-shou-bagua-shi-yong-jiao-cheng-fan-yi-yu-zhai-chao/">
        </link>
        <updated>2021-09-01T05:32:43.000Z</updated>
        <content type="html"><![CDATA[<p>翻译自<a href="https://bagua-tutorials.kwai-seattle.com/introduction">八卦文档</a></p>
<p>八卦已经整合的原语包括</p>
<ul>
<li>集中式同步通信（AllReduce）</li>
<li>去中心化同步通讯</li>
<li>低精度通信</li>
</ul>
<p>其有效性已经在各种场景和模型中得到验证，包括ImageNet上的VGG和ResNet，Bert Large，以及快手等多个大规模工业应用：</p>
<ul>
<li>支持数十个TB参数模型训练的推荐系统，</li>
<li>超过 10 亿个图像/视频的视频/图像理解，</li>
<li>ASR 与 TB 级数据集，</li>
<li>等等。</li>
</ul>
<h1 id="使用举例">使用举例</h1>
<p>使用bagua类似于使用其他分布式训练库，如 PyTorch DDP 和 Horovod。</p>
<pre><code class="language-cpp">def main():
    args = parse_args()
    # define your model and optimizer
    model = MyNet().to(args.device)
    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr)
    # transform to Bagua wrapper
    from bagua.torch_api.algorithms import gradient_allreduce
    model = model.with_bagua(
        [optimizer], gradient_allreduce.GradientAllReduceAlgorithm()
    )

    # train the model over the dataset
    for epoch in range(args.epochs):
        for b_idx, (inputs, targets) in enumerate(train_loader):
            outputs = model(inputs)
            loss = torch.nn.CrossEntropyLoss(outputs, targets)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

</code></pre>
<p>计算使用的是pytorch, 只需要包上一个通信算法原语就可以了。</p>
<h1 id="通信算法">通信算法</h1>
<p>八卦依靠分布式学习算法的多样性而蓬勃发展。 系统的极大灵活性使得可以平滑地结合各种SOTA算法，同时在执行过程中提供性能的自动优化。 对于最终用户，八卦提供了广泛的算法选择，她可以轻松地尝试执行她的任务。 对于算法开发人员来说，八卦是一个游乐场，在那里她可以只专注于算法本身（例如，逻辑和控制），而无需在不同的算法之间重新发明轮子（例如，通信原语和系统优化）。</p>
<h2 id="梯度-allreduce">梯度 allreduce</h2>
<h3 id="总览">总览</h3>
<p>Gradient AllReduce 算法是一种流行的同步数据并行分布式算法。 它是大多数现有解决方案中实现的算法，比如  <a href="https://pytorch.org/tutorials/intermediate/ddp_tutorial.html">PyTorch DistributedDataParallel</a>, <a href="https://horovod.ai/">Horovod</a>, 以及<a href="https://www.tensorflow.org/guide/distributed_training">TensorFlow Mirrored Strategy</a>。</p>
<h3 id="算法">算法</h3>
<p>使用此算法，每个worker在每次迭代中执行以下步骤。</p>
<ul>
<li>使用小批量计算梯度。</li>
<li>使用 AllReduce 集合计算所有worker的梯度平均值。</li>
<li>用平均梯度更新模型。</li>
</ul>
<p>在八卦中，这个算法是通过 GradientAllReduce 算法类来支持的。 八卦中 GradientAllReduce 实现的默认性能应该与 PyTorch DDP 相当，并且在大多数情况下比 Horovod 更快。 八卦支持额外的优化，例如可以在实例化 GradientAllReduce 类时配置的分层通信。 在某些情况下，例如当机器间网络成为瓶颈时，它们可以使八卦比其他实现更快。</p>
<h3 id="举例">举例</h3>
<p>运行 Gradient AllReduce 的完整示例可以在带有 --algorithm gradient_allreduce 命令行参数的 <a href="https://github.com/BaguaSys/examples/blob/main/benchmark/synthetic_benchmark.py">八卦示例</a>中找到。</p>
<p>您需要初始化八卦算法（请参阅 <a href="https://bagua.readthedocs.io/en/latest/autoapi/bagua/torch_api/algorithms/gradient_allreduce/index.html">API 文档</a>了解您可以自定义哪些参数）：</p>
<pre><code class="language-python">from bagua.torch_api.algorithms import gradient_allreduce
algorithm = gradient_allreduce.GradientAllReduceAlgorithm()
</code></pre>
<p>然后用以下方法装饰您的模型：</p>
<pre><code class="language-python">model = model.with_bagua([optimizer], algorithm)
</code></pre>
<h2 id="bytegrad">bytegrad</h2>
<h3 id="总览-2">总览</h3>
<p>大规模分布式训练需要大量的通信成本，对于大型模型尤其如此。 例如，在使用 AllReduce 同步梯度的传统同步分布式设置中（许多库都是这种情况，比如  <a href="https://github.com/horovod/horovod">Horovod</a> 和 <a href="https://pytorch.org/tutorials/intermediate/ddp_tutorial.html">PyTorch DDP</a>), 在训练过程的每次迭代中，需要在每个 worker 上发送和接收大小等于模型大小的梯度。 这样的通信成本很快成为很多场景下的训练瓶颈。</p>
<p>有很多关于如何应用模型/梯度压缩来节省这种通信成本的<a href="https://awesomeopensource.com/project/chester256/Model-Compression-Papers?categoryPage=21">现有论文</a>。 八卦提供了一种内置的梯度压缩算法，称为 ByteGrad，它在通信之前将梯度浮点数压缩到 8bit 字节。 这节省了原始成本的 3/4。 它通过优化的 CUDA 内核和分层通信实现了高精度的 min-max 量化算子。 这使得它比现有框架中(比如  <a href="https://pytorch.org/docs/stable/ddp_comm_hooks.html#powersgd-communication-hook">PyTorch PowerSGD</a>)的其他压缩实现快得多（在我们的基准测试中大约快 50%）。并且在相同的时期数 ByteGrad 在大多数任务上收敛近似于全精度算法。</p>
<h3 id="算法-2">算法</h3>
<p>ByteGrad 在每次迭代中执行以下步骤。 假设我们有 m 个节点，每个节点有 n 个 GPU。</p>
<ol>
<li>为所有<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow><annotation encoding="application/x-tex">i,j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.85396em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">i</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.05724em;">j</span></span></span></span> 计算第 i 个节点的第 j 个 GPU 上的梯度 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>g</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">g_{i,j}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.716668em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span></li>
<li>每个节点上的第一个 GPU 做一个 reduce 操作来计算同一节点上所有 GPU 的梯度的平均值，定义为第 i 个节点的 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>G</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">G_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">G</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></li>
<li>第 i 个节点上的第一个 GPU 使用量化函数 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>Q</mi><mo>(</mo><mo>⋅</mo><mo>)</mo><mi mathvariant="normal">：</mi><mi>Q</mi><mo>(</mo><msub><mi>G</mi><mi>i</mi></msub><mo>)</mo></mrow><annotation encoding="application/x-tex">Q(⋅)：Q(G_i )</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">Q</span><span class="mopen">(</span><span class="mord">⋅</span><span class="mclose">)</span><span class="mord cjk_fallback">：</span><span class="mord mathdefault">Q</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">G</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> 对所有 i 量化梯度 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>G</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">G_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">G</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>。 然后每个节点在节点之间交换量化版本，使得每个节点都有所有 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>Q</mi><mo>(</mo><msub><mi>G</mi><mi>i</mi></msub><mo>)</mo></mrow><annotation encoding="application/x-tex">Q(G_i )</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">Q</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">G</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> 的平均值</li>
<li>每个节点上的第一个 GPU 将所有 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>Q</mi><mo>(</mo><msub><mi>G</mi><mi>i</mi></msub><mo>)</mo></mrow><annotation encoding="application/x-tex">Q(G_i)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">Q</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">G</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> 的平均值广播给同一节点上的所有其他 GPU，并且所有 worker 上的所有 GPU 都使用此量化平均值来更新模型</li>
</ol>
<p>量化函数 Q(⋅) 计算其输入的最小值 x 和最大值 y，并将 [x,y] 拆分为均匀间隔的 256 个区间。 然后用一个 8 位整数表示其输入的每个元素，表示原始元素在哪个区间。</p>
<h3 id="用法举例">用法举例</h3>
<p>您需要初始化八卦算法（请参阅 <a href="https://bagua.readthedocs.io/en/latest/autoapi/bagua/torch_api/algorithms/bytegrad/index.html">API 文档</a>了解您可以自定义哪些参数）：</p>
<pre><code class="language-python">from bagua.torch_api.algorithms import bytegrad
algorithm = bytegrad.ByteGradAlgorithm()
model = model.with_bagua([optimizer], algorithm)
</code></pre>
<h2 id="去中心化sgd">去中心化SGD</h2>
<h3 id="去中心化训练概述">去中心化训练概述</h3>
<p>Decentralized SGD 是一种数据并行的分布式学习算法，它消除了所有worker之间集中式全局模型的要求，这使得它在通信模式上与基于 Allreduce 或基于参数服务器的算法有很大不同。 使用去中心化 SGD，每个 worker 只需要与一个或几个特定的 worker 交换数据，而不是全局聚合数据。 因此，去中心化通信的通信连接数比 Allreduce 少得多，通信开销比 Parameter Server 更均衡。 尽管去中心化 SGD 可能会导致每个worker的模型不同，但理论上已经证明，去中心化 SGD 算法的收敛速度与其中心化对应版本相同。 您可以在我们的 <a href="https://arxiv.org/abs/1705.09056">论文</a>中找到关于去中心化 SGD 的详细分析 。</p>
<h3 id="去中心化训练算法">去中心化训练算法</h3>
<p>目前，不时有许多分散的训练算法被提出。 这些令人惊叹的作品集中在分散训练的不同方面，如<strong>peer选择、数据压缩、异步</strong>等，并提供了许多有前景的见解。 到目前为止，八卦已经整合了两种基本的去中心化算法，即去中心化 SGD 和低精度去中心化 SGD。 凭借八卦对去中心化的自动系统支持，我们预计在不久的将来会实现越来越多的去中心化算法。</p>
<h3 id="decentralized-sgd">Decentralized SGD</h3>
<p>现在我们将描述在八卦中实现的去中心化 SGD 算法。 假设worker数量为n，worker i上的模型参数为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>x</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mo separator="true">,</mo><mi>i</mi><mo>∈</mo><mrow><mn>0</mn><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><mi>n</mi><mi mathvariant="normal">−</mi><mn>1</mn></mrow></mrow><annotation encoding="application/x-tex">x^{(i)},i∈{0,...,n−1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0824399999999998em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathdefault mtight">i</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">i</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8388800000000001em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">n</span><span class="mord">−</span><span class="mord">1</span></span></span></span></span>。 每个worker 都能够直接从任何其他worker发送或接收数据。 在每次迭代 t 中，算法重复以下步骤：</p>
<ol>
<li>每个worker i 计算迭代的局部梯度 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msubsup><mi>g</mi><mi>t</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup></mrow><annotation encoding="application/x-tex">g_t^{(i)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2905559999999998em;vertical-align:-0.24575599999999992em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em;"><span style="top:-2.454244em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span><span style="top:-3.2197999999999998em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathdefault mtight">i</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.24575599999999992em;"><span></span></span></span></span></span></span></span></span></span>.</li>
<li>将本地模型与其选定的同伴的模型进行平均（表示为 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msubsup><mi>x</mi><mi>t</mi><mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow></msubsup></mrow><annotation encoding="application/x-tex">x_t^{(j)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2905559999999998em;vertical-align:-0.24575599999999992em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em;"><span style="top:-2.454244em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span><span style="top:-3.2197999999999998em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.24575599999999992em;"><span></span></span></span></span></span></span></span></span></span>), <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msubsup><mi>x</mi><mfrac><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mn>2</mn></mfrac><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup><mo>=</mo><mfrac><mrow><mi>t</mi><mo>(</mo><mi>i</mi><mo>)</mo><mi mathvariant="normal">​</mi><mo>+</mo><mi>x</mi><mi>t</mi><mo>(</mo><mi>j</mi><mo>)</mo></mrow><mn>2</mn></mfrac></mrow><annotation encoding="application/x-tex">x_{\frac{t+1}{2}}^{(i)}=\frac{t(i)​+xt(j)}{2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.6918199999999999em;vertical-align:-0.6470199999999999em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em;"><span style="top:-2.59378em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mopen nulldelimiter sizing reset-size3 size6"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8443142857142857em;"><span style="top:-2.656em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span><span style="top:-3.2255000000000003em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line mtight" style="border-bottom-width:0.049em;"></span></span><span style="top:-3.384em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.344em;"><span></span></span></span></span></span><span class="mclose nulldelimiter sizing reset-size3 size6"></span></span></span></span></span><span style="top:-3.5198em;margin-right:0.05em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathdefault mtight">i</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.6470199999999999em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.355em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.01em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.485em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mopen mtight">(</span><span class="mord mathdefault mtight">i</span><span class="mclose mtight">)</span><span class="mord mtight">​</span><span class="mbin mtight">+</span><span class="mord mathdefault mtight">x</span><span class="mord mathdefault mtight">t</span><span class="mopen mtight">(</span><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>.</li>
<li>使用局部梯度更新平均模型.<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msubsup><mi>x</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup><mo>=</mo><msubsup><mi>x</mi><mfrac><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mn>2</mn></mfrac><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup><mo>−</mo><mi>γ</mi><msubsup><mi>g</mi><mi>t</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup></mrow><annotation encoding="application/x-tex">x_{t+1}^{(i)}=x_{\frac{t+1}{2}}^{(i)}-\gamma g_t^{(i)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.3694389999999999em;vertical-align:-0.3246389999999999em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em;"><span style="top:-2.433692em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2198em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathdefault mtight">i</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3246389999999999em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.6918199999999999em;vertical-align:-0.6470199999999999em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em;"><span style="top:-2.59378em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mopen nulldelimiter sizing reset-size3 size6"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8443142857142857em;"><span style="top:-2.656em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span><span style="top:-3.2255000000000003em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line mtight" style="border-bottom-width:0.049em;"></span></span><span style="top:-3.384em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.344em;"><span></span></span></span></span></span><span class="mclose nulldelimiter sizing reset-size3 size6"></span></span></span></span></span><span style="top:-3.5198em;margin-right:0.05em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathdefault mtight">i</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.6470199999999999em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.2905559999999998em;vertical-align:-0.24575599999999992em;"></span><span class="mord mathdefault" style="margin-right:0.05556em;">γ</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em;"><span style="top:-2.454244em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span><span style="top:-3.2197999999999998em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathdefault mtight">i</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.24575599999999992em;"><span></span></span></span></span></span></span></span></span></span></li>
</ol>
<p>在步骤 2 中，我们采用一种策略为每次迭代中的每个 worker 选择一个 peer，这样所有 worker 都正确配对并且数据交换是有效的，因为每个 worker 可以在迭代之间与不同的 peer 交换数据。 简而言之，我们的策略将worker 平均分成两组，并在两组之间动态配对工worker，每次迭代都不同。</p>
<blockquote>
<p>点评，最后以谁的参数作为最终的ckpt呢？</p>
</blockquote>
<h3 id="通信开销">通信开销</h3>
<p>去中心化 SGD 的通信开销与网络程度高度相关，即一个worker与其他worker的连接数。 不同的拓扑或策略会导致不同程度的网络。 很明显，我们之前描述的Decentralized SGD算法的网络度为1。因此，在每次迭代中，一个worker只需要与一个worker建立一个连接来交换模型大小的1倍。 我们比较了不同通信模式在最繁忙节点的延迟和带宽方面的通信复杂性。</p>
<table>
<thead>
<tr>
<th style="text-align:center">算法</th>
<th style="text-align:center">时延复杂度</th>
<th style="text-align:center">带宽复杂度</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Allreduce (Ring)</td>
<td style="text-align:center">O(n)</td>
<td style="text-align:center">O(1)</td>
</tr>
<tr>
<td style="text-align:center">ps</td>
<td style="text-align:center">O(1)</td>
<td style="text-align:center">O(n)</td>
</tr>
<tr>
<td style="text-align:center">八卦去中心化SGD</td>
<td style="text-align:center">O(1)</td>
<td style="text-align:center">O(1)</td>
</tr>
</tbody>
</table>
<h3 id="用法举例-2">用法举例</h3>
<p>您需要初始化八卦算法（请参阅 <a href="https://bagua.readthedocs.io/en/latest/autoapi/bagua/torch_api/algorithms/decentralized/index.html">API 文档</a>了解您可以自定义哪些参数）：</p>
<pre><code class="language-python">from bagua.torch_api.algorithms import decentralized algorithm = decentralized.DecentralizedAlgorithm()
model = model.with_bagua([optimizer], algorithm)
</code></pre>
<h3 id="其他的算法">其他的算法</h3>
<p>八卦提供的其他的算法没什么好说的，可以参考<a href="https://bagua-tutorials.kwai-seattle.com/algorithms/">原文</a></p>
<p>八卦还支持<a href="https://baguasys.github.io/tutorials/elastic-training/index.html">弹性训练</a>, 看上去是保存中间状态实现的。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[快手的八卦：bagua论文翻译与赏析]]></title>
        <id>https://DragonFive.github.io/post/kuai-shou-de-ba-gua-bagua-lun-wen-fan-yi-yu-shang-xi/</id>
        <link href="https://DragonFive.github.io/post/kuai-shou-de-ba-gua-bagua-lun-wen-fan-yi-yu-shang-xi/">
        </link>
        <updated>2021-08-31T11:21:16.000Z</updated>
        <content type="html"><![CDATA[<p>《八卦-使用系统松弛（Relaxations）扩展分布式学习</p>
<blockquote>
<p>点评：这篇论文的作者中的后面发了很多论文，快手人才济济呀</p>
</blockquote>
<h1 id="摘要">摘要</h1>
<p>近年来，用于分布式数据并行训练的系统列表越来越多。 现有系统在很大程度上适合两种范式，即参数服务器和 MPI 风格的集体操作。 在算法方面，研究人员提出了广泛的技术来通过“系统松弛”降低通信：<strong>量化、去中心化和通信延迟</strong>。然而，大多数（如果不是全部）现有系统仅依赖于基于标准同步和异步随机梯度 (SG) 的优化，因此无法利用机器学习社区最近开发的所有可能的优化。鉴于当前系统和理论之间出现的这种差距，我们构建了 BAGUA，<strong>这是一个通信框架</strong>，其设计目标是提供一个既灵活又模块化的系统抽象，以支持最先进的分布式训练系统松弛技术 . 在新系统设计的支持下，BAGUA 具有强大的实现和扩展各种最先进的分布式学习算法的能力。 在拥有多达 16 台机器（128 个 GPU）的生产集群中，BAGUA 可以在端到端的训练时间内在各种任务中以显着的优势（高达 1.95 倍）优于 PyTorch-DDP、Horovod 和 BytePS。 此外，我们进行了严格的权衡探索，表明不同的算法和系统松弛在不同的网络条件下实现了最佳性能。</p>
<h1 id="1-引言">1. 引言</h1>
<p>分布式机器学习系统不断提高的可扩展性和性能一直是机器学习技术快速发展的主要推动力之一。模型质量的每一次飞跃都是通过模型大小和可以训练模型的数据量的增长以及计算量的快速增加来实现的。这种改进的背后是两个主要的促成因素：硬件加速（例如 GPU 和 TPU）和高效且可扩展的分布式训练算法的开发。可以说可扩展的分布式训练系统是现代深度学习技术的基石。</p>
<p><em>表一：不同的系统松弛技术</em></p>
<table>
<thead>
<tr>
<th style="text-align:center">论文</th>
<th style="text-align:center">同步</th>
<th style="text-align:center">精确度</th>
<th style="text-align:center">集中式</th>
<th style="text-align:center">PyTorch-DDP</th>
<th style="text-align:center">Horovod</th>
<th style="text-align:center">BytePS</th>
<th style="text-align:center">BAGUA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">limu ps</td>
<td style="text-align:center">同步</td>
<td style="text-align:center">全精度</td>
<td style="text-align:center">中心</td>
<td style="text-align:center">v</td>
<td style="text-align:center">v</td>
<td style="text-align:center">v</td>
<td style="text-align:center">v</td>
</tr>
<tr>
<td style="text-align:center"><a href="https://arxiv.org/abs/1902.00340">arXiv:1902.00340</a></td>
<td style="text-align:center">同步</td>
<td style="text-align:center">全精度</td>
<td style="text-align:center">去中心</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">v</td>
</tr>
<tr>
<td style="text-align:center"><a href="https://arxiv.org/abs/1610.02132">arXiv:1610.02132</a></td>
<td style="text-align:center">同步</td>
<td style="text-align:center">低精度</td>
<td style="text-align:center">中心</td>
<td style="text-align:center">v</td>
<td style="text-align:center">v</td>
<td style="text-align:center">v</td>
<td style="text-align:center">v</td>
</tr>
<tr>
<td style="text-align:center"><a href="https://arxiv.org/abs/1809.07599">arXiv:1809.07599</a></td>
<td style="text-align:center">同步</td>
<td style="text-align:center">低精度</td>
<td style="text-align:center">去中心</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">v</td>
</tr>
<tr>
<td style="text-align:center"><a href="https://arxiv.org/abs/1803.06443">arXiv:1803.06443</a></td>
<td style="text-align:center">异步</td>
<td style="text-align:center">全精度</td>
<td style="text-align:center">中心</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">v</td>
<td style="text-align:center">v</td>
</tr>
<tr>
<td style="text-align:center"><a href="https://arxiv.org/abs/1609.08326">arXiv:1609.08326</a></td>
<td style="text-align:center">异步</td>
<td style="text-align:center">全精度</td>
<td style="text-align:center">去中心</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">v</td>
</tr>
<tr>
<td style="text-align:center"><a href="https://arxiv.org/abs/1710.06952">arXiv:1710.06952</a></td>
<td style="text-align:center">异步</td>
<td style="text-align:center">低精度</td>
<td style="text-align:center">中心</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">v</td>
</tr>
<tr>
<td style="text-align:center">无</td>
<td style="text-align:center">异步</td>
<td style="text-align:center">低精度</td>
<td style="text-align:center">去中心</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
</tbody>
</table>
<blockquote>
<p>点评：这里对于各项技术的综述视角不错，值得学习</p>
</blockquote>
<h3 id="数据并行训练系统的现状">数据并行训练系统的现状:</h3>
<p>在本文中，我们将自己的范围定为并专注于数据并行训练，这是最流行的分布式训练范式之一，其中数据集在不同的工作人员之间进行分区，并且模型适合单个设备。 毫不奇怪，最近几年见证了越来越多的分布式数据并行训练系统。现有系统适合<strong>两个范式</strong>：ps(遵循 Li mu等人所做的开创性工作<a href="http://web.eecs.umich.edu/~mosharaf/Readings/Parameter-Server.pdf">ps</a>) 和 使用 MPI 集体操作（Sergeev等人<a href="https://arxiv.org/abs/1802.05799">horovod</a>），例如 Allreduce。两种范式都支持工业规模的分布式训练系统。基于ps 的有 Adam (Microsoft) , TensorFlow (Google) , Poseidon (Petuum) , Angel (Tencent) , and BytePS (ByteDance)。基于 MPI 风格的有：PyTorch-DDP (Facebook) , Mariana (Tencent) , MALT (NEC Labs) , NCCL (NVIDIA) , and Horovod (Uber)。这些系统通常涉及机器学习、系统和数据管理社区的共同努力，并成功地使分布式训练变得更容易和更具可扩展性。</p>
<blockquote>
<p>点评：byteps 是ps模式，还是allreduce模式在知乎上也引发正义</p>
</blockquote>
<h3 id="数据并行训练算法的现状">数据并行训练算法的现状</h3>
<p>在理论和算法方面，研究人员也一直在积极改进基于标准同步和异步随机梯度 (SG) 算法的性能。 注意到一个主要的系统瓶颈是通信，研究人员提出了一系列技术来降低通信开销，主要是通过“放松”通信的某些方面。比如包括 (1) 通信压缩（例如，量化、稀疏化和误差补偿 ），(2) 通信分散化和 (3) 通信延迟（例如 LocalSGD）和异步 。这些技术针对不同的工作负载和不同的网络条件进行了优化。 这些技术在<strong>带宽和延迟方面显着降低通信开销</strong>，或增加对落后者存在的容忍度方面有很好的保障。</p>
<h3 id="系统与理论之间新出现的鸿沟">系统与理论之间新出现的鸿沟</h3>
<p>在本文中，我们受到当前系统和理论景观之间新出现的差距的启发。尽管分布式学习理论和算法在系统松弛方面取得了最新进展，但大多数（如果不是全部）现有系统仅依赖于标准的同步和异步基于随机梯度 (SG) 的算法。 主要后果是现有系统没有利用机器学习社区一直在开发的所有可能的优化，并且许多实际应用程序可能会进一步加速。在本文中，我们问：我们能否通过通信系统松弛来进一步加速分布式学习系统？ 如果是这样，为此目的正确的系统抽象是什么？</p>
<h3 id="技术挑战">技术挑战</h3>
<p>要缩小这一差距需要的不仅仅是简单地使用这些算法来实现从现有系统中抽象出参数服务器和 Allreduce。有两个挑战。首先，在参数服务器或 Allreduce 范式中直接和自然地支持这些系统松弛是具有挑战性的。例如，使用参数服务器提供的 put/get 抽象来支持需要服务器端内存和状态的算法是具有挑战性的，这是大多数使用错误补偿的通信压缩算法所需要的。 同样，两种范式都很难支持去中心化通信。因此，必须从根本上重新审视系统抽象的设计，以支持当今许多松弛算法。其次，我们需要支持模块化系统抽象和优化来处理这些系统松弛的多样性。当 Horovod 和 BytePS 等现有系统针对性能进行优化时，它们通常会关注基于 教科书式的随机梯度算法的通信模式。当我们希望支持大量训练算法时，如表 1 所示，我们无法单独优化每个算法； 相反，我们必须了解如何在通用框架中自动优化这组不同的算法。</p>
<h3 id="bagua-系统和我们的贡献">BAGUA 系统和我们的贡献</h3>
<p>受这两个挑战的启发，我们构建了 BAGUA，这是一个通信框架，其设计目标是支持分布式训练的最先进系统松弛技术。 我们做出了两项技术贡献：</p>
<ul>
<li>我们的第一个贡献是 BAGUA 的系统设计，它为通信提供了模块化设计。 我们的抽象囊括了参数服务器和 Allreduce 范式，并提供了一系列 MPI 风格的集体操作，以促进具有不同精度和集中化策略的通信。<br>
这种抽象非常灵活和模块化，足以支持许多算法，如表 1 所示。此外，我们还开发了一个简单的自动优化框架，可以加速在 BAGUA 框架内实现的算法。 该框架背后的关键是通信的自动batching和调度。 与之前的工作，如 Horovod 和 BytePS 不同，我们的优化框架可以更广泛地应用，超出基于标准 SG 的算法</li>
<li>我们的第二个贡献是围绕两个假设进行的广泛的实证研究：(1) 通过支持不同的系统松弛技术，BAGUA 能够通过现实世界的基础设施为现实世界的应用程序和工作负载提供比现有系统显著的改进； (2) 通过支持多种系统松弛，BAGUA 能够在不同的网络条件下提供可扩展的 ML 训练，以允许用户选择不同的算法。为此，我们对两者进行了大规模的实证研究在快手公司运行的基准测试任务和实际应用程序。 在具有多达 16 台机器（总共 128 个 GPU，使用 Tensor Core 聚合 2 petaFLOPS）的集群上，我们按照 V100 GPU 机器 (p3.8xlarge, p3. 16xlarge、p3dn.24xlarge) 在 AWS 上连接：10Gbps、25Gbps 和 100Gbps，使用 TCP/IP 连接。在各种任务中，BAGUA 的性能明显优于 BytePS 、Horovod 和 PyTorch-DDP（对于 10Gbps 可达 1.9 倍，对于 100Gbps 可达 1.34 倍）。 此外，我们进行了严格的权衡探索，表明不同的算法和系统松弛可以在不同的网络条件下实现最佳性能。 这说明了向最终用户提供这种不同的算法队列的重要性。</li>
</ul>
<h3 id="局限和进步">局限和进步</h3>
<p>当前的 BAGUA 系统存在一些局限性，我们希望我们在构建 BAGUA 方面所做的努力可以帮助和启发未来在这些令人兴奋的方向上的研究。首先，BAGUA 没有提供一种原则性的方法来帮助用户自动选择最合适的系统松弛来应用。 在 BAGUA 为所有这些算法提供支持之后，一个令人兴奋的方向是了解如何构建一个有原则的自动调整系统。 其次，当前版本的 BAGUA 只关注数据并行性，集成其他技术（例如模型并行性和流水线并行性）并理解系统抽象是有趣的未来工作。</p>
<h3 id="大纲">大纲</h3>
<p>大纲 本文的其余部分组织如下。 我们首先简要回顾第 2 节中的数据并行训练和现有系统的优化框架，作为准备工作和相关工作。 我们在第 3 节中讨论了 BAGUA 的设计和优化。我们在第 4 节中描述了我们的实验研究，并在第 5 节中总结。</p>
<h1 id="2-前期工作及相关工作">2. 前期工作及相关工作</h1>
<p>BAGUA 建立在数十年关于分布式机器学习系统和算法的研究之上。 其中很多来自数据库社区。 我们现在总结相关工作并详细讨论一些以提供背景和背景。 我们建议读者参考 <a href="https://arxiv.org/abs/2104.05245">Distributed Learning Systems with First-order Methods</a> 对不同系统松弛算法的严格理论分析。</p>
<h2 id="21-基于数据并行sg的算法">2.1 基于数据并行SG的算法</h2>
<p>分布式学习系统的基石是基于数据并行随机梯度（DP-SG）的算法，这是现有系统支持和优化的主导算法。 设 D 是一个数据集，n 是worker 的数量，每个worker  i 持有它在步骤 t 的数据 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msubsup><mi>D</mi><mi>i</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msubsup></mrow><annotation encoding="application/x-tex">D^{(t)}_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.321664em;vertical-align:-0.27686399999999994em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em;"><span style="top:-2.4231360000000004em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span><span style="top:-3.2198em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathdefault mtight">t</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.27686399999999994em;"><span></span></span></span></span></span></span></span></span></span> 和模型副本的分区：<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msubsup><mi>x</mi><mi>i</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msubsup></mrow><annotation encoding="application/x-tex">x^{(t)}_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.321664em;vertical-align:-0.27686399999999994em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em;"><span style="top:-2.4231360000000004em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span><span style="top:-3.2198em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathdefault mtight">t</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.27686399999999994em;"><span></span></span></span></span></span></span></span></span></span>。 设 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msubsup><mi>g</mi><mi>i</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msubsup></mrow><annotation encoding="application/x-tex">g^{(t)}_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.321664em;vertical-align:-0.27686399999999994em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em;"><span style="top:-2.4231360000000004em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span><span style="top:-3.2198em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathdefault mtight">t</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.27686399999999994em;"><span></span></span></span></span></span></span></span></span></span> 是第 t 步worker i 的随机梯度，教科书 DP-SG 更新worker i 处的每个本地模型副本，如下所示：</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msubsup><mi>x</mi><mi>i</mi><mrow><mo>(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo>)</mo></mrow></msubsup><mo>=</mo><msubsup><mi>x</mi><mi>i</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msubsup><mi mathvariant="normal">−</mi><mi>γ</mi><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msubsup><mi>g</mi><mi>j</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msubsup></mrow><annotation encoding="application/x-tex">x^{(t+1)}_i = x^{(t)}_i − γ \sum^n_{j=1} g^{(t)}_j
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.321664em;vertical-align:-0.276864em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em;"><span style="top:-2.4231360000000004em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span><span style="top:-3.2198em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathdefault mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.276864em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:3.0651740000000007em;vertical-align:-1.4137769999999998em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em;"><span style="top:-2.4231360000000004em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span><span style="top:-3.2198em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathdefault mtight">t</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.276864em;"><span></span></span></span></span></span></span><span class="mord">−</span><span class="mord mathdefault" style="margin-right:0.05556em;">γ</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.6513970000000007em;"><span style="top:-1.872331em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.050005em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3000050000000005em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.4137769999999998em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em;"><span style="top:-2.4231360000000004em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span><span style="top:-3.2198em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathdefault mtight">t</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.412972em;"><span></span></span></span></span></span></span></span></span></span></span></p>
<p>其中γ是学习率。 为了实现这一点，所有机器都需要交换它们的局部梯度 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msubsup><mi>g</mi><mi>i</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msubsup></mrow><annotation encoding="application/x-tex">g^{(t)}_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.321664em;vertical-align:-0.27686399999999994em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em;"><span style="top:-2.4231360000000004em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span><span style="top:-3.2198em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathdefault mtight">t</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.27686399999999994em;"><span></span></span></span></span></span></span></span></span></span>，聚合并广播到所有机器。 当然，这可以通过标准的 Allreduce 通信模式来实现。</p>
<p>当有很多worker 或一些潜在的落后者时，可以将上述算法扩展到其异步对应物。 我们允许访问一些过时的版本(staled version)，而不是在迭代 t 时使用最新的梯度。</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msubsup><mi>x</mi><mi>i</mi><mrow><mo>(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo>)</mo></mrow></msubsup><mo>=</mo><msubsup><mi>x</mi><mi>i</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msubsup><mi mathvariant="normal">−</mi><mi>γ</mi><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msubsup><mi>g</mi><mi>j</mi><mrow><mo>(</mo><msubsup><mi>t</mi><mi>j</mi><mi>i</mi></msubsup><mo>)</mo></mrow></msubsup></mrow><annotation encoding="application/x-tex">x^{(t+1)}_i = x^{(t)}_i − γ \sum^n_{j=1} g^{(t^i_j)}_j
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.321664em;vertical-align:-0.276864em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em;"><span style="top:-2.4231360000000004em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span><span style="top:-3.2198em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathdefault mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.276864em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:3.0651740000000007em;vertical-align:-1.4137769999999998em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em;"><span style="top:-2.4231360000000004em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span><span style="top:-3.2198em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathdefault mtight">t</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.276864em;"><span></span></span></span></span></span></span><span class="mord">−</span><span class="mord mathdefault" style="margin-right:0.05556em;">γ</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.6513970000000007em;"><span style="top:-1.872331em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.050005em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3000050000000005em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.4137769999999998em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.2736799999999997em;"><span style="top:-2.4231360000000004em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span><span style="top:-3.3422199999999997em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9020857142857143em;"><span style="top:-2.214em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span><span style="top:-2.931em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.42488571428571426em;"><span></span></span></span></span></span></span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.412972em;"><span></span></span></span></span></span></span></span></span></span></span></p>
<p>t 是计算worker j 梯度的前一次迭代，由worker i 在迭代 t 时访问。 理论上可以通过 异步SGD 实现线性加速。</p>
<h2 id="22-现有的分布式学习系统">2.2 现有的分布式学习系统</h2>
<p>分布式学习系统在过去十年中吸引了大量研究。 大多数现有系统，包括 DistBelief [63]、Adam [25]、SparkNet [64]、GeePS [65]、Poseidon [27]、Angel [28]、BytePS [29]、PyTorch-DDP [30]、Mariana [31] ]、MALT [32]、NCCL [33]、SINGA [66, 67]、Rafiki [68] 和 Horovod [24]，都专注于 DP-SG 算法或其异步对应算法的优化。 这些系统的设计基于两个基本问题：</p>
<ul>
<li>1.（抽象通信）应该如何通信和聚合梯度和模型？</li>
<li>2.（优化）如何通过平衡通信和计算来优化端到端执行？</li>
</ul>
<h3 id="通信抽象">通信抽象</h3>
<p>在通信抽象方面，现有系统分为两种范式：参数服务器（PS）和 Allreduce ，图 1 说明了这两种范式。 在参数服务器架构中，模型可以划分为分片并分布到多个节点（我们称这些节点为“参数服务器”）。 在训练阶段，worker 定期从 PS 中获取模型，利用 GPU 等计算单元进行前向和反向传播并将梯度推送到 PS，而 PS 聚合梯度并更新参数。使用 Allreduce 范式，所有worker 与他们的邻居合作进行模型/梯度交换。 现有系统通常采用环形拓扑 <a href="https://link.springer.com/chapter/10.1007/978-3-540-24685-5_1">Optimization of Collective Reduction Operations</a> 进行两阶段通信：首先，范式将模型/梯度划分为 n 个块（其中 n 是节点数），并使用 n 个具有不同起点和终点的环 指向聚合 n 个块； 其次，通过环广播位于不同节点的每个块的聚合结果。</p>
<figure data-type="image" tabindex="1"><img src="https://DragonFive.github.io//post-images/1630754817474.png" alt="" loading="lazy"></figure>
<figure data-type="image" tabindex="2"><img src="https://DragonFive.github.io//post-images/1630754865306.png" alt="" loading="lazy"></figure>
<p><em>图 2：DP-SG 的通信模式以及 Horovod、BytePS 和 PyTorch-DDP 如何优化此通信模式的执行</em></p>
<blockquote>
<p>点评：这个图需要关注一下</p>
</blockquote>
<p>用于两阶段通信的现有系统：首先，范式将模型/梯度划分为 n 个块（其中 n 是节点数），并使用具有不同起点和终点的 n 个环来聚合 n 个块； 其次，通过环广播位于不同节点的每个块的聚合结果。</p>
<h3 id="性能优化">性能优化</h3>
<p>在决定使用哪种通信范式之后，一个关键的设计是如何在计算过程中隐藏尽可能多的通信。 这通常是以前系统的核心技术组件。Horovod、BytePS 和 PyTorch-DDP。 这些系统通过开发不同的方法来平衡通信和计算来优化 DP-SG 通信模式。 关键的复杂性源于这样一个事实，即 DP-SG 的训练过程由不同层之间的微妙依赖及其自身的 (1) 前向传递、(2) 反向传递、(3) 梯度同步和 (4) 模型更新四个阶段组成 。</p>
<p>图 2（Vanilla）说明了 DP-SG 在四层模型上的简单实现。 一旦向后传递（蓝色）完成，系统就会为每一层传达梯度（绿色），并在所有层（粉色）完成所有通信后一次性更新所有层的模型。 然后系统开始下一次向前传递（黄色）。</p>
<p>PyTorch-DDP 和 Horovod 是两个基于 Allreduce 的系统，<strong>并通过将通信 (Allreduce) 与反向传递重叠</strong>并将多个<strong>梯度分桶为一个 Allreduce 操作来专门优化此流水线</strong>。 通过重叠，Allreduce 操作可以与梯度计算并行进行。 Allreduce 操作仅在存储桶中的所有梯度都准备就绪时触发。 分桶的直觉是，像 Allreduce 这样的集体通信在大张量上更有效。 所有的 Allreduce 操作完成后，模型将通过聚合梯度进行更新。BytePS 遵循参数服务器范例，以不同的方式优化了这个管道。</p>
<p><strong>BytePS 将每个梯度分割成大小相同的小块来进行 Push/Pull</strong>。 BytePS 重叠向前和向后推/拉。 它有一个调度器来维护梯度块的通信顺序。 其原理是，阻塞下一次前向传递执行的参数将优先进行通信。 一旦从服务器中提取了参数的所有梯度块，该参数将单独更新。 因此，下一次迭代的前向传递可能会与当前迭代的通信重叠。 在异步 DP-SG 方面，BytePS 通过允许每个worker单独更新服务器状态而不等待其他worker 来支持它。 而 PyTorch-DDP 和 Horovod 不支持异步通信，因为它们依赖于 Allreduce 运算符。</p>
<h2 id="23-分布式-dp-sg-的系统放松">2.3 分布式 DP-SG 的系统放松</h2>
<p>虽然现有系统主要关注同步和异步 DP-SG 算法，但研究界已经开发出一套多样化的技术来进一步优化通信的不同方面。 这些技术通常会导致不同的训练算法，从而导致不同的通信模式，如 DP-SG。鉴于这些差异，Horovod、BytePS 和 PyTorch-DDP 都没有提供对这些算法的系统支持，如表 1 所述。BAGUA 的目标是提供灵活的抽象来支持这些具有自动性能优化框架的不同训练算法，而无需 假设特定的通信模式，例如 DP-SG 之一。 提出了不同的策略来加速 DP-SG 中昂贵的参数交换阶段。 为了减少通信量，引入了有损通信压缩方法，如量化，稀疏化，素描（sketching）和误差补偿 . 为了摆脱延迟瓶颈，人们提出了分散式通信方法。 此外，还讨论了 localSGD 以优化训练期间的通信轮数。最后，值得一提的是，有些方法结合了上面列出的多种策略。</p>
<p><strong>示例</strong><br>
为了说明这些高级训练算法与普通 DP-SG 之间的通信模式的差异，以及仅考虑使用 DP-SG 进行优化的系统在以模块化和系统的方式支持这些新算法方面面临挑战的原因，我们取 QSGD 和去中心化低精度 SGD 的示例。 图 3 说明了 DP-SG、QSGD 和分散的低精度 SGD 的执行管道和通信模式。 与DP-SG相比，流水线的执行组件及其依赖关系可以在高级算法中改变。例如，DP-SG 中甚至不存在两种算法所需的组件“量化”，而去中心化低精度 SGD 中的“模型更新”组件需要在通信之前发生。 由于这些高级算法无法适应 DP-SG 通信模式，因此为 DP-SG 而生的系这些统处理这些算法具有挑战性。</p>
<figure data-type="image" tabindex="3"><img src="https://DragonFive.github.io//post-images/1630755592348.png" alt="" loading="lazy"></figure>
<p><em>图 3：BAGUA 自动优化的系统松弛训练算法的通信模式</em></p>
<blockquote>
<p>点评：</p>
</blockquote>
<h1 id="3-系统设计">3 系统设计</h1>
<p>BAGUA 的目标是提供一个灵活的框架来支持 DP-SG 之外的高级训练算法。 为了实现这一点，我们重新审视了控制以前系统设计的两个基本问题，而不假设 DP-SG 的模式：</p>
<ol>
<li>（抽象通信）应该如何通信和聚合梯度和模型？ 在 BAGUA 中，除了参数服务器和 Allreduce 之外，我们设计了一系列 MPI 风格的集体操作，以促进具有不同精度和中心化策略的通信。</li>
<li>（优化）如何通过平衡通信和计算来优化端到端执行？ 在 BAGUA 中，我们开发了一个简单但有效的自动优化框架，可用于优化在 BAGUA 中实现的算法的执行。</li>
</ol>
<p>这两个设计决策使 BAGUA 具有灵活性和效率——通过系统松弛实现新的高级算法（例如，1-big Adam 或去中心化 SGD ）在 BAGUA 中，开发人员无需担心手动平衡通信与计算； 相反，开发人员可以在高层指定逻辑语义，BAGUA 将自动优化其执行。 在本节中，我们首先提供高级系统概述，然后描述这些原语及其实现，然后是 BAGUA 中简单但有效的优化框架。</p>
<figure data-type="image" tabindex="4"><img src="https://DragonFive.github.io//post-images/1630755614240.png" alt="" loading="lazy"></figure>
<h2 id="31-系统总览">3.1 系统总览</h2>
<p>BAGUA 的目标是促进利用系统松弛的高效且可扩展的分布式训练算法的开发。 如图 4 所示，存在三个参与者：最终用户、优化算法和 BAGUA 运行时。</p>
<figure data-type="image" tabindex="5"><img src="https://DragonFive.github.io//post-images/1630755631672.png" alt="" loading="lazy"></figure>
<p><strong>终端用户</strong> 从最终用户的角度来看，使用 BAGUA 非常类似于在单台机器上使用 PyTorch 或 TensorFlow 进行训练，只需对其现有代码进行最少的更改。 最终用户应提供：（1）需要训练的神经网络模型，在 PyTorch 中指定为图形，以及（2）数据样本流。 然后，最终用户指定要使用的训练算法，例如 QSGD （通信压缩训练）、1 位 Adam 或 DecentralizedSGD，以及训练基础设施的信息，例如 机器数量以及是否应该使用 MPI 或 NCCL 进行通信。</p>
<p>BAGUA 的核心是一个训练算法，由开发人员使用 BAGUA 提供的通信原语和抽象实现。 算法将最终用户提供的神经网络作为输入，并为其配备特定于算法的通信功能。 具体来说，算法的开发者通过在不同的执行阶段将此通信函数注册为钩子来实现这一点。 一个例子是在每一层的反向计算之后注册一个钩子。 通信函数包含训练算法的核心逻辑，其签名如下：</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi><mo>(</mo><mo>(</mo><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>g</mi><mn>1</mn></msub><mo>)</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo>(</mo><msub><mi>x</mi><mi>n</mi></msub><mo separator="true">,</mo><msub><mi>g</mi><mi>n</mi></msub><mo>)</mo><mo>)</mo><mo>−</mo><mo>&gt;</mo><mo>(</mo><msubsup><mi>x</mi><mn>1</mn><mi>l</mi></msubsup><mo separator="true">,</mo><msubsup><mi>g</mi><mn>1</mn><mi>l</mi></msubsup><mo>)</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo>(</mo><msubsup><mi>x</mi><mi>n</mi><mi>l</mi></msubsup><mo separator="true">,</mo><msubsup><mi>g</mi><mi>n</mi><mi>l</mi></msubsup><mo>)</mo></mrow><annotation encoding="application/x-tex">f((x_1, g_1)...(x_n, g_n)) -&gt; (x^l_1 , g^l_1 )...(x^l_n , g^l_n )
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mclose">)</span><span class="mord">−</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.1491079999999998em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8991079999999998em;"><span style="top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8991079999999998em;"><span style="top:-2.4530000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8991079999999998em;"><span style="top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8991079999999998em;"><span style="top:-2.4530000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></p>
<p>其中 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><msub><mi>x</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>g</mi><mi>i</mi></msub><mo>)</mo></mrow><annotation encoding="application/x-tex">(x_i, g_i)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> 是第 i 台机器上的当前模型 (<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">x_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>) 和梯度 (<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>g</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">g_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>)， (<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msubsup><mi>x</mi><mi>i</mi><mi>l</mi></msubsup><mo separator="true">,</mo><msubsup><mi>g</mi><mi>i</mi><mi>l</mi></msubsup></mrow><annotation encoding="application/x-tex">x^l_i, g^l_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.107772em;vertical-align:-0.258664em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-2.441336em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.258664em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-2.441336em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.258664em;"><span></span></span></span></span></span></span></span></span></span>) 是第 i 台机器上的更新模型和梯度。为了实现通信功能，算法的开发者假设了一个类似 MPI 的执行模型。 关键的区别在于，开发者不仅配备了 MPI 中的标准通信原语（例如 Allreduce），还配备了 BAGUA 提供的一组通信原语。 这些原语支持系统放宽，例如带有错误补偿的压缩通信或分散式通信。</p>
<p>在 BAGUA 中实现通信功能时，开发者提供了这种功能的批处理版本，将一组层作为输入。 这允许 BAGUA 稍后自动批处理通信并优化其与计算的重叠。 当BAGUA调用该函数时，它会将所有层的参数重新排列到连续的内存空间中，并传入这些层的扁平版本，将它们视为单个变量。 算法开发者可以决定她的算法是否可以使用这个扁平化的版本，通过对所有层进行一次通信来避免对每一层进行通信。</p>
<p>BAGUA Runtime 对通信功能的每次调用（由注册的钩子触发）都向 BAGUA 注册，这为 BAGUA 提供了工作负载的全局视图，<strong>以实现自动调度和批处理。 BAGUA 的关键技术贡献是自动对计算和通信应用一系列优化</strong>。 为了实现这一点，BAGUA 的核心是执行优化器，它分两个阶段运行。</p>
<ol>
<li>分析阶段。 在梯度下降计算的第一次前向/后向传递期间，BAGUA 保留所有通信函数调用的日志，在没有任何优化的情况下执行它们。然后它会自动： (1. Bucketing) 将层分组到不同的桶中，它们的通信将同时发生； (2.Flattening)将同一组内所有层的所有模型和梯度重新排列到连续的内存空间中，以达到更好的局部性； (3. Scheduling) 调度何时进行每个桶的通信，与计算重叠。</li>
<li>执行阶段。 对于梯度下降计算的其余前向/后向传递，BAGUA 将在模型的自动优化版本上执行。 默认情况下，BAGUA 每个桶进行一次通信。</li>
</ol>
<blockquote>
<p>点评：这个自动优化应该是这篇论文为数不多的亮点。<br>
除了这个自动优化，其他就像是系统工程上的大杂烩</p>
</blockquote>
<h2 id="32-通信原语">3.2 通信原语</h2>
<p>BAGUA 的一个关键组件是一组通信原语。 所有这些算子都遵循类似于 MPI 的执行模型，将 n 个张量 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>x</mi><mn>1</mn></msub><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><msub><mi>x</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">x_1...x_n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>（可以存储参数、梯度等）作为输入，每个张量在不同的 worker 上，并输出新的数据产出 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msubsup><mi>x</mi><mn>1</mn><mi>l</mi></msubsup><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><msubsup><mi>x</mi><mi>n</mi><mi>l</mi></msubsup></mrow><annotation encoding="application/x-tex">x^l_1...x^l_n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.097216em;vertical-align:-0.24810799999999997em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-2.4518920000000004em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.24810799999999997em;"><span></span></span></span></span></span></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span></span></span></span> ，每个在不同的worker：</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>o</mi><mi>p</mi><mo>(</mo><msub><mi>x</mi><mn>1</mn></msub><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><msub><mi>x</mi><mi>n</mi></msub><mo>)</mo><mo>→</mo><mo>&gt;</mo><msubsup><mi>x</mi><mn>1</mn><mi>l</mi></msubsup><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><msubsup><mi>x</mi><mi>n</mi><mi>l</mi></msubsup></mrow><annotation encoding="application/x-tex">op(x_1...x_n) →&gt; x^l_1 ...x^l_n
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">o</span><span class="mord mathdefault">p</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">→</span></span><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.146108em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8991079999999998em;"><span style="top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8991079999999998em;"><span style="top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span></span></span></span></span></p>
<h3 id="集中式全精度">集中式全精度</h3>
<p>BAGUA 提供了一个简单的原语 C FP S，它提供与标准 Allreduce 运算符相同的功能。</p>
<p>我们使用这个符号来表示，CFPS 算子的作用是计算所有本地副本的总和 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mo>∑</mo><mi>j</mi></msub><msub><mi>x</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\sum_jx_j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.185818em;vertical-align:-0.43581800000000004em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.16195399999999993em;"><span style="top:-2.40029em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.43581800000000004em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> ，并使其可供所有worker 访问。</p>
<h3 id="集中式-低精度">集中式、低精度</h3>
<p>鉴于许多深度神经网络可以容忍其梯度的激进有损压缩，最近通信压缩引起了广泛的兴趣。BAGUA 为此提供了 CLPS 原语。 具体来说：</p>
<figure data-type="image" tabindex="6"><img src="https://DragonFive.github.io//post-images/1630755670795.png" alt="" loading="lazy"></figure>
<p>其中 Q 是有损压缩函数，由开发人员指定，CLPS 支持带有错误补偿的通用通信压缩形式。 请注意，将 δi 和 i 设置为 None 将禁用误差补偿并给出。直观上，δi 和 i 保留了上次迭代压缩引起的误差。 误差补偿方法引入的收敛效率对压缩非常稳健。 当压缩功能相对激进时，此技术特别有用。</p>
<h3 id="去中心化-全精度">去中心化、全精度</h3>
<p>BAGUA 还支持去中心化通信，消除了模型同步的延迟瓶颈——而不是在集群中的所有 n 个 worker 之间同步，每个 worker <strong>只将更新发送给它的邻居</strong>。 例如，根据<strong>基于环的拓扑</strong>，工人的邻居包括环中的紧邻左worker和紧邻右worker。 形式上，BAGUA 的去中心化全精度通信原语 DFPS 可以形式化如下：</p>
<blockquote>
<p>点评：就是ring allreduce 呀</p>
</blockquote>
<figure data-type="image" tabindex="7"><img src="https://DragonFive.github.io//post-images/1630755688224.png" alt="" loading="lazy"></figure>
<h3 id="去中心化-低精度">去中心化、低精度</h3>
<p>BAGUA 还为去中心化低精度通信提供原始 DLPS</p>
<h3 id="讨论支持异步算法">讨论：支持异步算法</h3>
<p>当前版本的 BAGUA 没有提供这些原语的任何异步版本，而是支持使用这些同步原语的异步算法，如下所示。 一个算法可以实现两个并发线程，一个处理计算，另一个处理通信。 这两个线程不会互相等待。 这提供了许多异步算法的实现。总结在表 1 中。它还可以实现 LocalSGD 和模型平均。 进一步探索提供异步版本的原语的好处很有趣，我们将其留作未来的工作。</p>
<h2 id="33-原语的实现">3.3 原语的实现</h2>
<h3 id="集中原语">集中原语</h3>
<p>BAGUA 使用 ScatterReduce 通信模式运行集中原语。 特别地，目标张量被分成n个分区，其中n是worker的数量。 第 i 个工作器负责聚合第 i 个分区。 由于底层通信库 NCCL 不提供 ScatterReduce 原语，我们使用基本的 send 和 recv NCCL 操作符来实现这个原语。</p>
<blockquote>
<p>点评，在nccl 2 里面提供了 <a href="https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/api/colls.html#c.ncclReduceScatter">reducescatter</a>，不太确定底层通信库 NCCL 不提供 ScatterReduce 原语指的是什么</p>
</blockquote>
<p>每个worker 1) 对本地张量进行分区，2) 将分区发送给相应的worker，3) 从其他worker 接收负责的分区，4) 合并收到的分区，以及5) 将合并的分区发送给其他worker。</p>
<blockquote>
<p>点评：这个意思是中心化是worker 当ps用了？</p>
</blockquote>
<p>ScatterReduce 通信模式可以利用所有 worker（如 Allreduce）的聚合带宽，并支持压缩技术（与 Allreduce 不同）。 低精度原语 CLPS 利用 ScatterReduce 通信来合并两个压缩阶段。每个工作人员在发送之前只压缩本地张量的分区（阶段 1）和合并的分区（阶段 2）。 请注意，压缩和解压缩过程可以与错误补偿技术相结合可以减少信息丢失（参见第 3.2 节中的语义）。</p>
<h3 id="分散原语">分散原语</h3>
<p>与所有 worker 都参与交流的集中式培训不同，分散式培训中的每个worker只与一个或几个同伴进行交流。 BAGUA 设计了两种机制来分配对等点——环和随机。 环形策略为worker提供连续的等级并将所有worker组织成一个环。rank-i worker 只与两个相邻的 peer 通信，rank-(i − 1) 和 rank-(i + 1)。 或者，随机策略为每个worker随机选择一个对等点。 当通信对等点被分配时，每个worker将本地张量发送给对等点，从对等点接收张量，并计算它们的平均值。 低精度原语 D L P S 使用与 D FP S 相同的对等点选择和通信过程。不同之处在于 D L P S 在发送前使用压缩函数 Q 压缩张量并在接收后解压张量。</p>
<blockquote>
<p>点评：如果分散式指的是仅用两个worker 的信息进行参数更新，这需要首先证明是有效的。</p>
</blockquote>
<h2 id="34-bagua优化框架">3.4 bagua优化框架</h2>
<p>BAGUA 的核心组件是它的执行优化器。 给定一个神经网络作为输入，训练算法（例如，QSGD）将在每一层的计算过程中利用一系列通信原语。 BAGUA 执行优化器的目标是自动调度和优化这些计算和通信。 我们在 BAGUA 中探索了以下技术。</p>
<h3 id="重叠通信和计算">重叠通信和计算</h3>
<p>重叠通信和计算是加速分布式 DP-SG 的一项核心优化。不限于 DP-SG 算法，BAGUA 能够以灵活和自动的方式重叠通信原语以及其他算法的计算。 BAGUA 自动分析包含就地张量操作和张量通信原语的计算图。尽管可以通过静态分析构建此图，但 BAGUA 决定利用动态分析方法，在第一次迭代中收集张量操作和通信原语的调用依赖性。与现有系统相比，BAGUA 考虑了更复杂的调度。在vanilla DP-SG中，优化只能通过逆序将Allreduce通信隐藏在反向传播的计算中；相比之下，BAGUA 负责调度额外的元素，例如低精度压缩/解压缩和优化算法指定的模型更新计算。</p>
<blockquote>
<p>点评：但如果没有低精度需求，是不是就跟现有的方法一样了？通过scatterreduce 合并压缩解压。vanilla 这里需要关注一下。</p>
</blockquote>
<h3 id="张量分桶和内存扁平化">张量分桶和内存扁平化</h3>
<p>为了有效地将通信与计算重叠，将层划分为桶是必不可少的步骤——频繁调用通信范式来传输小片段参数对于充分利用网络带宽而言远非理想。 因此，Horovod 和 PyTorch-DDP 都采用了分桶技巧。 然而，他们的分桶模式可以简单地将 Allreduce 通信硬编码为启发式中的成本，并使用<strong>神经网络中层的相反顺序来确定桶</strong>。</p>
<p>相比之下，由于 BAGUA 支持更多由优化算法指定并由 BAGUA 的通信原语形式化的通信方式，因此根据<strong>分析阶段收集的依赖关系确定分桶</strong>。 一旦我们将计算图拆分成桶，BAGUA 就会对桶进行融合。 这使 BAGUA 可以实现更有效的流水线，尤其是在包含低精度系统松弛时</p>
<blockquote>
<p>点评：所以关键在于分桶方式，分析阶段的依赖关系确定是重点</p>
</blockquote>
<p>在第一次反向传播中确定桶的分区后，BAGUA 会仔细地将<strong>桶内的参数（例如模型参数、梯度和优化器状态）对齐到连续的内存空间中</strong>。 然后，这种扁平化的参数视图被用于所有流水线执行。 例如，低精度压缩/解压 lambda 直接应用于桶的扁平视图而不是单个参数； 用于模型更新的基于 SG 的优化器也在存储桶级别进行（来自 NVIDIA 的 Apex 也使用了类似的优化）。 请注意，这种<strong>扁平化视图可以更有效地利用计算单元提供的并行性</strong>。</p>
<blockquote>
<p>点评：看来扁平化优化的也是通信时候的操作。扁平化在取计算的时候是否会拖慢计算的速度呢，为什么可以有效利用计算的并行性呢，是说一边计算一边通信吗</p>
</blockquote>
<h3 id="分层通信">分层通信</h3>
<p>BAGUA 的通信可以分层进行。 这在处理网络连接的异构性时特别有用，例如，服务器内 GPU 之间的带宽远高于服务器之间的带宽。 因此，BAGUA <strong>以节点内和节点间</strong>两个层次进行分层通信，并基于此抽象优化通信原语的实现。</p>
<p>例如，集中式低精度原语（CLPS）可以优化为首先在没有压缩的情况下在每个节点内部的本地 worker上聚合张量，然后对从每个节点选出的leader worker进行压缩进行节点间聚合，最后 让每个leader worker在节点内广播聚合数据。</p>
<blockquote>
<p>点评：那么分层和leader worker 的好处在哪里呢</p>
</blockquote>
<p>请注意，这种优化可能会改变通信原语的语义。 对于去中心化原语，节点内的工作人员将始终更改为中心化的 Allreduce 方式。</p>
<blockquote>
<p>点评：所以去中心化就不能用这个分层优化呗，但这个算是什么优化呢</p>
</blockquote>
<h1 id="4-评估">4. 评估</h1>
<p>我们围绕三个假设进行了广泛的实验研究：</p>
<ul>
<li>与最先进的系统相比，BAGUA 能够在以下方面提供显着的性能改进<br>
端到端的培训时间和可扩展性，超过现实的工业规模基础设施。</li>
<li>BAGUA 支持的不同算法为不同网络条件下的不同模型和数据集提供了好处。 因此， BAGUA 支持所有这些算法非常重要。</li>
<li>BAGUA 的自动执行优化器有效优化了各种分布式训练算法的执行？</li>
</ul>
<h2 id="44-系统优化的消融研究">4.4 系统优化的消融研究</h2>
<p>我们现在验证 BAGUA 优化框架的有效性。 如 3.4 节所述，优化框架由三个优化组成： <strong>O：训练计算和 BAGUA 执行之间的重叠； F：张量的融合和扁平化。 H：分层通信</strong></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[华为的《ScaleFreeCTR:a MixCache-based distributed training system for CTR》]]></title>
        <id>https://DragonFive.github.io/post/hua-wei-de-scalefreectr/</id>
        <link href="https://DragonFive.github.io/post/hua-wei-de-scalefreectr/">
        </link>
        <updated>2021-06-15T12:23:03.000Z</updated>
        <content type="html"><![CDATA[<p>华为诺亚方舟提出了SFCTR <a href="https://arxiv.org/pdf/2104.08542.pdf">ScaleFreeCTR: a MixCache-based distributed training system for CTR</a>，scalefree 可能是因为数据规模对训练吞吐没有影响，后面实验部分有具体的数据。</p>
<h1 id="一-动机与主要创新">一、动机与主要创新</h1>
<p>现有的分布式CTR训练框架使用CPU内存来保存和更新参数，使用gpu 进行前向和反向计算（也有用CPU的），会有两个瓶颈</p>
<ol>
<li>
<p>CPU 和 gpu 之间的pull 和 push 操作有一定的延迟</p>
</li>
<li>
<p>cpu 进行参数的同步和更新比较慢</p>
</li>
</ol>
<p>分布式训练的关键在于</p>
<ol>
<li>关键在于减少host_gpu之间的延迟</li>
<li>减少host-gpu 及gpu之间的数据传输量也很重要</li>
</ol>
<p>推荐中的参数有两个特点：</p>
<ol>
<li>实际 working parameters 比较少，sparse 参数和 MLP参数都很少</li>
<li>sparse 特征符合幂律分布，小部分特征被高频访问</li>
</ol>
<p>根据两个特点，可以有两个方法</p>
<ol>
<li>使用缓存机制减少 host-gpu 延迟</li>
<li>通过重组batch数据来减少参数传输量(unique?)</li>
</ol>
<p>由此提出了 SFCTR:<br>
在CPU中通过 虚拟sparse id op 来减少host-gpu 和gpu-gpu 的数据传输量，使用 mixcache 实验特征预取来减少传输延迟，使用3级pipeline 来减少整体训练时长。</p>
<p>系统将会在MindSpore 上开源，现在看似乎还没有开源。</p>
<h1 id="二-相关工作">二、相关工作</h1>
<h2 id="一论文介绍的跟sfctr无关但挺有用的经验知识">（一）论文介绍的跟SFCTR无关，但挺有用的经验知识</h2>
<p>为了提高训练效率，有两种通用的做法：</p>
<ol>
<li>增量学习（batch训练的补充，用最近的数据更新模型）</li>
<li>分布式训练（使用额外的训练资源）</li>
</ol>
<p>CTR模型稀疏部分参数量太大，所以不能使用reduce 数据并行，大多数考虑用了模型并行。<br>
模型并行解决方案ps 架构的局限性：<br>
Ps server 保存并同步参数，worker执行前向和反向计算，</p>
<ul>
<li>worker pull and push from ps</li>
<li>Ps 从worker接收梯度之后进行同步<br>
分布式训练包括两个阶段：计算和参数同步。</li>
</ul>
<p>百度的综述 <a href="https://arxiv.org/abs/2003.05622">Distributed Hierarchical GPU Parameter Server for Massive Scale Deep Learning Ads Systems</a> 介绍了三种同步模式：</p>
<ul>
<li>BSP(bulk sync parallel)，严格对所有worker的更新进行同步</li>
<li>SSP(stale sync parallel)，对快worker 进行同步</li>
<li>ASP（async parallel）, 不同步gradient</li>
</ul>
<p>后两种方式虽然提升了训练效率，但是降低了模型性能。SFCTR 使用的是BSP，XDL使用的是ASP。</p>
<h1 id="三-sfctr-架构">三、SFCTR 架构</h1>
<p>SFCTR 由三部分构成</p>
<ul>
<li>Data-Loader, 提出虚拟sparse id op 来减少batch中重复的特征emb(unique?)</li>
<li>Host-Manager, 使用混合缓存策略来减少host-gpu延迟，MixCache 的管理器部分在 CPU 的内存中，MixCache 的缓冲区在 GPU 的 HBM 中</li>
<li>GPU-WORKER<br>
<img src="https://DragonFive.github.io//post-images/1625142600332.png" alt="" loading="lazy"></li>
</ul>
<p><strong>3级pipeline</strong><br>
把 Data-Loader,Host-Manager 和gpu worker 中，三阶段资源不同 Disk, CPU and GPU在三个不同的线程里完成</p>
<figure data-type="image" tabindex="1"><img src="https://DragonFive.github.io//post-images/1625142730082.png" alt="" loading="lazy"></figure>
<h2 id="1data-loader">（1）data loader</h2>
<p>sparse id op 来减少batch中重复的特征emb，就是xdl中的unique。减少host-gpu 和gpu-gpu 的数据传输量。</p>
<h2 id="2host-manager">（2）HOST-MANAGER</h2>
<p>MixCache用来减少延迟，在每个GPU的HBM 上申请一个cache buffer，使用modulo 哈希方法对 working parameters 进行分组，放在不同的GPU 上，embedding参数在data loader执行完VSI op 之后检查哪些参数 gpu 上没有就把那些参数传输到gpu 上。</p>
<p>当cache满了之后，满足两种情况的emb 会回传到host。</p>
<ol>
<li>参数完成了更新</li>
<li>下个batch不需要这个参数</li>
</ol>
<h2 id="3gpu-worker">（3）GPU-WORKER</h2>
<p>不同的GPU保存了不同的参数，所以前向和反向的时候都需要同步。<br>
前向传播，每个worekr从其它worker拿到batch所需要的参数，使用all-reduce 通信方式，一个gpu只需要跟另外两个gpu通信两次，首先通过gather_cache ，从cache buffer 中获得local common emb, 因为global_id 顺序一致，所以可以做all_reduce同步, 然后通过all_reduce 或者 global common emb，最后通过vis 算出来自己worker需要执行的batch emb<br>
<img src="https://DragonFive.github.io//post-images/1625142803471.png" alt="" loading="lazy"></p>
<p>梯度更新<br>
<img src="https://DragonFive.github.io//post-images/1625142812224.png" alt="" loading="lazy"></p>
<h1 id="四-sfctr-执行流程">四、SFCTR 执行流程</h1>
<p>执行流程<br>
<img src="https://DragonFive.github.io//post-images/1625142836322.png" alt="" loading="lazy"></p>
<ul>
<li>
<p>2-3行是 data loader 部分，有个虚拟Sparse Id OP，对batch Sparse ID 去重后形成 global_id，对于batch 中每个实例有个virtual_id，可以找到其对应的global_id ，跟XDL 的unique 操作很像。使用global_id, 各个gpu在同步的时候数据量就会少很多。</p>
</li>
<li>
<p>4-7行是 Host-Manager 部分，负责在主存中报错embedding参数（存得下吗？），使用mixcache把working parameters 放到 gpu 的cache buffer中。mixcache 还更新gpu cache buffer， 检查下一个batch需要哪些embedding ，预测哪些embedding未来一段时间不需要，在buffer满的时候进行pull, 发送数据到gpu，并对每个特征在gpu设置一个local_id (?)</p>
</li>
<li>
<p>9-15行是GPU部分，包括embedding查表，前向反向和参数更新</p>
</li>
</ul>
<p>host 和 gpu 是生产者与消费者模式</p>
<h1 id="五-一些对我们有用的实验">五、一些对我们有用的实验：</h1>
<h2 id="1环境">（1）环境</h2>
<p>GPU 集群使用 InfiniBand 连接，4台GPU服务器通过100Gb  RDMA提速</p>
<p>Intel Xeon Gold-5118 CPUs with 18 cores (36 threads), 8 Tesla V100 GPUs with 32 GB HBM，1GB内存。GPU之间PCI连接<br>
使用 Criteo-TB 数据库，使用filter构造10GB和100GB两个数据集，因为包括了优化器的信息，33×4B×80=10GB，所以实际parameter table是embedding table的3倍，所以实际上是 30GB和300GB的模型参数。</p>
<p>使用 DeepFM 模型，与 hugectr与ps mxnet对比</p>
<h2 id="2框架对比实验">（2）框架对比实验</h2>
<p>基于VSI OP，混合缓存机制，和三级pipeline，在10GB数据上SFCTR 在4机32卡上的吞吐量是psmxnet的1.6倍，hugectr的5.6倍，100GB 数据上是1.3倍和6.9倍<br>
如果GPU卡只有8个，hugectr在100GB数据上根本无法训练</p>
<figure data-type="image" tabindex="2"><img src="https://DragonFive.github.io//post-images/1625143001706.png" alt="" loading="lazy"></figure>
<h2 id="3vsi-op">（3）vsi op</h2>
<p>Host-gpu 数据传输量减少 94% ，g pu-gpu数据量减少88%</p>
<figure data-type="image" tabindex="3"><img src="https://DragonFive.github.io//post-images/1625143009606.png" alt="" loading="lazy"></figure>
<h2 id="4mixcache">（4）mixcache</h2>
<p>Cache 大小对传数据的影响<br>
2GB的cache可以把数据传输推迟到1000步之后<br>
如果cache 大小比较大，batch中要传输的数据的比例就会小，因为可以存更多高频特征<br>
12%（2GB）, 27%（0.5GB） and 29%（0.25GB）</p>
<figure data-type="image" tabindex="4"><img src="https://DragonFive.github.io//post-images/1625143020154.png" alt="" loading="lazy"></figure>
<h2 id="53级pipeline">（5）3级pipeline</h2>
<p>GPU-Worker 训练时间在pipeline中占比最高<br>
一个节点跑100GB数据，使用pipeline需要 75 s，不用pipeline就需要150s</p>
<h1 id="六-总结与思考">六、总结与思考</h1>
<p>文章写的通俗易懂，很有条理，related worker 也总结了很多训练的经验，工作很有实用性。<br>
作者提到未来的工作有两个方向</p>
<ol>
<li>提升通信效率，（使用all2all）</li>
<li>调查提升收敛速度的方法</li>
</ol>
<p>思考借鉴意义</p>
<ol>
<li>
<p>SFCTR相当于把图完全放在GPU中执行，没有进行图的分隔，所以实现起来更容易一些。CPU只是一个ps的存储和更新后落盘以及dataloader</p>
</li>
<li>
<p>CPU内存1T，而实验中的数据最大的为300GB，所以可以放在CPU内存中，其实我们的模型大小似乎也在几百GB，如果可以放在worker内存中，就没有必要单独弄一个ps server；<br>
如果模型超过1T，也可以融合AIBox的做法，使用SSD做cpu mem的缓存</p>
</li>
<li>
<p>pipeline 和 vsiop 其实 XDL 都有，只缺了缓存机制，但XDL如果不动ps这一块，参数的更新其实是在ps 上完成的，所以 ps 的 push 也会继续有延迟，参数预取只能解决pull 的问题，</p>
</li>
<li>
<p>但如果都在本地更新，那不同worker之间参数同步就会比较麻烦，所以缓存预取、更新后缓存失效再回传的机制必然依赖多机间 RDMA  单机allreduce 同步通信技术，ps存在的意义不大</p>
</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[业界CTR深度学习框架的一些新的进展 ]]></title>
        <id>https://DragonFive.github.io/post/ye-jie-ctr-shen-du-xue-xi-kuang-jia-de-yi-xie-xin-de-jin-zhan/</id>
        <link href="https://DragonFive.github.io/post/ye-jie-ctr-shen-du-xue-xi-kuang-jia-de-yi-xie-xin-de-jin-zhan/">
        </link>
        <updated>2021-05-31T13:03:49.000Z</updated>
        <content type="html"><![CDATA[<p>为了充分利用GPU的能力和高速带宽<br>
英伟达的 hugeCtr https://github.com/NVIDIA/HugeCTR 和 脸书 的 DLRM<br>
<a href="https://arxiv.org/abs/1906.00091">【CoRR2019】Deep Learning Recommendation Model for Personalization and Recommendation Systems</a><br>
把emb参数分成不同的份放在GPU HMB中，需要需要昂贵的GPU，不实用。</p>
<p>腾讯的DES<br>
<a href="https://arxiv.org/abs/1909.04823">Distributed Equivalent Substitution Training for Large-Scale Recommender Systems</a><br>
和 百度的 HierPs<br>
<a href="https://arxiv.org/abs/2003.05622">Distributed Hierarchical GPU Parameter Server for Massive Scale Deep Learning Ads System</a><br>
使用主存保存emb ，DES采用 field-aware 分片策略来reduce(减少or规约)GPU间的数据通信，但没有进行主存和GPU之间通信优化。<br>
HierPS使用大batch策略来在gpu中缓存使用大参数，以此减少传输延时。</p>
<p>Tensorflow, MxNet 和 PyTorch 并不能很好的支持大规模embedding的训练:</p>
<ul>
<li>PyTorch 中没有官方支持的ps</li>
<li>Mxnet 支持的模型大小因为实现问题受到了限制</li>
<li>tensorflow 使用它的ps后吞吐会严重下降</li>
</ul>
<p>为了提升tensorflow, mxnet, pytorch较差的分布式性能，uber 的horovod<br>
<a href="https://arxiv.org/abs/1802.05799">Horovod: fast and easy distributed deeplearninginTensorFlow</a><br>
和字节的byteps<br>
<a href="https://arxiv.org/abs/1807.00311">Product-based Neural Networks for User Response Prediction over Multi-field Categorical Data</a><br>
都支持不同的平台:</p>
<ul>
<li>horovod 使用Ring-AllReduce实现来加速dense模型的训练</li>
<li>byteps 通过调度优先来在不同的层加速同步参数，优化不同层的顺序来在反向传播和前向计算的时候同步参数</li>
</ul>
<p>之前写过一篇关于 horovod 的知识总结：<a href="https://dragonfive.github.io/post/uber-de-horovod/">uber的Horovod | dragon</a></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[百度的《AIBox: CTR Prediction Model Training on a Single Node》]]></title>
        <id>https://DragonFive.github.io/post/bai-du-de-lesslessaibox-ctr-prediction-model-training-on-a-single-nodegreatergreater/</id>
        <link href="https://DragonFive.github.io/post/bai-du-de-lesslessaibox-ctr-prediction-model-training-on-a-single-nodegreatergreater/">
        </link>
        <updated>2021-03-11T12:43:07.000Z</updated>
        <content type="html"><![CDATA[<p>AIBox 是百度提出的训练框架，论文 AIBox: CTR Prediction Model Training on a Single Node 进行了相关介绍。</p>
<h1 id="一-aibox-的技术创新及优势">一、AIBox 的技术创新及优势</h1>
<p>创新点与动机：<br>
AIBox 的核心想法就是想在一台机器上用GPU加速训练，但是参数实在太大了，所以就把计算密集的模型运算部分（joint learning）放在GPU完成，把取embedding 部分放在cpu部分完成(embedding learning)，这就是AiBox 的第一个创新：把网络切分为两部分。</p>
<p>但即便主存用了1TB 的内存，embedding 还是太大了，10^12 个key，每个key 的 weight 即使用8个字节存，key用8个字节存，也需要1.6TB , 所以论文提出了第二个创新：把embedding 存在SSD上，同时为了降低延迟和减少写操作对ssd寿命的影响，建立了二级缓存机制。</p>
<p>为了提高速度，AIBOX使用了流水线，把从hdfs 读数据(socket IO)，从SSD查Embedding （SSD io） 和 cpu+gpu 计算组成3阶段的 pipeline</p>
<p>优势：<br>
AIBOX不存在像分布式系统普遍存在的网络通信开销问题，然后在系统稳定性方面AIBOX与具有数千台机器的分布式集群相比更加稳定不会轻易宕机，而且在同步开销方面AIBOX只是涉及到一些内存锁和GPU片之间的少量通信。</p>
<h1 id="二-关于网络结构切分">二、关于网络结构切分</h1>
<p>the first module focuses on the embedding learning with high-dimensional &amp; sparse features and the second module is for joint learning with dense features resulted from the first module.</p>
<p>The embedding learning is processed on CPUs to help learn low dimensional dense embedding representations.</p>
<p>By transferring the learned embedding vectors from CPUs to GPUs, the computation-intensive joint learning module can make full use of the powerful GPUs for CTR prediction.</p>
<p>CPU 部分把数据从稀疏特征转化成 embedding （embedding learning），然后把embedding 传到 GPU，GPU进行一轮训练 (joint learning)</p>
<p>论文这部分讲了一些网络的设计细节，但这块感觉跟 AIBox本身没什么关系，论文写到：</p>
<p>把第一隐含层和最后一层隐含层的结果合并起来，第一层包含了low-level 的与输入信息最相关的feature，最后一层包含了high-level 的最抽象和有用的信息。这样会得到更准确的CTR预估结果。</p>
<p>训练两阶段(cpu+gpu)，梯度更新也是两阶段(gpu+cpu)。</p>
<h1 id="三-aibox-架构划分">三、AIBox 架构划分</h1>
<p><strong>架构：</strong><br>
分为三部分：CPU、GPU和 sparse table buff</p>
<ul>
<li>
<p>cpu模块：协调调度和embedding学习<br>
从hdfs读数据(一个pass)，向Sparse Table模块查embedding，然后发给GPU<br>
拿到gpu传来的梯度，更新sparse table<br>
定期save ckpt 到 hdfs</p>
</li>
<li>
<p>sparse table：把10^12 的离散特征的数据存储到ssd上的kv系统里<br>
内存中的key hash 索引存了特征到文件的映射关系，<br>
in-memory cache strategy 构造cache 和 buffer 来减少延迟</p>
</li>
<li>
<p>gpu模块：联合学习<br>
cpu传来的 embedding 被放入 HBMs 中，然后被fed 给 dense 联合学习网络<br>
emb通过pci-e总线进行传输<br>
一个CUDA stream进行数据传输，另一个cuda stream 进行学习计算<br>
HBMs如同片上ps一样工作，<br>
每个pass 在每个gpu 上计算新的参数，各gpu通过NVLink进行同步</p>
</li>
</ul>
<p><strong>3阶段流水线：network, SSDs and CPUs + GPUs</strong></p>
<figure data-type="image" tabindex="1"><img src="https://DragonFive.github.io//post-images/1625143570973.png" alt="" loading="lazy"></figure>
<figure data-type="image" tabindex="2"><img src="https://DragonFive.github.io//post-images/1625143575338.png" alt="" loading="lazy"></figure>
<h1 id="四-sparsetable-架构">四、sparseTable 架构</h1>
<p>由两部分构成：key hash index and bi-level cache management</p>
<h2 id="一key-hash-index">（一）key hash index</h2>
<p>Key Hash Index 存的是 10^12 个 feature key 到 ssd 文件的映射关系，直接每个key 存一个文件需要1.6TB大小的内存，放不下。</p>
<p>通过对key 取模进行分组建立 group 与 file 的对应关系，放在内存中。</p>
<p>group(key) → key mod 1012/m. We set m = ⌊BLOCK/(8 + sizeof(value))⌋,其中Block 是每次从ssd取数据的最小单元。</p>
<p>hash函数可以通过预训练一个模型来最大化feature 共现，把共现的feature 分到同样的桶里面。</p>
<h2 id="二二级缓存机制">（二）二级缓存机制</h2>
<p>ssd 的延迟是内存的1000倍，ssd是微秒级别延迟，内存是纳秒级别延迟</p>
<p>在一个 pass of mini batch 中只有1%的参数会被用到，所以我们可以用in-memory cache 来存储高频访问的hot parameters</p>
<p>SSD有物理性能限制：每个存储单元只能被写入（擦除）数千次，cache机制可以作为参数缓存，来减少更新参数对SSD使用寿命的影响</p>
<p>使用两个分离的链表进行拉链来提升探测性能。对每个ssd文件使用Bloom filter来减少不必要的读取。</p>
<p><strong>第一级缓存</strong></p>
<p>使用 si =hash1(g_id) 来算出一个 cache slot 槽，对应一个ssd 文件，对于参数并未进行真正初始化，而是在第一次访问到参数的时候，先用 bloom filter 探测key 是否在 slot 集合里，如果不在就不用读取这个文件，而是直接使用默认值，以此来减少不必要的ssd读取。</p>
<p><strong>二级缓存</strong></p>
<p>hash2(g_id, bucket)</p>
<p>对 一级的槽进行分桶bucket，来使得拉的链比较短。bucket 参数通过调节可以权衡空间和探测效率</p>
<p><strong>两条拉链</strong></p>
<ul>
<li>LRU 链用于保存最近访问过的key，以此来减少探测次数</li>
<li>LFU链按访问频次来保存key，用于缓存管理，只有当LFU满了需要删除低频key时，相应的数据才会写回到ssd上面</li>
</ul>
<p>由于经常有链条中的节点进行增删，所以使用线程池以Slab memory allocation mechanism机制 进行管理。</p>
<figure data-type="image" tabindex="3"><img src="https://DragonFive.github.io//post-images/1625143705399.png" alt="" loading="lazy"></figure>
<p><strong>文件管理系统</strong><br>
batch产生的小文件对于先有的文件系统有很大的压力，把许多小文件组成一个组来创建较少的文件。小文件的名字由大文件的名字加上offset构成，保存在第一级cache slot 中</p>
<p>监控文件系统的大小，合并访问量少的小问题，当model_size 达到最大冗余度的时候删掉访问少的文件，MAX_REPLICATION=SSD capacity ∗ (85% + overprovisioning)/model size<br>
<img src="https://DragonFive.github.io//post-images/1625143710125.png" alt="" loading="lazy"></p>
<h1 id="五-实验部分">五、实验部分</h1>
<p><strong>实验</strong><br>
AIBox 8 个GPU， 服务器级别的cpu, 1T 内存，Raid 0 nvme ssd<br>
MPI集群方式用75个计算节点</p>
<ul>
<li>
<p>AIBox 的硬件和维护费用比集群训练方式少 10%，执行时间多25%</p>
</li>
<li>
<p>AIBox 的auc 比集群方式稍好，可能是因为AIBox 这种单节点的方式，同步参数频率更高</p>
</li>
</ul>
<figure data-type="image" tabindex="4"><img src="https://DragonFive.github.io//post-images/1625143749656.png" alt="" loading="lazy"></figure>
<p>六、总结与一些细节问题：<br>
论文介绍了 AIBox 架构的一些细节方面，借助一系列系统设计方案如缓存机制来解决问题，通过这些并不是很fashion的技术合并，论文实现了集中式训练的技术突破，这种通过技术积累然后撬动难题的解决问题的方式值得我们学习。</p>
<p>但是还有一些细节没有讲清楚：</p>
<ol>
<li>
<p>AIBox 有几个worker 进行工作，他们是数据并行，还是使用同样的数据进行训练（文中提到AIBox 会在每个pass of mini batch 进行同步，所以应该不是一个worker 在参与训练）</p>
</li>
<li>
<p>AIBox 使用集中的训练方式，那如果这台机器挂掉，是不是根本没有办法进行恢复，只能另找一个机器从 ckpt 训练</p>
</li>
<li>
<p>文章没有介绍使用的具体计算引擎 (怀疑跟 horovod 接近）</p>
</li>
<li>
<p>同样文章没有介绍参数同步的细节，没有相关 all_reduce 的介绍（可以是使用了一个开源的框架，而这部分论文没有进行改进，所以没有做深入介绍）</p>
</li>
<li>
<p>文章开头提到使用 in-HBM ps 来减少数据传输，但是后面没有详细进行介绍</p>
</li>
</ol>
<p>总体上感觉这篇论文实用性强，但是细节介绍得不多</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[tensorflow2.x 分布式训练]]></title>
        <id>https://DragonFive.github.io/post/tensorflow2x-fen-bu-shi-xun-lian/</id>
        <link href="https://DragonFive.github.io/post/tensorflow2x-fen-bu-shi-xun-lian/">
        </link>
        <updated>2020-06-24T07:41:26.000Z</updated>
        <content type="html"><![CDATA[<blockquote>
<p>去年总结了一些 tensorflow 1.x 分布式训练的一些知识（<a href="https://dragonfive.github.io/post/tensorflow-1x-de-fen-bu-shi-xun-lian/">tensorflow 1.x 的分布式训练</a>）。最近总结了一些 tf2.x 的分布式训练相关知识。</p>
</blockquote>
<h1 id="tf2x-分布式训练策略">tf2.x 分布式训练策略</h1>
<p>TensorFlow 的 tf.distribute 模块下包含有一系列分布式训练策略，它们都是基于数据并行模式实现的。有些策略目前还在 experimental 模块下，表示它们是实验性质的策略，未来可能会发生变动。</p>
<p>训练分布式模型，只需要将原先的模型代码置于distribution Strategy的Scope()下，即可。</p>
<pre><code class="language-py">import tensorflow as tf
data_train, _ = tf.keras.datasets.mnist.load_data()
dataset = tf.data.Dataset.from_tensor_slices(data_train) # 该处可接收numpy数组
dataset = dataset.shuffle(buffer_size=60000) # 该处要大于data_train的长度
dataset = dataset.batch(32)
 
mirrored_strategy = tf.distribution.MirroredStrategy()
 
# mirrored_strategy策略: 在每一个gpu上训练一个模型，每次更新时需要汇总所有gpu上的梯度。
with mirrored_strategy.scope():
  model = tf.keras.Sequential([...])
# tf 2.0中，所有的optimizer都在tf.keras.optimizer下
model.compile(optimizer=tf.keras.optimizer.adam(lr=...))， 
              loss = &quot;sparse_categorical_crossentropy&quot;,
              metrics = ['accuracy'])
model.fit(dataset, epoch=5)
</code></pre>
<h1 id="单机多卡训练">单机多卡训练</h1>
<h2 id="mirrored">Mirrored</h2>
<p>MirroredStrategy 是一种单机的同步的分布式训练策略。它支持在一台机器的多个 GPU 之间进行分布式训练，它会在每个 GPU 上创建一个模型副本，模型中的每个变量 (Variables) 都会进行镜像复制并放置到相应的 GPU 上，这些变量被称作镜像变量 (MirroredVariable)。</p>
<p>MirroredStrategy 策略通过 AllReduce 算法使得所有镜像变量在每个 GPU 之间保持同步更新， AllReduce 算法默认使用英伟达的 NcclAllReduce ，也可以通过 cross_device_ops 参数修改为其他的 AllReduce 算法，如 HierarchicalCopyAllReduce 。</p>
<p>MirroredStrategy 策略会自动使用所有能被 TensorFlow 发现的 GPU 来做分布式训练，如果只想使用部分的 GPU 则可以通过 devices 参数来指定。</p>
<p>MirroredStrategy 实例的创建代码如下所示：</p>
<pre><code class="language-py">mirrored_strategy = tf.distribute.MirroredStrategy(
    devices=[&quot;/gpu:0&quot;, &quot;/gpu:1&quot;],
    cross_device_ops=tf.distribute.HierarchicalCopyAllReduce(),
)

</code></pre>
<p>如果 TensorFlow 没有发现 GPU 则默认会退化为使用 CPU 来进行训练。 MirroredStrategy 的典型使用场景为单机多 GPU 。</p>
<p><strong>MirroredStrategy 的步骤如下：</strong></p>
<ul>
<li>
<p>训练开始前，该策略在所有 N 个计算设备上均各复制一份完整的模型；</p>
</li>
<li>
<p>每次训练传入一个批次的数据时，将数据分成 N 份，分别传入 N 个计算设备（即数据并行）；</p>
</li>
<li>
<p>N 个计算设备使用本地变量（镜像变量）分别计算自己所获得的部分数据的梯度；</p>
</li>
<li>
<p>使用分布式计算的 All-reduce 操作，在计算设备间高效交换梯度数据并进行求和，使得最终每个设备都有了所有设备的梯度之和；</p>
</li>
<li>
<p>使用梯度求和的结果更新本地变量（镜像变量）；</p>
</li>
<li>
<p>当所有设备均更新本地变量后，进行下一轮训练（即该并行策略是同步的）。</p>
</li>
</ul>
<h2 id="centralstorage">CentralStorage</h2>
<p>CentralStorageStrategy 也是一种单机的同步的分布式训练策略。但与 MirroredStrategy 策略不同的是，它会将模型的所有变量保存在 CPU 内存上，而不是通过镜像复制的方式保存在每个 GPU 上，所有的计算操作则会在每个 GPU 上以同样的方式执行。</p>
<p>如果机器只有一个 GPU ， 那么所有的变量和计算操作都会放在该 GPU 上。在对 CPU 上的变量进行更新前，该策略会先将所有 GPU 副本的上的变量梯度进行聚合，然后应用到 CPU 变量更新中。</p>
<p>CentralStorageStrategy 实例的创建代码如下所示：</p>
<pre><code class="language-py">central_storage_strategy = tf.distribute.experimental.CentralStorageStrategy()
</code></pre>
<p>CentralStorageStrategy 策略在 CPU 与 GPU 通信代价远低于 GPU 与 GPU 之间的通信代价时，较为适用，基本上很少会有这种情况出现。</p>
<h1 id="多机训练策略">多机训练策略</h1>
<h2 id="multiworkermirroredstrategy">MultiWorkerMirroredStrategy</h2>
<p>MultiWorkerMirroredStrategy 策略因为要涉及到多个 worker 节点之间的通信交互，因此每个 worker 节点需要提前获知集群中各节点配置信息以便在变量更新时使用。</p>
<p>TensorFlow 中定义集群配置信息的标准方式是使用 TF_CONFIG 环境变量来实现的，该环境变量定义了集群中所有节点的配置信息，包括所有 worker 节点的网络地址，当前 worker 节点的索引 (index) 以及当前 worker 节点的角色 (type)。</p>
<p>示例如下:</p>
<pre><code class="language-python">os.environ['TF_CONFIG'] = json.dumps({
    'cluster': {
        'worker': [&quot;localhost:20000&quot;, &quot;localhost:20001&quot;]
    },
    'task': {'type': 'worker', 'index': 0}
})
</code></pre>
<p>TF_CONFIG 由 cluster 和 task 两部分组成：</p>
<p>cluster 说明了整个多机集群的结构和每台机器的网络地址（IP + 端口号）。对于每一台机器，cluster 的值都是相同的；</p>
<p>task 说明了当前机器的角色。例如， {'type': 'worker', 'index': 0} 说明当前机器是 cluster 中的第 0 个 worker（即 localhost:20000 ）。每一台机器的 task 值都需要针对当前主机进行分别的设置。</p>
<p>以上内容设置完成后，在所有的机器上逐个运行训练代码即可。先运行的代码在尚未与其他主机连接时会进入监听状态，待整个集群的连接建立完毕后，所有的机器即会同时开始训练。</p>
<p>MultiWorkerMirroredStrategy 策略与 MirroredStrategy 策略很相似，可以理解为是 MirroredStrategy 策略的多机的同步的分布式训练版本，它也会在每一台机器上创建所有变量的副本。</p>
<p>多个 worker 节点之间使用 AllReduce 算法来保持模型变量的同步更新， TensorFlow 里将这一操作称为 CollectiveOps。 CollectiveOps 会在 TensorFlow 模型运行时自动根据硬件，网络拓扑以及张量的大小来自动选择合适的 AllReduce 算法来进行网络通信以完成变量更新。</p>
<p>MultiWorkerMirroredStrategy 策略目前有两种可供选择的 CollectiveOps 。 一种为 CollectiveCommunication.RING ，它使用 gRPC 作为通信层实现了基于环的 AllReduce 操作。 另一种为 CollectiveCommunication.NCCL， 它使用了英伟达的 NCCL 库来实现 AllReduce 操作。在实际使用中，可以基于自己的运行环境选择合适的 CollectiveOps，或者使用 CollectiveCommunication.AUTO 交由 TensorFlow 运行时自行选择。</p>
<p>MultiWorkerMirroredStrategy 实例的创建代码如下所示:</p>
<pre><code class="language-python">multiworker_strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(
    tf.distribute.experimental.CollectiveCommunication.RING)
</code></pre>
<p>如果所有 worker 节点都不包含 GPU ，则该策略会退化为使用 CPU 在多个 worker 节点间进行分布式训练。如果集群中的 worker 节点数量只有一个则该策略会退化为 MirroredStrategy 策略。</p>
<h2 id="parameterserverstrategy">ParameterServerStrategy</h2>
<p>ParameterServerStrategy 是一种多机的异步的分布式训练策略。所以它也需要提前指定 TF_CONFIG 环境变量信息，与 MultiWorkerMirroredStrategy 策略不同的是集群中的节点不全是 worker ，有一部分节点会被指定为 ps 用来存储变量信息。模型的每一个变量都会存储在一个 ps 节点上，所有的计算操作会在所有的 worker 节点上以同样的方式执行。 ParameterServerStrategy 实例的创建代码如下所示：</p>
<pre><code class="language-python">ps_strategy = tf.distribute.experimental.ParameterServerStrategy()
</code></pre>
<h1 id="分布式集群定义">分布式集群定义</h1>
<p>一个典型的 TF_CONFIG 环境变量的值如下所示：</p>
<pre><code class="language-json">{
  &quot;cluster&quot;: {
    &quot;chief&quot;: [&quot;host1:port&quot;],
    &quot;worker&quot;: [&quot;host2:port&quot;, &quot;host3:port&quot;],
    &quot;ps&quot;: [&quot;host4:port&quot;],
    &quot;evaluator&quot;: [&quot;host5:port&quot;]
  },
  &quot;task&quot;: {
    &quot;type&quot;: &quot;worker&quot;,
    &quot;index&quot;: 0
  }
}

</code></pre>
<p>chief 节点的作用和 worker 节点大致相同，不过它还会做一些额外的工作，比如保存检查点文件 (checkpoints) 以及为 Tensorboard 记录日志文件等，如果不指定 cheif 节点，则默认会以 worker 列表中的第一个节点作为 chief 节点； worker 节点用来执行训练操作； ps 节点用来存储变量，只有在使用 ParameterServerStrategy 训练策略时才需要指定； evaluator 用来执行交叉验证操作，一般也是在使用 ParameterServerStrategy 策略时才会指定。</p>
<p>注意所有节点的 TF_CONFIG 环境变量中的 cluster 信息都是相同的，不同的地方在于 task 部分，而且所有角色 (task type) 的 index 必须从 0 开始，因为 TensorFlow 会根据该 index 从 cluster 下相应角色的列表中读取节点信息。</p>
<p>TF_CONFIG 环境变量可以写入到系统的环境变量中，但前提是该物理节点上只会同时启动一个集群节点实例，在大多数情况下，我们会在 python 程序中通过 os.environ[&quot;TF_CONFIG&quot;] 来指定集群的信息以实现按需创建，TensorFlow 运行时会自动解析其中的信息并启动训练任务。</p>
<h1 id="tf-集群分布式训练的难点">TF 集群分布式训练的难点</h1>
<p>集群分布式训练的难点在于每个节点的 TF_CONFIG 环境变量的构建，因为我们不能在每次训练时都去手动指定 ip 和端口（还需确定该端口是否被占用），一两个节点还可以忍受，可如果同时运行多个训练任务，并且每个任务都会使用几十个集群节点，那么手动构造这个环境变量的工作量是巨大的。</p>
<p>我们需要找到一种自动构建 TF_CONFIG 环境变量的方法，一些分布式训练框架可以为我们排忧解难。比如阿里的 x-deeplearning。</p>
<h1 id="参考资料">参考资料</h1>
<p><a href="https://tf.wiki/zh_hans/appendix/distributed.html">TensorFlow分布式训练 — 简单粗暴 TensorFlow 2 0.4 beta 文档</a></p>
<p><a href="https://juejin.cn/post/6885151250124374023">TensorFlow 篇 | TensorFlow 2.x 分布式训练概览</a></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[uber的Horovod]]></title>
        <id>https://DragonFive.github.io/post/uber-de-horovod/</id>
        <link href="https://DragonFive.github.io/post/uber-de-horovod/">
        </link>
        <updated>2020-05-14T09:03:21.000Z</updated>
        <content type="html"><![CDATA[<p>uber的Horovod 发表在 <a href="https://arxiv.org/abs/1802.05799">Horovod: fast and easy distributed deeplearninginTensorFlow</a>。</p>
<p>horovod 提供的各种框架的支持可以让 horovod 比较好的在各个框架的基础上使用，他支持 tensorflow/keras/mxnet/pytorch，MPI 的实现也有很多，比如 OpenMPI 还有 Nvidia 的 NCCL，还有 facebook 的 gloo，他们都实现了一种并行计算的通信和计算方式。</p>
<h1 id="用法">用法</h1>
<p>horovod追求以尽可能小的代码侵入性。<br>
<img src="https://DragonFive.github.io//post-images/1625217840093.png" alt="" loading="lazy"></p>
<p>在用户已经构建的代码上，只需要插入三段很短的代码即可，Horovod易用性甚好。因为只要用户的代码没问题，Horovod这三段植入不会让你的程序break。</p>
<ul>
<li>
<p>hvd.init()</p>
</li>
<li>
<p>创建horovod的优化器，即DistributedOptimizer，将旧的优化器封装起来</p>
</li>
<li>
<p>创建horovod的初始化hook，即BroadcastGlobalVariablesHook，将master的初始化值广播给其他worker</p>
</li>
</ul>
<p>hvd.init()这个函数。用户的这一句话，启动了Horovod的所有轮询进程及资源管理过程，下图描述了hvd.init()的宏观调用栈，核心就是background thread上启动的BackgroundThreadLoop()函数，它将常驻在进程中并不断轮询，直到程序完全结束。</p>
<figure data-type="image" tabindex="1"><img src="https://DragonFive.github.io//post-images/1625218453054.png" alt="" loading="lazy"></figure>
<p>Horovod借助BackgroundThreadLoop()函数对RunLoopOnce()函数做无限循环调用。<br>
若某份gradients已经产生，何时做AllReduce才能不死锁？显然，不可能见到一份gradients就马上做，因为这有概率会陷入死锁。正确答案应该是：</p>
<blockquote>
<p>“当该份gradients在所有的worker上均已经产出时，才能统一发动AllReduce”</p>
</blockquote>
<p>此时，不会有worker因为在等待其他某个worker没有产出该份gradients而进入无限等待的情况。那么就需要有一种机制，能够观察每份gradients在每个worker上的产出情况。</p>
<p>实际上，上述过程其实就是Horovod的做法。BackgroundThreadLoop为什么一直要轮询？就是要不断地做通知，计数等管理工作。因此，rank 0又被称为——Coordinator。等到真正需要做AllReduce时，RunLoopOnce会调用PerformOperation发动通信过程。</p>
<h1 id="参考资料">参考资料</h1>
<p><a href="https://ggaaooppeenngg.github.io/zh-CN/2019/08/30/horovod-%E5%AE%9E%E7%8E%B0%E5%88%86%E6%9E%90/">horovod 实现分析 | ggaaooppeenngg</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/332825987">Horovod 源码分析 - 知乎</a></p>
<p><a href="https://mp.weixin.qq.com/s/7c7Q0P3g3IEL_r4BU2ZxRg">Horovod架构剖析——解密最成功的第三方DL分布式训练框架</a></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[XDL的OP调用]]></title>
        <id>https://DragonFive.github.io/post/xdl-de-op-diao-yong/</id>
        <link href="https://DragonFive.github.io/post/xdl-de-op-diao-yong/">
        </link>
        <updated>2020-03-23T06:58:34.000Z</updated>
        <content type="html"><![CDATA[<p>emb 和 statis，那么最基本要实现的功能就是增、删、改、查，其中增对应的就是注册和初始化变量，删就是进行特征过滤，改就是进行更新，查就是从ps上把最新的值取下来，</p>
<p>并且这些功能需要保证一定的顺序，只有先注册才能初始化，之后才能删改查。</p>
<p>但是emb和statis都是函数，返回的分别是embedding_op 对应的op，加入到计算图中，那么对于变量的注册、初始化、更新就需要想其它的办法加入计算，</p>
<p>这里使用的是hook的机制，并且使用hook 的priority优先级机制来保证 op 调用的顺序，优先级数值越小的越先被调用，hook基类的默认优先级为2000</p>
<h1 id="1注册与初始化增">1.注册与初始化(增)</h1>
<p>注册与初始化两个OP，但注册只需要在client端完成 variableinfo 的设置即可，所以每个worker都需要做，就放在workerHook中完成</p>
<p>初始化择需要各个ps端完成，只需要一个worker向ps请求即可，所以只在workermaster 的 ChiefHook 中完成，并且worker master 的注册也是在 chiefHook 中完成（worker master 没有 workerHook）</p>
<p>chiehook 的优先级是1001 ，workerhook 的优先级是1002</p>
<pre><code class="language-python">class SimpleSession(object):
    def __init__(self, hooks=None):
        ...
        if self._is_chief:
            self._hooks = self._hooks + [ChiefHook()]
        else:
            self._hooks = self._hooks + [WorkerHook()]
        def take_priority(elem):
            return elem._priority
        self._hooks.sort(key=take_priority)
        self._session = Session(self._hooks)
        ...
 
 
class ChiefHook(Hook):
    def __init__(self):
        super(ChiefHook, self).__init__(priority=1001)
 
    def create_session(self):
        scopes = list(get_model_scopes())
        if global_variables(scopes) is None or\
                len(global_variables(scopes)) == 0:
            return
        execute_with_retry(variable_registers(scopes))
        execute_with_retry(global_initializers(scopes))

</code></pre>
<p>注册最后调用的是 ps_register_variable_op，初始化使用的是 用户指定的初始化方式 如 Ones，Zeros，xdl.TruncatedNormal 等等</p>
<p>注册在client端就结束了 ，只是在每个worker记一下variable的信息</p>
<pre><code class="language-c++">Status RawClient::RegisterVariable(const std::string&amp; name, const VariableInfo&amp; info) {
  std::lock_guard&lt;std::mutex&gt; lock(variable_info_mutex_);
  auto iter = args_.variable_info.find(name);
  if (iter != args_.variable_info.end()) {
    std::string arg_name = &quot;slots&quot;;
    auto it2 = info.args.find(arg_name);
    if (it2 != info.args.end()) {
      auto it = args_.variable_info[name].args.find(arg_name);
      if (it == args_.variable_info[name].args.end()) {
        args_.variable_info[name].args[arg_name] = it2-&gt;second;
      } else {
        args_.variable_info[name].args[arg_name] += &quot;|&quot; + it2-&gt;second;
      }
    }
    return Status::Ok();
  }
  args_.variable_info[name] = info;
  init_variable_info_ = false;
  return Status::Ok();
}

</code></pre>
<p>初始化会通过partitioner 将data 的shape split 给各个ps，然后请求各个ps按照 udf 来执行simplerun, 完成 data 和 slot 的初始化</p>
<p>worker0 初始化的时候会调用 initop, 在调用process的时候会把本地的variableinfo发送给scheduler，在从头训练（没有checkpoint）的情况下，scheduler会调用 placement</p>
<h1 id="2-filter-删">2. filter (删)</h1>
<p>就是各种用户指定的filter， 会在variable 带上对应的 slot, 这里所谓的删除是删除key对应的data行和slot行</p>
<p>需要注意的是，虽然fea_statis 或fea_score 是一起使用的，但在featurescorefilter中只计算fea_score, 而在HashSlotsUpdateHook中只更新 fea_statis,两者的计算是分开的<br>
还有一些slot不是通过filter添加的，比如__dacay_rate 和accumulate，这些没有删除功能，只能更新，将在下一部分介绍。</p>
<p>下面以 featurescorefilter 为例，介绍filter的工作。</p>
<p>featurescorefilter调用的是PsHashFeatureScoreFilterOp，</p>
<p>Broadcast这样的spliter是指向所有ps广播clk_weight,non_clk_weight和train_threshold的所有内容<br>
然后在client向各个ps发送“HashFeatureScoreFilter”这样的udf</p>
<pre><code class="language-cpp">class PsHashFeatureScoreFilterOp : public xdl::OpKernelAsync {
 public:
  ...
  void Compute(OpKernelContext* ctx, Callback done) override {
    ...
    ps::client::UdfData udf(&quot;HashFeatureScoreFilter&quot;,
                            ps::client::UdfData(0),
                            ps::client::UdfData(1),
                            ps::client::UdfData(2));

    std::vector&lt;ps::client::Partitioner*&gt; spliters{
      new ps::client::partitioner::Broadcast,
      new ps::client::partitioner::Broadcast,
      new ps::client::partitioner::Broadcast};

    client-&gt;Process(udf, var_name_, client-&gt;Args(nonclk_weight_, clk_weight_,
                                                 train_threshold_),
                                                 spliters, {}, outputs, cb);
  }
</code></pre>
<p>在ps端调用 HashFeatureScoreFilter 的simplerun</p>
<pre><code class="language-cpp">class HashFeatureScoreFilter : public SimpleUdf&lt;float, float, float&gt; {
 public:
  virtual Status SimpleRun(UdfContext* ctx,
                           const float&amp; nonclk_weight,
                           const float&amp; clk_weight,
                           const float&amp; score_threshold) const {
    Variable* variable = ctx-&gt;GetVariable();
    ...
    //2. compute fea score
    auto score = show_vector * nonclk_weight + clk_vector * (clk_weight - nonclk_weight);
    printf(&quot;HashFeatureScoreFilter for %s fea score min %f max %f\n&quot;,
           var_name.c_str(), score.minCoeff(), score.maxCoeff());

    //3. select keys and store fea score
    std::vector&lt;size_t&gt; ids;
    for (size_t i = 0; i &lt; items.size(); ++i) {
      *fea_scores-&gt;Raw&lt;float&gt;(items[i].id) = score(i);
      if (score(i) &lt; score_threshold) {
        ids.push_back(items[i].id);
      }
    }

    //4. delete
    ctx-&gt;GetServerLocker()-&gt;ChangeType(QRWLocker::kWrite);
    tbb::concurrent_vector&lt;size_t&gt; unfiltered_ids;
    size_t del_size = hashmap-&gt;EraseById(ctx-&gt;GetVariableName(), ids, &amp;unfiltered_ids);
    ...
    return Status::Ok();
  }
};
</code></pre>
<p>hashmap 完成删除工作，把id放到free_list_里面，供特征准入时重用</p>
<pre><code class="language-cpp">virtual size_t EraseById(const std::string&amp; variable_name, std::vector&lt;size_t&gt;&amp; ids, tbb::concurrent_vector&lt;size_t&gt;* unfiltered_ids) {
    std::atomic&lt;size_t&gt; size(0);
    tbb::concurrent_vector&lt;KeyType&gt; keys;
    std::sort(ids.begin(), ids.end());
    tbb::parallel_for_each(begin(table_), end(table_), [&amp;](const std::pair&lt;KeyType, size_t&gt;&amp; pr) {
      auto iter = std::lower_bound(ids.begin(), ids.end(), pr.second);
      if (iter != ids.end() &amp;&amp; *iter == pr.second) {
        keys.push_back(pr.first);
        free_list_.push(pr.second);
        size++;
      } else {
        unfiltered_ids-&gt;push_back(pr.second);
      }
    });
    for (auto&amp;&amp; key : keys) {
      table_.unsafe_erase(key);
    }
    LOG(INFO) &lt;&lt; &quot;Filter for &quot; + variable_name + &quot;, clear=&quot; + std::to_string(keys.size()) + &quot;, left=&quot; + std::to_string(table_.size());
    return size;
  }
</code></pre>
<h1 id="3-更新-改">3. 更新 (改)</h1>
<p>这一部分我们介绍filter 的slot 和 statis 的slot的计算更新方法</p>
<p>decay_rate 是statis自带的，维度是label_len+1，通过label累加计算出来，通过HashSlotUpdaterHook 进行更新slot值，通过HashFeatureDecayHook对slot进行decay</p>
<p>accumulate 是adagrade优化器的参数，优化器负责data和自身参数的更新，我们这部分主要介绍 slot 的更新。</p>
<p>可以发现带有衰减率的两个slot，需要两个hook来分别完成slot的更新和decay_rate的更新，并且是先更新slot，再更新decay_rate，并且hook的优先级能够保证，先更新slot，然后才考虑各个filter。</p>
<p>下面我们以 __dacay_rate_xx 这个slot的更新为例，来介绍slot的更新，顺便介绍统计特征。</p>
<p>__dacay_rate_xx 的更新分为两部分，一是计算统计增量更新slot，二是应用decat_rate更新slot</p>
<h2 id="1-计算统计增量更新slot">1） 计算统计增量更新slot</h2>
<p>先通过 FeaStatsCpuOp 计算要更新的增量</p>
<pre><code class="language-cpp">void FeaStatsFunctor&lt;CpuDevice, T, I&gt;::operator()(CpuDevice* d, const Tensor&amp; sindex, const Tensor&amp; ssegment,
                                                  const Tensor&amp; fea_stat_input, Tensor* stat_delta) {
  ...
  size_t out_fea_stat_dim = in_fea_stat_dim + 1;
  TensorShape out_shape({ssegment.Shape()[0], out_fea_stat_dim});
  *stat_delta = Tensor(d, out_shape, DataTypeToEnum&lt;T&gt;::v());
  T* pout = stat_delta-&gt;Raw&lt;T&gt;();
  std::memset(pout, 0, sizeof(T) * out_shape.NumElements());
  ...

  #pragma omp parallel for
  for (size_t i = 0; i &lt; id_num; ++i) {
    size_t sseg_idx = std::lower_bound(psseg, psseg + sseg_size, i + 1) - psseg;
    T* src = pclick + psindex[i] * in_fea_stat_dim;
    T* dst = pout + sseg_idx * out_fea_stat_dim;

    // show
    common::cpu_atomic_add&lt;T&gt;(1, dst);
    // others
    for (size_t j = 0; j &lt; in_fea_stat_dim; j++) {
      common::cpu_atomic_add&lt;T&gt;(src[j], dst + j + 1);
    }
  }

};
</code></pre>
<p>HashSlotsUpdateHook 通过调用 client 的 ps_sparse_push_slots_op 完成对slot的更新，调用链路如下：</p>
<p>(client)ps_sparse_push_slots_op → (client)HashPushSlots → (ps)BuildHashSlice → (ps)AssignAddSlotUpdater</p>
<p>client 的 HashPushSlots 代码如下</p>
<pre><code class="language-cpp">void Client::HashPushSlots(...) {
  ...
  std::vector&lt;Data*&gt; inputs = Args(ids_vec, name_vec, save_ratio_vec, true, insertable);
  UdfData slice_udf(&quot;BuildHashSlice&quot;, UdfData(0), UdfData(1), UdfData(2), UdfData(3), UdfData(4));
  std::vector&lt;std::unique_ptr&lt;Data&gt;&gt;* outputs = 
    new std::vector&lt;std::unique_ptr&lt;Data&gt;&gt;;
  std::vector&lt;Partitioner*&gt; splitter = {
    new partitioner::HashId,
    ...
  };
  std::vector&lt;UdfData&gt; udf_chain;
  for (size_t i = 0; i &lt; slot_names.size(); i++) {
    UdfData one_updater = UdfData(updaters[i], slice_udf, UdfData(5+i*2), UdfData(6+i*2));
    std::string slot_name = slot_names[i];
    inputs.push_back(Args(slot_name)[0]);
    std::vector&lt;ps::Tensor&gt; grad = {grads[i]};
    inputs.push_back(Args(grad)[0]);
    splitter.push_back(new partitioner::Broadcast);
    splitter.push_back(new partitioner::HashData);
    udf_chain.push_back(one_updater);
  }
  ...

  Process(udf_chain, variable_name, inputs, splitter, 
          combiner, outputs, realcb);
}  
</code></pre>
<p>splitter 中的 HashId 和 HashData 会分离出不同的数据发给不同的ps</p>
<pre><code class="language-cpp">Status SparseData::Split(PartitionerContext* ctx, Data* src, std::vector&lt;Data*&gt;* dst) {
    ...
    dst-&gt;clear();
    for (size_t i = 0; i &lt; info-&gt;parts.size(); ++i) {
      WrapperData&lt;std::vector&lt;Tensor&gt;&gt;* result = new WrapperData&lt;std::vector&lt;Tensor&gt;&gt;();
      dst-&gt;emplace_back(result);
      ctx-&gt;AddDeleter(result);
    }
    for (size_t i = 0; i &lt; data_vec.size(); ++i) {
      std::vector&lt;Data*&gt; one_dst;
      Status one_status = SplitOneSparseData(ctx, data_vec[i], &amp;one_dst, i);
      if (!one_status.IsOk()) {
        return one_status;
      }
      for(size_t j = 0; j &lt; one_dst.size(); ++j) {
        dynamic_cast&lt;WrapperData&lt;std::vector&lt;Tensor&gt;&gt;*&gt;((*dst)[j])-&gt;Internal().push_back(dynamic_cast&lt;WrapperData&lt;Tensor&gt;*&gt;(one_dst[j])-&gt;Internal());
      }
    }
    ...
}
</code></pre>
<p>buildhashslice 的代码上面分析过，就是会做特征准入</p>
<p>AssignAddSlotUpdater 完成slot的更新</p>
<pre><code class="language-cpp">class AssignAddSlotUpdater : public SimpleUdf&lt;vector&lt;Slices&gt;, std::string, vector&lt;Tensor&gt; &gt; {
 public:
  virtual Status SimpleRun(
      UdfContext* ctx, const vector&lt;Slices&gt;&amp; sslices, const std::string&amp; slot_name, const vector&lt;Tensor&gt;&amp; grad_tensors) const {
      ...
      Tensor* assigned_tensor;
      ...
      PS_CHECK_BOOL(assigned_slice_size == slice_size, Status::ArgumentError(&quot;var &quot; + var_name + &quot; slot &quot; + slot_name + &quot; AssignAddSlotUpdater shape mismatch.&quot;));
      CASES(grad_tensor.Type(), MultiThreadDo(slices.slice_id.size(), [&amp;](const Range&amp; r) {
                for (size_t i = r.begin; i &lt; r.end; i++) {
                  int64_t slice = slices.slice_id[i];
                  if ((int64_t)slice == ps::HashMap::NOT_ADD_ID) {
                    continue;
                  }
                  T* grad = grad_tensor.Raw&lt;T&gt;(i);
                  T* data = assigned_tensor-&gt;Raw&lt;T&gt;(slice);
                  for (size_t j = 0; j &lt; slice_size; j++) {
                    *data += *grad;
                    data++;grad++;
                  }
                ...
};
</code></pre>
<h2 id="2-应用decay-更新slot-仅针对fea_statis-和-__decay_rate_xx">2） 应用decay 更新slot （仅针对fea_statis 和 __decay_rate_xx）</h2>
<p>每个interval 会执行一次</p>
<p>调用链路为（client）HashFeatureDecayHook → （client）ps_hash_feature_decay_op → (ps) HashFeatureDecay</p>
<pre><code class="language-cpp">
    ...
    ps::client::UdfData udf(&quot;HashFeatureDecay&quot;,
                            ps::client::UdfData(0),
                            ps::client::UdfData(1),
                            ps::client::UdfData(2),                            
                            ps::client::UdfData(3));


    std::vector&lt;ps::client::Partitioner*&gt; spliters{
      new ps::client::partitioner::Broadcast,
      new ps::client::partitioner::Broadcast,
      new ps::client::partitioner::Broadcast,        
      new ps::client::partitioner::Broadcast};

    client-&gt;Process(udf, var_name_,
                    client-&gt;Args(slot_names_, decay_rates, decay_intervals, decay_mark),
                    spliters, {}, outputs, cb);
   ...
</code></pre>
<p>HashFeatureDecay 计算衰减</p>
<pre><code class="language-cpp">
    ...
      PS_CHECK_STATUS(variable-&gt;GetExistSlot(slots[i], &amp;tensor));
      auto&amp; shape = tensor-&gt;Shape();
      ...
      decay_rate = pow(decay_rate, ((current_decay_mark - *last_decay_mark) * 1.0 / decay_intervals[i]));

      CASES(tensor-&gt;Type(), MultiThreadDo(items.size(), [&amp;](const Range&amp; r) {
            for (size_t j = r.begin; j &lt; r.end; ++j) {
              T* dst = tensor-&gt;Raw&lt;T&gt;(items[j].id);
              for (size_t k = 0; k &lt; shape[1]; ++k) {
                *(dst + k) *= decay_rate;
              }
            }
            return Status::Ok();
          }));
   ...
</code></pre>
<h1 id="4-拉取最新数据查">4. 拉取最新数据(查)</h1>
<p>emb 和 statis 中需要拉取新数据对就是 var.gather 和 var.gather_slots 两个操作, 分别为特定的ids 从ps 上获取 data 和 slot</p>
<p>gather 的调用链路是 : （client）ps_sparse_pull_op→ (client)HashPull→ (ps) BuildHashSlice  → (ps)TransSlice → (client) HashData.Combine</p>
<p>使用 HashId 对id 按照ps server 进行分隔，如果上面的 HashData, BuildHashSlice 的代码上面也分析过了，就是进行特征准入，如果这个hashkey 在hashmap里没有就加进去并初始化，TransSlice 就是获取data</p>
<pre><code class="language-cpp">Status SparseData::Combine(PartitionerContext* ctx, Data* src, size_t server_id, std::unique_ptr&lt;Data&gt;* output) {
  ...
  char* res_ptr = result-&gt;Raw&lt;char&gt;();
  shape.Set(0, 1);
  size_t single_size = shape.NumElements() * SizeOfType(type);
  MultiThreadDo(slices.ids[server_id].size(), [&amp;](const Range&amp; r) {
        char* src_ptr = data.Raw&lt;char&gt;() + r.begin * single_size;
        for (size_t i = r.begin; i &lt; r.end; ++i) {
          size_t item = slices.ids[server_id][i];
          memcpy(res_ptr + item * single_size, src_ptr, single_size);
          src_ptr += single_size;
        }
        return Status::Ok();
      }, 1000);
  return Status::Ok();
}
</code></pre>
<p>gather_slot 的调用链路是  （client）ps_sparse_pull_slots_op→ (client)HashPullSlots→ (ps) BuildHashSlice  → (ps) TransSlotsSlice → (ps)TransTensorSliceOffVector →  (client)HashAuxiliaryData.Combine</p>
<p>BuildHashSlice 就是进行特征准入，如果这个hashkey 在hashmap里没有就加进去并初始化，TransSlotsSlice 就是获取 slot， combine 是把相同id 的slot的值放在一行里</p>
<pre><code class="language-cpp">std::vector&lt;Partitioner*&gt; combiner = {
    new partitioner::HashAuxiliaryData({slots_dim_sum})
  };


class HashAuxiliaryData : public SparseData {
 public:
  HashAuxiliaryData(const std::vector&lt;size_t&gt;&amp; dims, size_t id = 0) : SparseData(id), dims_(dims) {}
  virtual Status CombineInit(PartitionerContext* ctx, std::unique_ptr&lt;Data&gt;* output) override;
 protected:
  std::vector&lt;size_t&gt; dims_;
};


Status HashAuxiliaryData::CombineInit(PartitionerContext* ctx, std::unique_ptr&lt;Data&gt;* output) {
  ...
  std::vector&lt;size_t&gt; dims;
  dims.push_back(slices.id_size);
  for (size_t i = 0; i &lt; dims_.size(); ++i) {
    dims.push_back(dims_[i]);
  }
  TensorShape shape(dims);
  DataType type = info-&gt;datatype;
  output-&gt;reset(new WrapperData&lt;Tensor&gt;(type, shape, new initializer::NoneInitializer));
  return Status::Ok();
}
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[XDL 的特征准入与退出机制]]></title>
        <id>https://DragonFive.github.io/post/xdl-de-te-zheng-zhun-ru-yu-tui-chu-ji-zhi/</id>
        <link href="https://DragonFive.github.io/post/xdl-de-te-zheng-zhun-ru-yu-tui-chu-ji-zhi/">
        </link>
        <updated>2019-11-08T03:56:04.000Z</updated>
        <content type="html"><![CDATA[<p>本文主要介绍 xdl 中的 embedding 和 statis 的底层数据结构。</p>
<p>embedding 和 statis 其实都是保存在ps 端上的 variable，包含 data 和 slot 部分。</p>
<p>只不过对于statis来说主体是__decay_rate 这个slot，data部分是shape 为[featuredim,1]，初始化方式为zeros的tensor，并没有被用到。</p>
<p>在 ps server 端，variable 的结构如下：</p>
<pre><code class="language-c++">class Variable {
 
 
 private:
  std::unique_ptr&lt;Tensor&gt; data_;//emd_dim的参数
  std::unique_ptr&lt;Data&gt; slicer_;//hashmap,从hashkey到id的映射
  std::unordered_map&lt;std::string, Slot&gt; slots_;//slot_name到对应slot
 public:
  struct Slot {
    std::unique_ptr&lt;Tensor&gt; tensor;
    SlotJoiner joiner;
  };
};
</code></pre>
<p>hashmap 管理特征准入和退出策略，我们可以从函数签名中看到 ，建立hashkey到id的映射关系，ids存的是我们输入的key在data这个tensor中的位置</p>
<p>add_probability 会在这里起作用，进行特征准入，reused_ids会把一些id剔除</p>
<h1 id="特征准入">特征准入</h1>
<pre><code class="language-c++">int64_t Get(const int64_t* keys,
                 size_t size, bool not_insert,
                 float add_probability,
                 std::vector&lt;size_t&gt;* ids,
                 tbb::concurrent_vector&lt;size_t&gt;* reused_ids,
                 size_t* filtered_keys,
                 size_t block_size = 500, size_t timeout = 1800)
</code></pre>
<p>其中 add_probability 就是以一定概率对 feature key 进行准入。</p>
<h1 id="特征退出">特征退出</h1>
<pre><code class="language-c++">if (max_id &gt; 0) {
  PS_CHECK_STATUS(variable-&gt;ReShapeId(max_id));
}
if (reused_ids.size() != 0) {
  std::vector&lt;size_t&gt; raw_reused_ids;
  for (auto iter : reused_ids) {
    raw_reused_ids.push_back(iter);
  }
  variable-&gt;ClearIds(raw_reused_ids);
}
</code></pre>
<p>同一个 variable，在ps会通过均衡策略保存不同的部分到不同的ps上。<br>
向ps请求一个variable的指定key的时候，就需要计算分别向哪些ps请求哪些key，这部分工作是在client端的partition代码完成的</p>
<p>以初始化PsConstantInitializerOp为例,在client端调用HashInitializer，在client中调用partitioner::HashShape来计算在各个ps上的形状</p>
<pre><code class="language-c++">
void Client::HashInitializer(const std::string&amp; variable_name,
                             Initializer* init,
                             const Client::Callback&amp; cb) {
  VariableInfo info;
  CHECK_ASYNC(GetVariableInfo(variable_name, &amp;info));
  std::string extra_info;
  for (auto&amp; arg : info.args) {
    extra_info += arg.first + &quot;=&quot; + arg.second + &quot;&amp;&quot;;
  }
  if (!extra_info.empty()) { extra_info.pop_back(); }
  std::vector&lt;Data*&gt; inputs = Args(0, 0, extra_info, std::unique_ptr&lt;Initializer&gt;(init));
  std::vector&lt;std::unique_ptr&lt;Data&gt;&gt;* outputs =
    new std::vector&lt;std::unique_ptr&lt;Data&gt;&gt;;
  std::vector&lt;Partitioner*&gt; splitter = {
    new partitioner::HashDataType,
    new partitioner::HashShape,
    new partitioner::Broadcast,
    new partitioner::Broadcast
  };
  std::vector&lt;Partitioner*&gt; combiner = {};
  UdfData udf(&quot;HashVariableInitializer&quot;, UdfData(0), UdfData(1), UdfData(2), UdfData(3));
  ...
}
 
 
Status HashShape::Split(PartitionerContext* ctx, Data* src, std::vector&lt;Data*&gt;* dst) {
  VariableInfo* info = ctx-&gt;GetVariableInfo();
  std::vector&lt;size_t&gt; dims(info-&gt;shape.begin(), info-&gt;shape.end());
  dst-&gt;clear();
  for (size_t i = 0; i &lt; info-&gt;parts.size(); i++) {
    size_t k = info-&gt;parts[i].size * info-&gt;shape[0] / Hasher::kTargetRange;
    dims[0] = k + 10 * sqrt(k) + 10;
    Data* d = new WrapperData&lt;TensorShape&gt;(dims);
    ctx-&gt;AddDeleter(d);
    dst-&gt;push_back(d);
  }
  return Status::Ok();
}

</code></pre>
<p>client 通过 udf 通知ps做什么样的操作，比如这个初始化就是告诉 ps server 调用 HashVariableInitializer 的simplerun方法进行初始化</p>
<pre><code class="language-c++">class HashVariableInitializer : public SimpleUdf&lt;DataType, TensorShape, std::string, std::unique_ptr&lt;Initializer&gt;&gt; {
 public:
  virtual Status SimpleRun(
      UdfContext* ctx,
      const DataType&amp; dt,
      const TensorShape&amp; shape,
      const std::string&amp; extra_info,
      const std::unique_ptr&lt;Initializer&gt;&amp; initializer) const {
    ...
    std::vector&lt;std::string&gt; slots;
    ...
    ps::Status status = GetStorageManager(ctx)-&gt;Get(var_name, &amp;var);
    ...
    if (!status.IsOk()) {
      return ctx-&gt;GetStorageManager()-&gt;Set(var_name, [&amp;]{
            HashMap* hashmap = new HashMapImpl&lt;int64_t&gt;(init_shape[0]);
            Variable* var = new Variable(new Tensor(dt, init_shape, initializer-&gt;Clone(), Tensor::TType::kSegment, true), new WrapperData&lt;std::unique_ptr&lt;HashMap&gt; &gt;(hashmap), var_name);
            Status st = InitSlots(slots, var);
            ...
            return var;
          });
    } else {
      ...
  }
 
 
  Status InitSlots(const std::vector&lt;std::string&gt;&amp; slots, Variable* var) const {
    for (auto&amp; slot : slots) {
      ...
      Tensor* t = var-&gt;GetVariableLikeSlot(tuple[0], dtype, TensorShape(inner_dims), []{ return new initializer::ConstantInitializer(0); });
      ...
    return Status::Ok();
  }
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[tensorflow 1.x 的分布式训练]]></title>
        <id>https://DragonFive.github.io/post/tensorflow-1x-de-fen-bu-shi-xun-lian/</id>
        <link href="https://DragonFive.github.io/post/tensorflow-1x-de-fen-bu-shi-xun-lian/">
        </link>
        <updated>2019-09-05T07:04:42.000Z</updated>
        <content type="html"><![CDATA[<h1 id="模型并行">模型并行</h1>
<p>将模型部署到很多设备上（设备可能分布在不同机器上）运行，由于模型分割开的各个部分之间有相互依赖关系，因此计算效率不高。所以在模型大小不算太大的情况下一般不使用模型并行。</p>
<h1 id="数据并行">数据并行</h1>
<p>相比较模型并行，数据并行方式能够支持更大的训练规模，提供更好的扩展性，因此数据并行是深度学习最常采用的分布式训练策略。</p>
<blockquote>
<p>in-graph replication和between-graph replication 都用于数据并行。<br>
所谓 replication，指的是各个task，replication的对象是模型。<br>
在使用in-graph replication方式时，只有一个client进程（可以在参与训练的CPU或GPU上任选一个task来运行这个client，参与计算的其它tasks不运行这个client）来创建模型（即tf.Graph）及模型的参数（那些tf.Variables，比如权重W和偏置b）。由于参数（W和b）是共享的，该client指定把参数放在/job:ps，即parameter server上（比如 /job:ps/task:0/cpu:0）。模型的计算部分（前向传播，后向传播，loss和梯度计算，等等）也由该client进程定义好，然后client进程把这个计算部分分配到各个GPU device上（这个过程就相当于在各个GPU中复制模型），分配的方式类似函数调用，但每次调用都指定了设备（即 /job:worker/task:0/gpu:0，/job:worker/task:1/gpu:0，等等）。调用时，模型的参数（即W和b）被当作函数的参数输入给不同tasks（通常运行在不同GPU上）运行的模型，以保证这些参数确实是共享的。<br>
如果用between-graph replication方式，则每个task都运行自己的client进程用于创建模型和参数，并将参数pin到parameter server上（比如 /job:ps/task:0/cpu:0），然后各自独立地执行该模型。注意，每个task创建的模型必须一模一样，这很容易做到，因为只要每个task里的这部分代码都一样就行了。问题是，这些task各自创建并pin到parameter server上的模型参数是同样的吗？问这个问题是因为我们现在跑的是数据并行，而模型的参数及其更新都必须由parameter server统一处理。回答是，只要各task使用同样的parameter server设备名（比如都用 /job:ps/task:0/cpu:0）和同样的变量名（那些tf.Variable定义的变量，比如权重和偏置变量)， 那么在默认的情况下，它们被分配在parameter server的相同的存储里。</p>
</blockquote>
<p>由于in-graph replication的性能不好，现在基本上只使用between-graph replication了。</p>
<h1 id="参数更新方式">参数更新方式</h1>
<p>数据并行参数更新方式可以是同步的（synchronous），也可以是异步的（asynchronous）。</p>
<p>百度的综述<a href="https://arxiv.org/abs/2003.05622">Distributed Hierarchical GPU Parameter Server for Massive Scale Deep Learning Ads Systems</a> 介绍了三种同步模式：</p>
<ul>
<li>
<p>BSP(bulk sync parallel)，严格对所有worker的更新进行同步</p>
</li>
<li>
<p>SSP(stale sync parallel)，对快worker 进行同步</p>
</li>
<li>
<p>ASP（async parallel）, 不同步gradient</p>
</li>
</ul>
<p>后两种方式虽然提升了训练效率，但是降低了模型性能</p>
<p>XDL使用的是ASP。在tensorflow中异步训练是默认的并行训练模式。</p>
<h2 id="异步训练">异步训练</h2>
<p>异步训练中，各个设备完成一个mini-batch训练之后，不需要等待其它节点，直接去更新模型的参数。异步训练总体会训练速度会快很多，但是异步训练的一个很严重的问题是梯度失效问题（stale gradients），刚开始所有设备采用相同的参数来训练，但是异步情况下，某个设备完成一步训练后，可能发现模型参数已经被其它设备更新过了，此时这个设备计算出的梯度就过期了。由于梯度失效问题，异步训练可能陷入次优解。</p>
<h2 id="同步训练">同步训练</h2>
<p>所谓同步指的是所有的设备都是采用相同的模型参数来训练，等待所有设备的mini-batch训练完成后，收集它们的梯度后执行模型的一次参数更新。</p>
<p>Tensorflow提供了tf.train.SyncReplicasOptimizer类用于执行同步训练。把异步训练改造成同步训练只需要两步：</p>
<p>在原来的Optimizer上封装SyncReplicasOptimizer，将参数更新改为同步模式；</p>
<pre><code class="language-python">optimizer = tf.train.SyncReplicasOptimizer(optimizer, replicas_to_aggregate=num_workers)
</code></pre>
<p>在MonitoredTrainingSession或者EstimatorSpec的hook中增加sync_replicas_hook：</p>
<pre><code class="language-python"> sync_replicas_hook = optimizer.make_session_run_hook(is_chief, num_tokens=0)
</code></pre>
<p>同步训练需要各个设备的计算能力要均衡，而且要求集群的通信也要均衡，慢worker会拖慢整体进度。</p>
<h1 id="tensorflow-1-分布式架构">tensorflow 1 分布式架构</h1>
<p>2017年2月百度在PaddlePaddle平台上首次引入了ring-allreduce的架构，随后将其提交到tensorflow的contrib package中。同年8月，Uber为tensorflow平台开源了一个更加易用和高效的ring allreduce分布式训练库Horovod。<br>
最后，tensorflow官方终于也在1.11版本中支持了allreduce的分布式训练策略CollectiveAllReduceStrategy，其跟estimator配合使用非常方便，只需要构造tf.estimator.RunConfig 对象时传入CollectiveAllReduceStrategy参数即可。</p>
<p>关于 ring-allreduce 之前总结在 <a href="https://dragonfive.github.io/post/fen-bu-shi-jia-gou-ring-all-reduce-suan-fa/">分布式架构：ring all-reduce算法</a>。</p>
<h2 id="使用-tensorflow-estimator-api-来编写分布式训练代码">使用 TensorFlow Estimator API 来编写分布式训练代码</h2>
<p>要让tensorflow分布式运行，首先我们需要定义一个由参与分布式计算的机器组成的集群，如下：</p>
<pre><code class="language-py">cluster = {'chief': ['host0:2222'], 'ps': ['host1:2222', 'host2:2222'], 'worker': ['host3:2222', 'host4:2222', 'host5:2222']}
</code></pre>
<p>集群中一般有多个worker，需要指定其中一个worker为主节点（cheif），chief节点会执行一些额外的工作，比如模型导出之类的。在PS分布式架构环境中，还需要定义ps节点。</p>
<p>要运行分布式Estimator模型，只需要设置好TF_CONFIG环境变量即可，可参考如下代码：</p>
<pre><code class="language-py"># Example of non-chief node:
os.environ['TF_CONFIG'] = json.dumps( {'cluster': cluster, 'task': {'type': 'worker', 'index': 1}})

# Example of chief node:
os.environ['TF_CONFIG'] = json.dumps( {'cluster': cluster, 'task': {'type': 'chief', 'index': 0}}) 

# Example of evaluator node (evaluator is not part of training cluster) 
os.environ['TF_CONFIG'] = json.dumps( {'cluster': cluster, 'task': {'type': 'evaluator', 'index': 0}})
</code></pre>
<p>定义好上述环境变量后，调用tf.estimator.train_and_evaluate即可开始分布式训练和评估，其他部分的代码跟开发单机的程序是一样的，可以参考下面的资料：<br>
<a href="https://zhuanlan.zhihu.com/p/41473323">构建分布式Tensorflow模型系列:Estimator - 知乎</a></p>
<h1 id="参考资料">参考资料</h1>
<p><a href="https://zhuanlan.zhihu.com/p/56991108">一文说清楚Tensorflow分布式训练必备知识 - 知乎</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/60474307">什么是in-graph replication和between-graph replication? - 知乎</a></p>
]]></content>
    </entry>
</feed>