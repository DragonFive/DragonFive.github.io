<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://DragonFive.github.io/</id>
    <title>dragon</title>
    <updated>2019-11-14T02:37:09.324Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://DragonFive.github.io/"/>
    <link rel="self" href="https://DragonFive.github.io//atom.xml"/>
    <subtitle>Code is cheap, show me the theory</subtitle>
    <logo>https://DragonFive.github.io//images/avatar.png</logo>
    <icon>https://DragonFive.github.io//favicon.ico</icon>
    <rights>All rights reserved 2019, dragon</rights>
    <entry>
        <title type="html"><![CDATA[CNN中卷积计算的内存和速度优化]]></title>
        <id>https://DragonFive.github.io//post/cnn-improve</id>
        <link href="https://DragonFive.github.io//post/cnn-improve">
        </link>
        <updated>2017-09-20T02:31:37.000Z</updated>
        <summary type="html"><![CDATA[<hr>
<p>title: CNN中卷积计算的内存和速度优化</p>
<p>date: 2017/9/20 12:04:12</p>
<p>categories:</p>
<ul>
<li>深度学习<br>
tags:</li>
<li>deeplearning</li>
<li>网络优化</li>
<li>神经网络</li>
</ul>
<hr>
<p>在现在的DNN中，不管是前向传播还是反向传播，绝大多数时间花费在卷积计算中。因此对于速度提升来说，优化卷积层意义重大。</p>
<p>虽说从参数量来讲，早期的一些网络(alexbnet,VGG，googlnet等)70%以上的参数都是全连接层的。但是现在从架构上的改进已经开始减少全连接层了，比如squeezenet,mobilenet已经使用global avg pooling层取代全连接层了。那么接下来再想提速那就得从卷积层下手了。当然还有一中思路是从量化的方式减少参数量和内存消耗的（如BNN，eBNN），对于提速来说意义并不大。</p>
]]></summary>
        <content type="html"><![CDATA[<hr>
<p>title: CNN中卷积计算的内存和速度优化</p>
<p>date: 2017/9/20 12:04:12</p>
<p>categories:</p>
<ul>
<li>深度学习<br>
tags:</li>
<li>deeplearning</li>
<li>网络优化</li>
<li>神经网络</li>
</ul>
<hr>
<p>在现在的DNN中，不管是前向传播还是反向传播，绝大多数时间花费在卷积计算中。因此对于速度提升来说，优化卷积层意义重大。</p>
<p>虽说从参数量来讲，早期的一些网络(alexbnet,VGG，googlnet等)70%以上的参数都是全连接层的。但是现在从架构上的改进已经开始减少全连接层了，比如squeezenet,mobilenet已经使用global avg pooling层取代全连接层了。那么接下来再想提速那就得从卷积层下手了。当然还有一中思路是从量化的方式减少参数量和内存消耗的（如BNN，eBNN），对于提速来说意义并不大。</p>
<!--more-->
<h1 id="以往的卷积计算方法">以往的卷积计算方法</h1>
<h2 id="sum循环法">sum循环法</h2>
<p>时间复杂度最高，为 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><mi>H</mi><mi>W</mi><mi>M</mi><mi>K</mi><mi>K</mi><mi>C</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">O(HWMKKC)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.08125em;">H</span><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="mord mathdefault" style="margin-right:0.07153em;">K</span><span class="mord mathdefault" style="margin-right:0.07153em;">K</span><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="mclose">)</span></span></span></span> 最笨的方法，只是用来理解。</p>
<pre><code>input[C][H][W];
kernels[M][K][K][C];
output[M][H][W];
for h in 1 to H do
	for w in 1 to W do
		for o in 1 to M do
			sum = 0;
			for x in 1 to K do
				for y in 1 to K do
					for i in 1 to C do
						sum += input[i][h+y][w+x]
						*kernels[o][x][y][i];
			output[o][w][h] = sum;
</code></pre>
<h2 id="patch-building-dnn-convolution-algorithms">patch-building DNN convolution algorithms</h2>
<p>based on gemm convolution algorithm</p>
<p>优点是：比较简单，方便理解和计算</p>
<p>缺点是：需要大量的内存做中间存储</p>
<h3 id="im2col过程">im2col过程</h3>
<p>图片来自 ：贾扬清的demo convolution in caffe<br>
<a href="https://www.zhihu.com/question/28385679">在 Caffe 中如何计算卷积？</a></p>
<p><img src="https://www.github.com/DragonFive/CVBasicOp/raw/master/1507194578418.jpg" alt="1"><br>
把卷积的第一个感受野里的矩阵转化成一个vector，并把各个channel的feature连接起来。<br>
<img src="https://www.github.com/DragonFive/CVBasicOp/raw/master/1507194598695.jpg" alt="2"><br>
随着划窗的进行，把接下来的窗口都转化乘vector,并排放在下面<br>
<img src="https://www.github.com/DragonFive/CVBasicOp/raw/master/1507194615038.jpg" alt="3"></p>
<p><img src="https://www.github.com/DragonFive/CVBasicOp/raw/master/1507194630559.jpg" alt="4"><br>
最后把有Cout个卷积核，每个卷积核有C个channel,那么转化乘Cout行的vector组。最后卷积就编程矩阵乘法了。</p>
<h2 id="各种方法占用内存量">各种方法占用内存量</h2>
<figure data-type="image" tabindex="1"><img src="https://www.github.com/DragonFive/CVBasicOp/raw/master/1507511974026.jpg" alt="enter description here"></figure>
<h1 id="reference">reference</h1>
<p><a href="https://www.zhihu.com/question/28385679">在 Caffe 中如何计算卷积？</a></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[神经网络的压缩优化方法总结]]></title>
        <id>https://DragonFive.github.io//post/network_compression</id>
        <link href="https://DragonFive.github.io//post/network_compression">
        </link>
        <updated>2017-08-05T02:15:50.000Z</updated>
        <summary type="html"><![CDATA[<hr>
<p>title: 神经网络的压缩优化方法总结</p>
<p>date: 2017/8/5 12:04:12</p>
<p>categories:</p>
<ul>
<li>深度学习<br>
tags:</li>
<li>deeplearning</li>
<li>网络优化</li>
<li>神经网络</li>
</ul>
<hr>
<p>这里回顾一下几个经典模型，我们主要看看深度和caffe模型大小，<a href="https://dragonfive.github.io/2017-07-05/deep_learning_model/">神经网络模型演化</a>。并总结一些模型压缩优化的方法。</p>
<p>![各种CNN模型][1]</p>
]]></summary>
        <content type="html"><![CDATA[<hr>
<p>title: 神经网络的压缩优化方法总结</p>
<p>date: 2017/8/5 12:04:12</p>
<p>categories:</p>
<ul>
<li>深度学习<br>
tags:</li>
<li>deeplearning</li>
<li>网络优化</li>
<li>神经网络</li>
</ul>
<hr>
<p>这里回顾一下几个经典模型，我们主要看看深度和caffe模型大小，<a href="https://dragonfive.github.io/2017-07-05/deep_learning_model/">神经网络模型演化</a>。并总结一些模型压缩优化的方法。</p>
<figure data-type="image" tabindex="1"><img src="https://www.github.com/DragonFive/CVBasicOp/raw/master/1502693251377.jpg" alt="各种CNN模型"></figure>
<!--more-->
<p>模型大小(参数量)和模型的深浅并非是正相关。</p>
<h1 id="一些经典的模型-修改网络架构">一些经典的模型-修改网络架构</h1>
<h2 id="fully-connect-to-local-connect-全连接到卷积神经网络-1x1卷积">fully connect to local connect 全连接到卷积神经网络  1x1卷积</h2>
<p>Alexnet<a href="https://www.github.com/DragonFive/CVBasicOp/raw/master/1502693251377.jpg">1</a>是一个8层的卷积神经网络，有约<strong>60M个参数</strong>，如果采用<strong>32bit float存下来有200M</strong>。值得一提的是，AlexNet中仍然有3个全连接层，其参数量占比参数总量超过了90%。</p>
<p>下面举一个例子，假如输入为28×28×192，输出feature map通道数为128。那么，直接接3×3卷积，参数量为3×3×192×128=221184。</p>
<p>如果先用<strong>1×1卷积进行降维到96个通道</strong>，然后再用3×3升维到128，则参数量为：1×1×192×96+3×3×96×128=129024，参数量减少一半。虽然参数量减少不是很明显，但是如果1×1输出维度降低到48呢？则参数量又减少一半。对于上千层的大网络来说，效果还是很明显了。</p>
<p>移动端对模型大小很敏感。下载一个100M的app与50M的app，首先用户心理接受程度就不一样。</p>
<p>原则上降低通道数是会降低性能的，这里为什么却可以降维呢？我们可以从很多embedding技术，比如PCA等中得到思考，<strong>降低一定的维度可以去除冗余数据</strong>，损失的精度其实很多情况下都不会对我们解决问题有很大影响。</p>
<p>1×1卷积，在 GoogLeNet Inception v1以及后续版本，ResNet中都大量得到应用，<strong>有减少模型参数</strong>的作用。</p>
<h2 id="卷积拆分">卷积拆分</h2>
<p>(1) VGG</p>
<p>VGG可以认为是AlexNet的增强版，<strong>两倍的深度，两倍的参数量</strong>。不过，也提出了一个模型压缩的trick，后来也被广泛借鉴。</p>
<p>那就是，<strong>对于5×5的卷积，使用两个3×3的卷积串联</strong>，可以得到同样的感受野，但参数量却有所降低，为3×3×2/(5×5)=0.72，同样的道理3个3×3卷积代替一个7×7，则参数压缩比3×3×3/(7×7)=0.55，降低一倍的参数量，也是很可观的。</p>
<p>(2) GoogLeNet</p>
<p>GoogleLet Inception v2就借鉴了VGG上面的思想。而到了Inception V3网络，则更进一步，将大卷积分解(Factorization)为小卷积。</p>
<p>比如<strong>7×7的卷积，拆分成1×7和7×1</strong>的卷积后。参数量压缩比为1×7×2/(7×7)=0.29，比上面拆分成3个3×3的卷积，更加节省参数了。</p>
<p>问题是这种<strong>非对称的拆分</strong>，居然比对称地拆分成几个小卷积核改进效果更明显，<strong>增加了特征多样性</strong>。</p>
<p>后来的Resnet就不说了，也是上面这些trick。到现在，基本上网络中都是3×3卷积和1×1卷积，5×5很少见，7×7几乎不可见。</p>
<p>(3) SqueezeNet</p>
<p>squeezenet将上面1×1降维的思想进一步拓展。通过减少3×3的filter数量，将其一部分替换为1×1来实现压缩。</p>
<p>具体的一个子结构如下：一个squeeze模块加上一个expand模块，使squeeze中的通道数量，少于expand通道数量就行。</p>
<figure data-type="image" tabindex="2"><img src="https://www.github.com/DragonFive/CVBasicOp/raw/master/1502694686047.jpg" alt="squeezenet网络的expand模块"></figure>
<p>假如输入为M维，如果直接接3×3卷积，输出为7个通道，则参数量：M×3×3×7。</p>
<p>如果按上图的做法，则参数量为M×1×1×3+3×4×1×1+3×4×3×3，压缩比为：(40+M)/21M，当M比较大时，约0.05。</p>
<p>文章最终<strong>将AlexNet压缩到原来1/50</strong>，而性能几乎不变。</p>
<hr>
<p>SqueezeNet的核心指导思想是——在保证精度的同时使用最少的参数。而这也是所有模型压缩方法的一个终极目标。</p>
<p>基于这个思想，SqueezeNet提出了3点网络结构设计策略：</p>
<p>策略 1. 将3x3卷积核替换为1x1卷积核。</p>
<p>这一策略很好理解，因为1个1x1卷积核的参数是3x3卷积核参数的1/9，这一改动理论上可以将模型尺寸压缩9倍。</p>
<p>策略 2. 减小输入到3x3卷积核的输入通道数。</p>
<p>我们知道，对于一个采用3x3卷积核的卷积层，该层所有卷积参数的数量（不考虑偏置）为：<br>
<img src="https://www.github.com/DragonFive/CVBasicOp/raw/master/1507038803077.jpg" alt="enter description here"></p>
<p>N是卷积核的数量，也即输出通道数，C是输入通道数。因此，为了保证减小网络参数，<strong>不仅仅需要减少3x3卷积核的数量，还需减少输入到3x3卷积核的输入通道数量</strong>，即式中C的数量。</p>
<p>策略 3.尽可能的将降采样放在网络后面的层中。</p>
<p>分辨率越大的特征图（延迟降采样）可以带来更高的分类精度，而这一观点从直觉上也可以很好理解，因为分辨率越大的输入能够提供的信息就越多。</p>
<p>上述三个策略中，前两个策略都是针对如何降低参数数量而设计的，最后一个旨在最大化网络精度。</p>
<p><strong>squeeze层</strong>借鉴了inception的思想，利用1x1卷积核来降低输入到expand层中3x3卷积核的输入通道数。</p>
<p>定义squeeze层中1x1卷积核的数量是<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>s</mi><mrow><mn>1</mn><mo>∗</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">s_{1*1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mbin mtight">∗</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，类似的，expand层中1x1卷积核的数量是<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>e</mi><mrow><mn>1</mn><mo>∗</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">e_{1*1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mbin mtight">∗</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>， 3x3卷积核的数量是<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>e</mi><mn>3</mn></msub><mo>∗</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">e_3*3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.61528em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">3</span></span></span></span>。令<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>s</mi><mrow><mn>1</mn><mo>∗</mo><mn>1</mn></mrow></msub><mo>&lt;</mo><msub><mi>e</mi><mrow><mn>1</mn><mo>∗</mo><mn>1</mn></mrow></msub><mo>+</mo><msub><mi>e</mi><mrow><mn>3</mn><mo>∗</mo><mn>3</mn></mrow></msub></mrow><annotation encoding="application/x-tex">s_{1*1} &lt; e_{1*1}+ e_{3*3}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6891em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mbin mtight">∗</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.73333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mbin mtight">∗</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span><span class="mbin mtight">∗</span><span class="mord mtight">3</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>从而保证输入到3x3的输入通道数减小。SqueezeNet的网络结构由若干个 fire module 组成</p>
<p><strong>速度考量</strong><br>
SqueezeNet在网络结构中大量采用1x1和3x3卷积核是有利于速度的提升的，对于类似caffe这样的深度学习框架，在卷积层的前向计算中，采用1x1卷积核可避免额外的im2col操作，而直接利用gemm进行矩阵加速运算，因此对速度的优化是有一定的作用的。</p>
<p>(4) mobilenet</p>
<p>mobilenet也是用卷积拆分的方法</p>
<figure data-type="image" tabindex="3"><img src="https://www.github.com/DragonFive/CVBasicOp/raw/master/1502694868072.jpg" alt="mobilenet"></figure>
<p>作出更多共享的是有一个width Multiplier宽度参数和resolution Multiplier 分辨率参数 ，可以降低更多的参数。</p>
<p>没使用这两个参数的mobilenet是VGGNet的1/30.<br>
<img src="https://www.github.com/DragonFive/CVBasicOp/raw/master/1502695760857.jpg" alt="mobilenet"></p>
<h1 id="权重参数量化与剪枝">权重参数量化与剪枝</h1>
<p>主要是通过权重剪枝，量化编码等方法来实现模型压缩。deep compression是float到uint的压缩,Binarized Neural Networks是 uint 到bool的压缩。</p>
<p>一些技巧：</p>
<ol>
<li>网络修剪<br>
采用当网络权重非常小的时候(小于某个设定的阈值),把它置0,就像二值网络一般；然后屏蔽被设置为0的权重更新，继续进行训练；以此循环，每隔训练几轮过后，继续进行修剪。</li>
<li>权重共享<br>
对于每一层的参数,我们进行k-means聚类,进行量化,对于归属于同一个聚类中心的权重,采用共享一个权重,进行重新训练.需要注意的是这个权重共享并不是层之间的权重共享,这是对于每一层的单独共享</li>
<li>增加L2权重<br>
增加L2权重可以让更多的权重，靠近0，这样每次修剪的比例大大增加。</li>
</ol>
<h2 id="deepcompresion">DeepCompresion</h2>
<figure data-type="image" tabindex="4"><img src="https://www.github.com/DragonFive/CVBasicOp/raw/master/1507039484844.jpg" alt="deep compression pipeline"></figure>
<p>文章早期的工作，是Network Pruning，就是去除网络中权重低于一定阈值的参数后，重新<strong>finetune一个稀疏网络</strong>。在这篇文章中，则进一步添加了量化和编码，思路很清晰简单如下。</p>
<h3 id="网络剪枝移除不重要的连接">网络剪枝：移除不重要的连接</h3>
<p>（1） 普通网络训练；</p>
<p>（2） 删除权重小于一定阈值的连接得到<strong>稀疏网络</strong>；</p>
<p>（3） 对稀疏网络再训练；</p>
<h3 id="权重量化与共享">权重量化与共享</h3>
<p>此处的权值量化基于<strong>权值聚类</strong>，将连续分布的权值<strong>离散化</strong>，从而减小需要存储的权值数量。</p>
<p>让许多连接共享同一权重，使原始存储整个网络权重变为只需要存储码本(有效的权重)和索引；</p>
<p>对于一个4×4的权值矩阵，量化权重为4阶（-1.0，0，1.5，2.0）。</p>
<figure data-type="image" tabindex="5"><img src="https://www.github.com/DragonFive/CVBasicOp/raw/master/1502698285109.jpg" alt="量化过程"></figure>
<p>对weights采用<strong>cluster index进行存储</strong>后，原来需要16个32bit float，现在只需要4个32bit float码字，与16个2bit uint索引，参数量为原来的(16×2+4×32)/(16×32)=0.31。</p>
<p>存储是没问题了，那如何对量化值进行更新呢？事实上，文中仅对码字进行更新。如上图：<strong>将索引相同的地方梯度求和乘以学习率</strong>，叠加到码字。</p>
<p>这样的效果，就等价于不断求取weights的聚类中心。原来有成千上万个weights，现在经过一个有效的聚类后，每一个weights都用其<strong>聚类中心进行替代</strong>.</p>
<p>看下表就知道最终的压缩效率非常可观，把500M的VGG压缩到到了11M，1/50。</p>
<figure data-type="image" tabindex="6"><img src="https://www.github.com/DragonFive/CVBasicOp/raw/master/1502698400878.jpg" alt="压缩VGG"></figure>
<h3 id="霍夫曼编码更高效利用了权重的有偏分布">霍夫曼编码：更高效利用了权重的有偏分布；</h3>
<h2 id="binarized-neural-networks">Binarized Neural Networks</h2>
<ul>
<li>提出了一个BWN（Binary-Weight-Network）和XNOR-Network，前者只对网络参数做二值化，带来约32x的存储压缩和2x的速度提升，而后者对网络输入和参数都做了二值化，在实现32x存储压缩的同时带了58x的速度提升；</li>
<li>提出了一个新型二值化权值的算法；</li>
<li>第一个在大规模数据集如ImageNet上提交二值化网络结果的工作；</li>
<li>无需预训练，可实现training from scratch。</li>
</ul>
<h1 id="reference">reference</h1>
<p><a href="https://cloud.tencent.com/community/article/678192">CNN 模型压缩与加速算法综述</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/25797790">知乎:为了压榨CNN模型，这几年大家都干了什么</a></p>
<p><a href="http://blog.csdn.net/hjimce/article/details/51564774"> 深度学习（六十）网络压缩简单总结</a></p>
<p><a href="https://dragonfive.github.io/2017-07-17/mobilenets/">mobile_net的模型优化</a></p>
<p><a href="https://dragonfive.github.io/2017-07-20/squeeze_net/">squeeze_net的模型优化</a></p>
<p><a href="https://dragonfive.github.io/2017-07-05/deep_learning_model/">神经网络模型演化</a></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[神经网络模型演化]]></title>
        <id>https://DragonFive.github.io//post/cnn_history</id>
        <link href="https://DragonFive.github.io//post/cnn_history">
        </link>
        <updated>2017-07-05T02:34:51.000Z</updated>
        <summary type="html"><![CDATA[<hr>
<p>title: 神经网络模型演化<br>
date: 2017/7/5 17:38:58</p>
<p>categories:</p>
<ul>
<li>深度学习<br>
tags:</li>
<li>deeplearning</li>
<li>alexnet</li>
<li>googlenet</li>
<li>caffenet</li>
<li>caffe</li>
</ul>
<hr>
<p>[TOC]</p>
<ol>
<li>Lenet，1986年</li>
<li>Alexnet，2012年</li>
<li>GoogleNet，2014年</li>
<li>VGG，2014年</li>
<li>Deep Residual Learning，2015年</li>
</ol>
]]></summary>
        <content type="html"><![CDATA[<hr>
<p>title: 神经网络模型演化<br>
date: 2017/7/5 17:38:58</p>
<p>categories:</p>
<ul>
<li>深度学习<br>
tags:</li>
<li>deeplearning</li>
<li>alexnet</li>
<li>googlenet</li>
<li>caffenet</li>
<li>caffe</li>
</ul>
<hr>
<p>[TOC]</p>
<ol>
<li>Lenet，1986年</li>
<li>Alexnet，2012年</li>
<li>GoogleNet，2014年</li>
<li>VGG，2014年</li>
<li>Deep Residual Learning，2015年</li>
</ol>
<!--more-->
<div class="github-widget" data-repo="DragonFive/python_cv_AI_ML"></div>
<figure data-type="image" tabindex="1"><img src="https://www.github.com/DragonFive/CVBasicOp/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1503832736486.jpg" alt="caffe model"></figure>
<h1 id="网络结构的基础知识">网络结构的基础知识</h1>
<ol>
<li>下采样层的目的是为了<strong>降低网络训练参数</strong>及模型的<strong>过拟合程度</strong>。</li>
<li>LRN局部响应归一化有利于模型泛化。（过拟合）</li>
<li>alexnet做了重叠池化，与lenet不同。也就是说它的pool kernel的步长比kernel size要小。</li>
<li>dropout在每个不同的样本进来的时候用不同的一半的神经元做fc层。但他们共享权重，</li>
<li>relu是线性的，非饱和的。收敛速度比sigmoid和tanh快</li>
<li>inception的主要思路是用密集成分来近似局部稀疏结构。网络越到后面，特征越来越抽象，3x3、5x5的卷积的比例也在增加，但是5x5的卷积计算量会大，后续层的参数会多，因此需要用1x1的卷积进行降维</li>
</ol>
<h2 id="alexnet">alexNet</h2>
<p>它证明了CNN在复杂模型下的有效性，然后GPU实现使得训练在可接受的时间范围内得到结果<br>
Alexnet有一个特殊的计算层，LRN层，做的事是对当前层的输出结果做平滑处理。</p>
<p>前面的结构  conv - relu - pool - LRN</p>
<figure data-type="image" tabindex="2"><img src="https://www.github.com/DragonFive/CVBasicOp/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1502709541017.jpg" alt="卷积层"></figure>
<p>全连接的结构 fc - relu - dropout</p>
<h2 id="googlenet">googLeNet</h2>
<p>从这里开始pooling层其实变少了。<br>
要想提高CNN的网络能力，比如分类准确率，一般的想法就是增大网络，比如Alexnet确实比以前早期Lenet大了很多，但是纯粹的增大网络——比如把<strong>每一层的channel数量翻倍</strong>——但是这样做有两个缺点——<strong>参数太多容易过拟合，网络计算量也会越来越大</strong>。</p>
<h3 id="inception-v1">inception v1</h3>
<figure data-type="image" tabindex="3"><img src="https://www.github.com/DragonFive/CVBasicOp/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1502709248613.jpg" alt="inceptionv1结构"></figure>
<p>一分四，然后做一些不同大小的卷积，之后再堆叠feature map。这样提取不同尺度的特征，能够提高网络表达能力。</p>
<p>目前很多工作证明，要想增强网络能力，可以：增加网络深度，增加网络宽度；但是为了<strong>减少过拟合，也要减少自由参数</strong>。因此，就自然而然有了这个第一版的Inception网络结构——同一层里面，有卷积1x1, 3x 3,5x 5 <strong>不同的卷积模板，他们可以在不同size的感受野做特征提取</strong>，也算的上是一种混合模型了。因为<strong>Max Pooling本身也有特征提取的作用</strong>，而且和卷积不同，没有参数不会过拟合，也作为一个分支。但是直接这样做，整个网络计算量会较大，且层次并没有变深，因此，在3x3和5x5卷 积前面<strong>先做1x1的卷积，降低input的channel数量，这样既使得网络变深，同时计算量反而小了</strong>；（在每一个卷积之后都有<strong>ReLU</strong>）</p>
<h3 id="inception-v2-v3">inception v2 v3</h3>
<p>用1x3和3x1卷积替代3x3卷积，计算量少了很多，深度变深，思路是一样的。（实际上是1xn和nx1替代nxn，n可以变）,使用的是不对称的卷积核</p>
<figure data-type="image" tabindex="4"><img src="https://www.github.com/DragonFive/CVBasicOp/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1502710845336.jpg" alt="incepiton v2"></figure>
<figure data-type="image" tabindex="5"><img src="https://www.github.com/DragonFive/CVBasicOp/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1502710981337.jpg" alt="incpiton 2网络结构"></figure>
<h2 id="vgg">VGG</h2>
<p>特点也是连续conv多，计算量巨大</p>
<figure data-type="image" tabindex="6"><img src="https://www.github.com/DragonFive/CVBasicOp/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1502710007686.jpg" alt="做连续卷积"></figure>
<figure data-type="image" tabindex="7"><img src="https://www.github.com/DragonFive/CVBasicOp/raw/master/1505876579495.jpg" alt="VGG的参数量"></figure>
<h2 id="resnet">resnet</h2>
<p>特殊之处在于设计了“bottleneck”形式的block（有跨越几层的直连）。最深的model采用的152层</p>
<p>block的结构如下图</p>
<figure data-type="image" tabindex="8"><img src="https://www.github.com/DragonFive/CVBasicOp/raw/master/%E5%B0%8F%E4%B9%A6%E5%8C%A0/1502710123203.jpg" alt="block"></figure>
<h2 id="global-average-pooling">Global Average Pooling</h2>
<p>在Googlenet网络中，也用到了Global Average Pooling，其实是受启发于Network In Network。Global Average Pooling一般用于放在网络的最后，用于替换全连接FC层，为什么要替换FC？因为在使用中，例如alexnet和vgg网络都在卷积和softmax之间串联了fc层，发现有一些缺点：</p>
<p>（1）<strong>参数量极大</strong>，有时候一个网络超过80~90%的参数量在最后的几层FC层中；<br>
（2）<strong>容易过拟合</strong>，很多CNN网络的过拟合主要来自于最后的fc层，因为参数太多，却没有合适的regularizer；过拟合导致模型的泛化能力变弱；<br>
（3）实际应用中非常重要的一点，paper中并没有提到：<strong>FC要求输入输出是fix的</strong>，也就是说图像必须按照给定大小，而实际中，图像有大有小，fc就很不方便；</p>
<p>作者提出了Global Average Pooling，做法很简单，是对<strong>每一个单独的feature map取全局average</strong>。要求输出的nodes和分类category数量一致，这样后面就可以直接接softmax了。</p>
<p>global avg pooling层没有参数所以不会过拟合。</p>
<h1 id="reference">reference</h1>
<p><a href="http://blog.csdn.net/xbinworld/article/details/61674836">深度学习方法（十一）：卷积神经网络结构变化</a></p>
<p><a href="http://blog.csdn.net/xbinworld/article/details/61210499">卷积神经网络结构变化</a></p>
<p><a href="http://www.shuang0420.com/2017/04/25/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20CNN%20%E7%AC%94%E8%AE%B0(%E9%AB%98%E7%BA%A7%E7%AF%87)/">卷积神经网络总结</a></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[caffe-源码学习——只看一篇就够了]]></title>
        <id>https://DragonFive.github.io//post/caffe_study</id>
        <link href="https://DragonFive.github.io//post/caffe_study">
        </link>
        <updated>2017-06-12T13:18:22.000Z</updated>
        <content type="html"><![CDATA[<hr>
<p>title: caffe-源码学习——只看一篇就够了<br>
date: 2017/6/12 15:04:12</p>
<p>categories:</p>
<ul>
<li>深度学习<br>
tags:</li>
<li>深度学习</li>
<li>caffe</li>
<li>deeplearning</li>
<li>python</li>
</ul>
<hr>
<h1 id="网络模型">网络模型</h1>
<p>说caffe代码难懂，其实关键点在于caffe中有很多基础的数学运算代码，如果能够对掌握这些数学运算，剩下的就是推公式了。</p>
<h2 id="激活函数">激活函数</h2>
<h2 id="sigmoid">sigmoid</h2>
<p>看softmax函数之前先看一下简单的sigmoid, 这个sigmoid layer的cpp实现是非常简洁的。 sigmoid的cpp文件里主要给了三个函数的实现，分别是sigmoid函数，forward_cpu, backward_cpu,在cpp文件里只实现了算法的CPU版本，至于GPU版本的函数实现放在.cu文件里面</p>
<pre><code class="language-cpp">template &lt;typename Dtype&gt;
inline Dtype sigmoid(Dtype x) {
  return 0.5 * tanh(0.5 * x) + 0.5;
}

template &lt;typename Dtype&gt;
void SigmoidLayer&lt;Dtype&gt;::Forward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
    const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {
  const Dtype* bottom_data = bottom[0]-&gt;cpu_data();
  Dtype* top_data = top[0]-&gt;mutable_cpu_data();
  const int count = bottom[0]-&gt;count();
  for (int i = 0; i &lt; count; ++i) {
    top_data[i] = sigmoid(bottom_data[i]);
  }
}

template &lt;typename Dtype&gt;
void SigmoidLayer&lt;Dtype&gt;::Backward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,
    const vector&lt;bool&gt;&amp; propagate_down,
    const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom) {
  if (propagate_down[0]) {
    const Dtype* top_data = top[0]-&gt;cpu_data();
    const Dtype* top_diff = top[0]-&gt;cpu_diff();
    Dtype* bottom_diff = bottom[0]-&gt;mutable_cpu_diff();
    const int count = bottom[0]-&gt;count();
    for (int i = 0; i &lt; count; ++i) {
      const Dtype sigmoid_x = top_data[i];
      bottom_diff[i] = top_diff[i] * sigmoid_x * (1. - sigmoid_x);
    }
  }
}
</code></pre>
<p><strong>sigmoid函数</strong><br>
注意这里的sigmoid函数与标准的定义不太一样。参见ufld里面的定义<br>
[神经网络UFLD</p>
<p>](http://ufldl.stanford.edu/wiki/index.php/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C)</p>
<p>而在这里 sigmoid = 0.5 * tanh(0.5 * x) + 0.5, sigmoid变化范围为从0-1, tanh从-1到1，乘于0.5再加上0.5两者变化范围就一样了。<br>
<strong>forward_cpu</strong><br>
这个很容易就能看懂，就是对每一个bottom元素计算sigmoid就得到来top的元素。</p>
<p><strong>backward_cpu</strong><br>
发现新版的代码真的很好懂，sigmoid函数的到函数是sigmoid*(1-sigmoid) , 所以这里就直接利用来。其中propagate_down表明这一层是否要反传。</p>
<h3 id="softmaxlayer">softmaxlayer</h3>
<p>这段代码比较复杂，比较好的注释如下。但是这个注释针对的代码版本比较老。<br>
<a href="http://blog.csdn.net/u010668083/article/details/44857455">caffe深度学习网络softmax层代码注释</a></p>
<p>这里我们分析比较新的代码，当前(20170622)比较新的代码是20161202提交的代码，结构如下</p>
<pre><code class="language-cpp">/template &lt;typename Dtype&gt;
void SoftmaxLayer&lt;Dtype&gt;::Reshape(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {
  softmax_axis_ =
      bottom[0]-&gt;CanonicalAxisIndex(this-&gt;layer_param_.softmax_param().axis());
  //softmax层的输出应该与输入层一致
  top[0]-&gt;ReshapeLike(*bottom[0]);
  vector&lt;int&gt; mult_dims(1, bottom[0]-&gt;shape(softmax_axis_));
  sum_multiplier_.Reshape(mult_dims);
  Dtype* multiplier_data = sum_multiplier_.mutable_cpu_data();
  caffe_set(sum_multiplier_.count(), Dtype(1), multiplier_data);
  outer_num_ = bottom[0]-&gt;count(0, softmax_axis_);
  inner_num_ = bottom[0]-&gt;count(softmax_axis_ + 1);
  vector&lt;int&gt; scale_dims = bottom[0]-&gt;shape();
  // scale_尺寸为：num*1*height*width
  scale_dims[softmax_axis_] = 1;
  scale_.Reshape(scale_dims);
}
//前向计算，得到softmax的值
template &lt;typename Dtype&gt;
void SoftmaxLayer&lt;Dtype&gt;::Forward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
    const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {
  const Dtype* bottom_data = bottom[0]-&gt;cpu_data();
  Dtype* top_data = top[0]-&gt;mutable_cpu_data();
  Dtype* scale_data = scale_.mutable_cpu_data();
  int channels = bottom[0]-&gt;shape(softmax_axis_);
  int dim = bottom[0]-&gt;count() / outer_num_;
  caffe_copy(bottom[0]-&gt;count(), bottom_data, top_data);
  // We need to subtract the max to avoid numerical issues, compute the exp,
  // and then normalize.
  // 先找到最大值
  for (int i = 0; i &lt; outer_num_; ++i) {//outer_num就是num输出数据的数目
    // initialize scale_data to the first plane
    caffe_copy(inner_num_, bottom_data + i * dim, scale_data);//dim表示每个数据有多少个不同类别的值.
    for (int j = 0; j &lt; channels; j++) {
      for (int k = 0; k &lt; inner_num_; k++) {
        scale_data[k] = std::max(scale_data[k],//每个元素表示应该是当前位置中所有类别和channel里面最大的那一个。
            bottom_data[i * dim + j * inner_num_ + k]);
      }
    }
    // subtraction 减去最大值 详细分析见后面 
    caffe_cpu_gemm&lt;Dtype&gt;(CblasNoTrans, CblasNoTrans, channels, inner_num_,
        1, -1., sum_multiplier_.cpu_data(), scale_data, 1., top_data);
    // exponentiation 求指数
    caffe_exp&lt;Dtype&gt;(dim, top_data, top_data);
    // sum after exp 求和
    caffe_cpu_gemv&lt;Dtype&gt;(CblasTrans, channels, inner_num_, 1.,
        top_data, sum_multiplier_.cpu_data(), 0., scale_data);
    // division 做除法
    for (int j = 0; j &lt; channels; j++) {
      caffe_div(inner_num_, top_data, scale_data, top_data);
      top_data += inner_num_;
    }
  }
}

template &lt;typename Dtype&gt;
void SoftmaxLayer&lt;Dtype&gt;::Backward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,
    const vector&lt;bool&gt;&amp; propagate_down,
    const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom) {
  const Dtype* top_diff = top[0]-&gt;cpu_diff();
  const Dtype* top_data = top[0]-&gt;cpu_data();
  Dtype* bottom_diff = bottom[0]-&gt;mutable_cpu_diff();
  Dtype* scale_data = scale_.mutable_cpu_data();
  int channels = top[0]-&gt;shape(softmax_axis_);
  int dim = top[0]-&gt;count() / outer_num_;
  caffe_copy(top[0]-&gt;count(), top_diff, bottom_diff);
  for (int i = 0; i &lt; outer_num_; ++i) {
    // compute dot(top_diff, top_data) and subtract them from the bottom diff
	//计算top_diff与top_data的点集
    for (int k = 0; k &lt; inner_num_; ++k) {
      scale_data[k] = caffe_cpu_strided_dot&lt;Dtype&gt;(channels,
          bottom_diff + i * dim + k, inner_num_,
          top_data + i * dim + k, inner_num_);
    }
    // subtraction
    caffe_cpu_gemm&lt;Dtype&gt;(CblasNoTrans, CblasNoTrans, channels, inner_num_, 1,
        -1., sum_multiplier_.cpu_data(), scale_data, 1., bottom_diff + i * dim);
  }
  // elementwise multiplication
  caffe_mul(top[0]-&gt;count(), bottom_diff, top_data, bottom_diff);
}


#ifdef CPU_ONLY
STUB_GPU(SoftmaxLayer);
#endif

INSTANTIATE_CLASS(SoftmaxLayer);

</code></pre>
<p>**caffe_cpu_gemm减法 **<br>
在forward_cpu函数里做减法的时候调用来caffe_cpu_gemm函数，这个函数的实现在 src/caffe/util/math_functions.cpp里面<br>
<a href="http://blog.csdn.net/seven_first/article/details/47378697#1-caffecpugemm-%E5%87%BD%E6%95%B0">caffecpugemm-函数</a></p>
<pre><code class="language-cpp">template&lt;&gt;
void caffe_cpu_gemm&lt;float&gt;(const CBLAS_TRANSPOSE TransA,
    const CBLAS_TRANSPOSE TransB, const int M, const int N, const int K,
    const float alpha, const float* A, const float* B, const float beta,
    float* C) {
  int lda = (TransA == CblasNoTrans) ? K : M;
  int ldb = (TransB == CblasNoTrans) ? N : K;
  cblas_sgemm(CblasRowMajor, TransA, TransB, M, N, K, alpha, A, lda, B,
      ldb, beta, C, N);
}
</code></pre>
<blockquote>
<p>功能： C=alpha<em>A</em>B+beta*C<br>
A,B,C 是输入矩阵（一维数组格式）<br>
CblasRowMajor :数据是行主序的（二维数据也是用一维数组储存的）<br>
TransA, TransB：是否要对A和B做转置操作（CblasTrans CblasNoTrans）<br>
M： A、C 的行数<br>
N： B、C 的列数<br>
K： A 的列数， B 的行数<br>
lda ： A的列数（不做转置）行数（做转置）<br>
ldb： B的列数（不做转置）行数（做转置）</p>
</blockquote>
<p>所以这里求减法：</p>
<pre><code class="language-cpp">    caffe_cpu_gemm&lt;Dtype&gt;(CblasNoTrans, CblasNoTrans, channels, inner_num_,
        1, -1., sum_multiplier_.cpu_data(), scale_data, 1., top_data);
</code></pre>
<p>就是： <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>t</mi><mi>o</mi><mi>p</mi><mi mathvariant="normal">_</mi><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi><mo>=</mo><mi>t</mi><mi>o</mi><mi>p</mi><mi mathvariant="normal">_</mi><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi><mo>−</mo><mn>1</mn><mo>∗</mo><mi>s</mi><mi>u</mi><mi>m</mi><mi mathvariant="normal">_</mi><mi>m</mi><mi>u</mi><mi>l</mi><mi>t</mi><mi>i</mi><mi>p</mi><mi>l</mi><mi>i</mi><mi>e</mi><mi>r</mi><mi mathvariant="normal">_</mi><mi mathvariant="normal">.</mi><mi>c</mi><mi>p</mi><mi>u</mi><mi mathvariant="normal">_</mi><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi><mo>(</mo><mo>)</mo><mo>∗</mo><mi>s</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>e</mi><mi mathvariant="normal">_</mi><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow><annotation encoding="application/x-tex">top\_data = top\_data - 1* sum\_multiplier\_.cpu\_data()*scale\_data</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.00444em;vertical-align:-0.31em;"></span><span class="mord mathdefault">t</span><span class="mord mathdefault">o</span><span class="mord mathdefault">p</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathdefault">d</span><span class="mord mathdefault">a</span><span class="mord mathdefault">t</span><span class="mord mathdefault">a</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.00444em;vertical-align:-0.31em;"></span><span class="mord mathdefault">t</span><span class="mord mathdefault">o</span><span class="mord mathdefault">p</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathdefault">d</span><span class="mord mathdefault">a</span><span class="mord mathdefault">t</span><span class="mord mathdefault">a</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mord mathdefault">s</span><span class="mord mathdefault">u</span><span class="mord mathdefault">m</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathdefault">m</span><span class="mord mathdefault">u</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">t</span><span class="mord mathdefault">i</span><span class="mord mathdefault">p</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">i</span><span class="mord mathdefault">e</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord">.</span><span class="mord mathdefault">c</span><span class="mord mathdefault">p</span><span class="mord mathdefault">u</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathdefault">d</span><span class="mord mathdefault">a</span><span class="mord mathdefault">t</span><span class="mord mathdefault">a</span><span class="mopen">(</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.00444em;vertical-align:-0.31em;"></span><span class="mord mathdefault">s</span><span class="mord mathdefault">c</span><span class="mord mathdefault">a</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">e</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathdefault">d</span><span class="mord mathdefault">a</span><span class="mord mathdefault">t</span><span class="mord mathdefault">a</span></span></span></span>， 这里用top_data来减而不用bottom一方面是因为bottom是const的，取的是cpu_data(), 而top_data是mutable_cpu_data,另一方面之前已经把数据从bottom拷贝到top里面去了。</p>
<p>而在back_ward函数里面也用到了这个函数。</p>
<pre><code class="language-cpp">caffe_cpu_gemm&lt;Dtype&gt;(CblasNoTrans, CblasNoTrans, channels, inner_num_, 1,
        -1., sum_multiplier_.cpu_data(), scale_data, 1., bottom_diff + i * dim);
</code></pre>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>b</mi><mi>o</mi><mi>t</mi><mi>t</mi><mi>o</mi><mi>m</mi><mi mathvariant="normal">_</mi><mi>d</mi><mi>i</mi><mi>f</mi><mi>f</mi><mo>+</mo><mi>i</mi><mo>∗</mo><mi>d</mi><mi>i</mi><mi>m</mi><mo>=</mo><mi>b</mi><mi>o</mi><mi>t</mi><mi>t</mi><mi>o</mi><mi>m</mi><mi mathvariant="normal">_</mi><mi>d</mi><mi>i</mi><mi>f</mi><mi>f</mi><mo>+</mo><mi>i</mi><mo>∗</mo><mi>d</mi><mi>i</mi><mi>m</mi><mo>−</mo><mi>s</mi><mi>u</mi><mi>m</mi><mi mathvariant="normal">_</mi><mi>m</mi><mi>u</mi><mi>l</mi><mi>t</mi><mi>i</mi><mi>p</mi><mi>l</mi><mi>i</mi><mi>e</mi><mi>r</mi><mi mathvariant="normal">_</mi><mi mathvariant="normal">.</mi><mi>c</mi><mi>p</mi><msub><mi>u</mi><mi>d</mi></msub><mi>a</mi><mi>t</mi><mi>a</mi><mo>(</mo><mo>)</mo><mo>∗</mo><mi>s</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>e</mi><mi mathvariant="normal">_</mi><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow><annotation encoding="application/x-tex">bottom\_diff + i * dim = bottom\_diff + i * dim - sum\_multiplier\_.cpu_data() * scale\_data
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.00444em;vertical-align:-0.31em;"></span><span class="mord mathdefault">b</span><span class="mord mathdefault">o</span><span class="mord mathdefault">t</span><span class="mord mathdefault">t</span><span class="mord mathdefault">o</span><span class="mord mathdefault">m</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathdefault">d</span><span class="mord mathdefault">i</span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.65952em;vertical-align:0em;"></span><span class="mord mathdefault">i</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">d</span><span class="mord mathdefault">i</span><span class="mord mathdefault">m</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.00444em;vertical-align:-0.31em;"></span><span class="mord mathdefault">b</span><span class="mord mathdefault">o</span><span class="mord mathdefault">t</span><span class="mord mathdefault">t</span><span class="mord mathdefault">o</span><span class="mord mathdefault">m</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathdefault">d</span><span class="mord mathdefault">i</span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.65952em;vertical-align:0em;"></span><span class="mord mathdefault">i</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.77777em;vertical-align:-0.08333em;"></span><span class="mord mathdefault">d</span><span class="mord mathdefault">i</span><span class="mord mathdefault">m</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mord mathdefault">s</span><span class="mord mathdefault">u</span><span class="mord mathdefault">m</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathdefault">m</span><span class="mord mathdefault">u</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">t</span><span class="mord mathdefault">i</span><span class="mord mathdefault">p</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">i</span><span class="mord mathdefault">e</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord">.</span><span class="mord mathdefault">c</span><span class="mord mathdefault">p</span><span class="mord"><span class="mord mathdefault">u</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathdefault">a</span><span class="mord mathdefault">t</span><span class="mord mathdefault">a</span><span class="mopen">(</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.00444em;vertical-align:-0.31em;"></span><span class="mord mathdefault">s</span><span class="mord mathdefault">c</span><span class="mord mathdefault">a</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">e</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathdefault">d</span><span class="mord mathdefault">a</span><span class="mord mathdefault">t</span><span class="mord mathdefault">a</span></span></span></span></span></p>
<p><strong>caffe_exp指数</strong><br>
caffe_exp函数是用来求指数的，其中一个实现是这样的。</p>
<pre><code class="language-cpp">template &lt;&gt;
void caffe_exp&lt;float&gt;(const int n, const float* a, float* y) {
  vsExp(n, a, y);
}
</code></pre>
<blockquote>
<p>功能：  y[i] = exp(a[i] )</p>
</blockquote>
<p>所以， forward_cpu里面的指数就很容易理解了。</p>
<pre><code>caffe_exp&lt;Dtype&gt;(dim, top_data, top_data);
</code></pre>
<p>top_data[i] = exp(topdata[i])<br>
<strong>caffe_cpu_gemv求和</strong></p>
<pre><code class="language-cpp">template &lt;&gt;
void caffe_cpu_gemv&lt;float&gt;(const CBLAS_TRANSPOSE TransA, const int M,
    const int N, const float alpha, const float* A, const float* x,
    const float beta, float* y) {
  cblas_sgemv(CblasRowMajor, TransA, M, N, alpha, A, N, x, 1, beta, y, 1);
}
</code></pre>
<blockquote>
<p>功能： y=alpha<em>A</em>x+beta*y<br>
其中X和Y是向量，A 是矩阵<br>
M：A 的行数<br>
N：A 的列数<br>
cblas_sgemv 中的 参数1 表示对X和Y的每个元素都进行操作</p>
</blockquote>
<p>forward_cpu里面的求和就很容易理解了</p>
<pre><code class="language-cpp">    // sum after exp 求和
    caffe_cpu_gemv&lt;Dtype&gt;(CblasTrans, channels, inner_num_, 1.,
        top_data, sum_multiplier_.cpu_data(), 0., scale_data);

</code></pre>
<p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>s</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>e</mi><mi mathvariant="normal">_</mi><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi><mo>=</mo><mo>∑</mo><mi>t</mi><mi>o</mi><mi>p</mi><mi mathvariant="normal">_</mi><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi><mo>[</mo><mi>i</mi><mo>]</mo><mo>∗</mo><mi>s</mi><mi>u</mi><mi>m</mi><mi mathvariant="normal">_</mi><mi>m</mi><mi>u</mi><mi>l</mi><mi>t</mi><mi>i</mi><mi>p</mi><mi>l</mi><mi>i</mi><mi>e</mi><mi>r</mi><mi mathvariant="normal">_</mi><mi mathvariant="normal">.</mi><mi>c</mi><mi>p</mi><mi>u</mi><mi mathvariant="normal">_</mi><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi><mo>(</mo><mo>)</mo><mo>[</mo><mi>i</mi><mo>]</mo><mo separator="true">;</mo></mrow><annotation encoding="application/x-tex">scale\_data =\sum top\_data[i]*sum\_multiplier\_.cpu\_data()[i];</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.00444em;vertical-align:-0.31em;"></span><span class="mord mathdefault">s</span><span class="mord mathdefault">c</span><span class="mord mathdefault">a</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">e</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathdefault">d</span><span class="mord mathdefault">a</span><span class="mord mathdefault">t</span><span class="mord mathdefault">a</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em;">∑</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">t</span><span class="mord mathdefault">o</span><span class="mord mathdefault">p</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathdefault">d</span><span class="mord mathdefault">a</span><span class="mord mathdefault">t</span><span class="mord mathdefault">a</span><span class="mopen">[</span><span class="mord mathdefault">i</span><span class="mclose">]</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mord mathdefault">s</span><span class="mord mathdefault">u</span><span class="mord mathdefault">m</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathdefault">m</span><span class="mord mathdefault">u</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">t</span><span class="mord mathdefault">i</span><span class="mord mathdefault">p</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">i</span><span class="mord mathdefault">e</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord">.</span><span class="mord mathdefault">c</span><span class="mord mathdefault">p</span><span class="mord mathdefault">u</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathdefault">d</span><span class="mord mathdefault">a</span><span class="mord mathdefault">t</span><span class="mord mathdefault">a</span><span class="mopen">(</span><span class="mclose">)</span><span class="mopen">[</span><span class="mord mathdefault">i</span><span class="mclose">]</span><span class="mpunct">;</span></span></span></span></p>
<p><strong>caffe_div除法</strong></p>
<pre><code class="language-cpp">template &lt;&gt;
void caffe_div&lt;float&gt;(const int n, const float* a, const float* b,
    float* y) {
  vsDiv(n, a, b, y);
}

</code></pre>
<blockquote>
<p>功能 y[i] = a[i] / b[i]</p>
</blockquote>
<pre><code class="language-cpp">    // division 做除法
    for (int j = 0; j &lt; channels; j++) {
      caffe_div(inner_num_, top_data, scale_data, top_data);
      top_data += inner_num_;

</code></pre>
<p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>t</mi><mi>o</mi><mi>p</mi><mi mathvariant="normal">_</mi><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi><mo>[</mo><mi>i</mi><mo>]</mo><mo>=</mo><mi>t</mi><mi>o</mi><mi>p</mi><mi mathvariant="normal">_</mi><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi><mo>[</mo><mi>i</mi><mo>]</mo><mi mathvariant="normal">/</mi><mi>s</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>e</mi><mi mathvariant="normal">_</mi><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi><mo>[</mo><mi>i</mi><mo>]</mo><mo separator="true">;</mo></mrow><annotation encoding="application/x-tex">top\_data[i] = top\_data[i] / scale\_data[i];</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mord mathdefault">t</span><span class="mord mathdefault">o</span><span class="mord mathdefault">p</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathdefault">d</span><span class="mord mathdefault">a</span><span class="mord mathdefault">t</span><span class="mord mathdefault">a</span><span class="mopen">[</span><span class="mord mathdefault">i</span><span class="mclose">]</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mord mathdefault">t</span><span class="mord mathdefault">o</span><span class="mord mathdefault">p</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathdefault">d</span><span class="mord mathdefault">a</span><span class="mord mathdefault">t</span><span class="mord mathdefault">a</span><span class="mopen">[</span><span class="mord mathdefault">i</span><span class="mclose">]</span><span class="mord">/</span><span class="mord mathdefault">s</span><span class="mord mathdefault">c</span><span class="mord mathdefault">a</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">e</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathdefault">d</span><span class="mord mathdefault">a</span><span class="mord mathdefault">t</span><span class="mord mathdefault">a</span><span class="mopen">[</span><span class="mord mathdefault">i</span><span class="mclose">]</span><span class="mpunct">;</span></span></span></span></p>
<p><strong>caffe_cpu_strided_dot</strong></p>
<pre><code class="language-cpp">template &lt;&gt;
double caffe_cpu_strided_dot&lt;double&gt;(const int n, const double* x,
    const int incx, const double* y, const int incy) {
  return cblas_ddot(n, x, incx, y, incy);
}

</code></pre>
<blockquote>
<p>功能： 返回 vector X 和 vector Y 的内积。<br>
incx， incy ： 步长，即每隔incx 或 incy 个element 进行操作。</p>
</blockquote>
<p><strong>caffe_mul</strong></p>
<pre><code class="language-cpp">template &lt;&gt;
void caffe_mul&lt;float&gt;(const int n, const float* a, const float* b,
    float* y) {
  vsMul(n, a, b, y);
}

</code></pre>
<p>功能 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>y</mi><mo>[</mo><mi>i</mi><mo>]</mo><mo>=</mo><mi>a</mi><mo>[</mo><mi>i</mi><mo>]</mo><mo>∗</mo><mi>b</mi><mo>[</mo><mi>i</mi><mo>]</mo></mrow><annotation encoding="application/x-tex">y[i]=a[i] * b[i]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mopen">[</span><span class="mord mathdefault">i</span><span class="mclose">]</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">a</span><span class="mopen">[</span><span class="mord mathdefault">i</span><span class="mclose">]</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">b</span><span class="mopen">[</span><span class="mord mathdefault">i</span><span class="mclose">]</span></span></span></span></p>
<pre><code class="language-cpp"> caffe_mul(top[0]-&gt;count(), bottom_diff, top_data, bottom_diff);

</code></pre>
<p>bottom_diff[i] = bottom_diff[i] * top_data[i]</p>
<p><strong>反向传播公式推导</strong><br>
<a href="https://www.zhihu.com/question/28927103">Caffe Softmax层的实现原理,知乎</a></p>
<p>看完softmax layer的实现，我们再来看一下SoftmaxWithLossLayer的代码实现。</p>
<h2 id="卷积层">卷积层</h2>
<h3 id="计算量与参数量">计算量与参数量</h3>
<p>每个样本做一次前向传播时卷积计算量为： $ i* j<em>M</em>N<em>K</em>L  $ ，其中<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>i</mi><mo>∗</mo><mi>j</mi></mrow><annotation encoding="application/x-tex">i*j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.65952em;vertical-align:0em;"></span><span class="mord mathdefault">i</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.85396em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.05724em;">j</span></span></span></span>是卷积核的大小，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>M</mi><mo>∗</mo><mi>L</mi></mrow><annotation encoding="application/x-tex">M*L</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault">L</span></span></span></span>是输出特征图的大小，K是输入特征图通道数，L是输出特征图通道数。</p>
<p>参数量为：$ i<em>J</em>K*L $</p>
<p>所以有个比例叫做计算量参数量之比 CPR，如果在前馈时每个批次batch_size = B, 则表示将B个输入合并成一个矩阵进行计算，那么相当于每次的输出特征图增大来B倍，所以CPR提升来B倍，也就是，每次计算的时候参数重复利用率提高来B倍。</p>
<p>卷积层：局部互连，权值共享，</p>
<h3 id="源码学习">源码学习</h3>
<p>先用grep函数在caffe根目录下搜索一下包含ConvolutionLayer的文件有哪些，然后从头文件入手慢慢分析，下面是结果，精简来一些无效成分，在caffe的include文件夹下执行：</p>
<pre><code class="language-bash">grep -n -H -R &quot;ConvolutionLayer&quot;

</code></pre>
<p>-n表示显示行号，-H表示显示文件名，-R表示递归查找 后面部分表示查找的内容<br>
结果如下</p>
<pre><code class="language-bash">caffe/layer_factory.hpp:31: * (for example, when your layer has multiple backends, see GetConvolutionLayer
caffe/layers/base_conv_layer.hpp:15: *        ConvolutionLayer and DeconvolutionLayer.
caffe/layers/base_conv_layer.hpp:18:class BaseConvolutionLayer : public Layer&lt;Dtype&gt; {
caffe/layers/base_conv_layer.hpp:20:  explicit BaseConvolutionLayer(const LayerParameter&amp; param)
caffe/layers/deconv_layer.hpp:17: *        opposite sense as ConvolutionLayer.
caffe/layers/deconv_layer.hpp:19: *   ConvolutionLayer computes each output value by dotting an input window with
caffe/layers/deconv_layer.hpp:22: *   DeconvolutionLayer is ConvolutionLayer with the forward and backward passes
caffe/layers/deconv_layer.hpp:24: *   parameters, but they take the opposite sense as in ConvolutionLayer (so
caffe/layers/deconv_layer.hpp:29:class DeconvolutionLayer : public BaseConvolutionLayer&lt;Dtype&gt; {
caffe/layers/deconv_layer.hpp:32:      : BaseConvolutionLayer&lt;Dtype&gt;(param) {}
caffe/layers/im2col_layer.hpp:14: *        column vectors.  Used by ConvolutionLayer to perform convolution
caffe/layers/conv_layer.hpp:31:class ConvolutionLayer : public BaseConvolutionLayer&lt;Dtype&gt; {
caffe/layers/conv_layer.hpp:35:   *    with ConvolutionLayer options:
caffe/layers/conv_layer.hpp:64:  explicit ConvolutionLayer(const LayerParameter&amp; param)
caffe/layers/conv_layer.hpp:65:      : BaseConvolutionLayer&lt;Dtype&gt;(param) {}
caffe/layers/cudnn_conv_layer.hpp:16: * @brief cuDNN implementation of ConvolutionLayer.
caffe/layers/cudnn_conv_layer.hpp:17: *        Fallback to ConvolutionLayer for CPU mode.
caffe/layers/cudnn_conv_layer.hpp:30:class CuDNNConvolutionLayer : public ConvolutionLayer&lt;Dtype&gt; {
caffe/layers/cudnn_conv_layer.hpp:32:  explicit CuDNNConvolutionLayer(const LayerParameter&amp; param)
caffe/layers/cudnn_conv_layer.hpp:33:      : ConvolutionLayer&lt;Dtype&gt;(param), handles_setup_(false) {}
caffe/layers/cudnn_conv_layer.hpp:38:  virtual ~CuDNNConvolutionLayer();


</code></pre>
<p>主要有三个类包含这个卷积层的实现：<br>
base_conv_layer：主要是卷积层基类的实现<br>
deconv_layer： 目测是反向传播时候的卷积层的逆向过程<br>
cudnn_conv_layer：目测是cudnn实现的卷积层版本继承自BaseConvolutionLayer,GPU版本</p>
<p>接下来我们就打开这三个文件，跳转到相关行，详细看一下。</p>
<pre><code>class BaseConvolutionLayer : public Layer&lt;Dtype&gt; {
 public:
  explicit BaseConvolutionLayer(const LayerParameter&amp; param)
      : Layer&lt;Dtype&gt;(param) {}
  virtual void LayerSetUp(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top);
  virtual void Reshape(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top);

  virtual inline int MinBottomBlobs() const { return 1; }
  virtual inline int MinTopBlobs() const { return 1; }
  virtual inline bool EqualNumBottomTopBlobs() const { return true; }

 protected:
  // Helper functions that abstract away the column buffer and gemm arguments.
  // The last argument in forward_cpu_gemm is so that we can skip the im2col if
  // we just called weight_cpu_gemm with the same input.
  void forward_cpu_gemm(const Dtype* input, const Dtype* weights,
      Dtype* output, bool skip_im2col = false);
  void forward_cpu_bias(Dtype* output, const Dtype* bias);
  void backward_cpu_gemm(const Dtype* input, const Dtype* weights,
      Dtype* output);
  void weight_cpu_gemm(const Dtype* input, const Dtype* output, Dtype*
      weights);
  void backward_cpu_bias(Dtype* bias, const Dtype* input);

#ifndef CPU_ONLY
  void forward_gpu_gemm(const Dtype* col_input, const Dtype* weights,
      Dtype* output, bool skip_im2col = false);
  void forward_gpu_bias(Dtype* output, const Dtype* bias);
  void backward_gpu_gemm(const Dtype* input, const Dtype* weights,
      Dtype* col_output);
  void weight_gpu_gemm(const Dtype* col_input, const Dtype* output, Dtype*
      weights);
  void backward_gpu_bias(Dtype* bias, const Dtype* input);
#endif

</code></pre>
<p>这里给出来CPU和GPU版本的代码的声明，这些代码比较底层，先放一放以后再看。<br>
forward_cpu_gemm:猜测可能是前馈过程计算weight部分，来看看CPP里面的实现吧。</p>
<p>在BaseConvolutionLayer中的卷积的实现中有一个重要的函数就是<strong>im2col以及col2im，im2colnd以及col2imnd</strong>。前面的两个函数是二维卷积的正向和逆向过程，而后面的两个函数是n维卷积的正向和逆向过程。</p>
<pre><code class="language-cpp">void BaseConvolutionLayer&lt;Dtype&gt;::forward_cpu_gemm(const Dtype* input,
    const Dtype* weights, Dtype* output, bool skip_im2col) {
  const Dtype* col_buff = input;
  if (!is_1x1_) {
    if (!skip_im2col) {
	  // 如果没有1x1卷积，也没有skip_im2col  
      // 则使用conv_im2col_cpu对使用卷积核滑动过程中的每一个kernel大小的图像块  
      // 变成一个列向量，形成一个height=kernel_dim_  
      // width = 卷积后图像heght*卷积后图像width  
      conv_im2col_cpu(input, col_buffer_.mutable_cpu_data());
    }
    col_buff = col_buffer_.cpu_data();
  }
  for (int g = 0; g &lt; group_; ++g) {
    caffe_cpu_gemm&lt;Dtype&gt;(CblasNoTrans, CblasNoTrans, conv_out_channels_ /
        group_, conv_out_spatial_dim_, kernel_dim_,
        (Dtype)1., weights + weight_offset_ * g, col_buff + col_offset_ * g,
        (Dtype)0., output + output_offset_ * g);
  }
}


</code></pre>
<p><strong>参考资料</strong><br>
<a href="http://blog.csdn.net/xizero00/article/details/51049858"> caffe代码阅读10：Caffe中卷积的实现细节</a></p>
<h1 id="数据集">数据集</h1>
<h2 id="生成数据集的均值文件">生成数据集的均值文件</h2>
<p>这里计算图像的均值使用的是pper_image_mean方法，在natural images上训练的时候，这种方式比较好，以imagenet数据集为例，caffe在使用imagenet数据集的时候需要计算均值文件，详细见 [python-caffe</p>
<p>](https://github.com/DragonFive/deep-learning-exercise/blob/master/caffe_python1.ipynb)</p>
<h2 id="caffe-blob">caffe-blob</h2>
<p><a href="http://blog.csdn.net/chenriwei2/article/details/46367023">【Caffe代码解析】Blob</a><br>
<a href="http://blog.csdn.net/lingerlanlan/article/details/24379689">caffe源码分析--Blob类代码研究</a><br>
<a href="http://www.cnblogs.com/louyihang-loves-baiyan/p/5149628.html">Caffe源码解析1：Blob</a></p>
<h3 id="结构体分析">结构体分析</h3>
<p>Blob 是Caffe作为数据传输的媒介，无论是网络权重参数，还是输入数据，都是转化为Blob数据结构来存储，网络，求解器等都是直接与此结构打交道的。</p>
<p>4纬的结构体（包含数据和梯度)，其4维结构通过shape属性得以计算出来.</p>
<p><strong>成员变量</strong></p>
<pre><code class="language-cpp"> protected:
  shared_ptr&lt;SyncedMemory&gt; data_;// 存放数据
  shared_ptr&lt;SyncedMemory&gt; diff_;//存放梯度
  vector&lt;int&gt; shape_; //存放形状
  int count_; //数据个数
  int capacity_; //数据容量

</code></pre>
<p><strong>成员函数</strong></p>
<pre><code>  const Dtype* cpu_data() const;			 //cpu使用的数据
  void set_cpu_data(Dtype* data);		//用数据块的值来blob里面的data。
  const Dtype* gpu_data() const;		//返回不可更改的指针，下同
  const Dtype* cpu_diff() const;
  const Dtype* gpu_diff() const;
  Dtype* mutable_cpu_data();    		//返回可更改的指针，下同
  Dtype* mutable_gpu_data();
  Dtype* mutable_cpu_diff();
  Dtype* mutable_gpu_diff();
  
  int offset(const int n, const int c = 0, const int h = 0,const int w = 0) const
// 通过n,c,h,w 4个参数来计算一维向量的偏移量。

Dtype data_at(const int n, const int c, const int h,const int w) const//通过n,c,h,w 4个参数来来获取该向量位置上的值。

Dtype diff_at(const int n, const int c, const int h,const int w) const//同上

inline const shared_ptr&lt;SyncedMemory&gt;&amp; data() const {
    CHECK(data_);
    return data_;			//返回数据，不能修改
  }

inline const shared_ptr&lt;SyncedMemory&gt;&amp; diff() const {
    CHECK(diff_);
    return diff_;			//返回梯度，不能修改
  }

Reshape(...)//reshape 有多种多态的实现，可以是四个数字，长度为四的vector，其它blob等。

if (count_ &gt; capacity_) {
    capacity_ = count_;
    data_.reset(new SyncedMemory(capacity_ * sizeof(Dtype)));
    diff_.reset(new SyncedMemory(capacity_ * sizeof(Dtype)));
  }//当空间不够的时候，需要扩大容量，reset。


</code></pre>
<p>函数名中带mutable的表示可以对返回的指针内容进行修改。</p>
<h1 id="caffe学习资料收集">caffe学习资料收集</h1>
<p><a href="https://absentm.github.io/2016/05/14/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0Caffe%E7%B3%BB%E5%88%97%E6%95%99%E7%A8%8B%E9%9B%86%E5%90%88/">深度学习Caffe系列教程集合</a></p>
<p><a href="http://blog.csdn.net/xizero00/article/details/50886829">caffe代码阅读1：blob的实现细节-2016.3.14</a></p>
<p><a href="https://yufeigan.github.io/">甘宇飞</a></p>
<p><a href="https://zhuanlan.zhihu.com/Edison-G">计算机视觉战队</a></p>
<p><a href="http://blog.163.com/yuyang_tech/blog/static/2160500832015713105052452/">caffe源码简单解析——Layer层  </a></p>
<p><a href="http://blog.csdn.net/kkk584520/article/details/41681085">Caffe代码导读（0）：路线图</a></p>
<p><a href="https://www.zhihu.com/question/27982282">知乎问题-深度学习caffe的代码怎么读？</a></p>
<p><a href="http://www.cnblogs.com/louyihang-loves-baiyan/p/5149628.html">Caffe源码解析1：Blob</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/24343706">深度学习大讲堂——深度学习框架Caffe源码解析</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/24709689">Caffe代码夜话1</a></p>
<p><a href="http://blog.leanote.com/post/fishing_piggy/Caffe%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%80%EF%BC%89">Caffe源码分析（一）</a></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hello Gridea]]></title>
        <id>https://DragonFive.github.io//post/hello-gridea</id>
        <link href="https://DragonFive.github.io//post/hello-gridea">
        </link>
        <updated>2012-12-12T16:00:00.000Z</updated>
        <summary type="html"><![CDATA[<p>👏  欢迎使用 <strong>Gridea</strong> ！<br>
✍️  <strong>Gridea</strong> 一个静态博客写作客户端。你可以用它来记录你的生活、心情、知识、笔记、创意... ...</p>
]]></summary>
        <content type="html"><![CDATA[<p>👏  欢迎使用 <strong>Gridea</strong> ！<br>
✍️  <strong>Gridea</strong> 一个静态博客写作客户端。你可以用它来记录你的生活、心情、知识、笔记、创意... ...</p>
<!-- more -->
<p><a href="https://github.com/getgridea/gridea">Github</a><br>
<a href="http://hvenotes.fehey.com/">Gridea 主页</a><br>
<a href="http://fehey.com/">示例网站</a></p>
<h2 id="特性">特性👇</h2>
<p>📝  你可以使用最酷的 <strong>Markdown</strong> 语法，进行快速创作</p>
<p>🌉  你可以给文章配上精美的封面图和在文章任意位置插入图片</p>
<p>🏷️  你可以对文章进行标签分组</p>
<p>📋  你可以自定义菜单，甚至可以创建外部链接菜单</p>
<p>💻  你可以在 <strong>𝖶𝗂𝗇𝖽𝗈𝗐𝗌</strong> 或 <strong>𝖬𝖺𝖼𝖮𝖲</strong> 设备上使用此客户端</p>
<p>🌎  你可以使用 <strong>𝖦𝗂𝗍𝗁𝗎𝖻 𝖯𝖺𝗀𝖾𝗌</strong> 或 <strong>Coding Pages</strong> 向世界展示，未来将支持更多平台</p>
<p>💬  你可以进行简单的配置，接入 <a href="https://github.com/gitalk/gitalk">Gitalk</a> 或 <a href="https://github.com/SukkaW/DisqusJS">DisqusJS</a> 评论系统</p>
<p>🇬🇧  你可以使用<strong>中文简体</strong>或<strong>英语</strong></p>
<p>🌁  你可以任意使用应用内默认主题或任意第三方主题，强大的主题自定义能力</p>
<p>🖥  你可以自定义源文件夹，利用 OneDrive、百度网盘、iCloud、Dropbox 等进行多设备同步</p>
<p>🌱 当然 <strong>Gridea</strong> 还很年轻，有很多不足，但请相信，它会不停向前🏃</p>
<p>未来，它一定会成为你离不开的伙伴</p>
<p>尽情发挥你的才华吧！</p>
<p>😘 Enjoy~</p>
]]></content>
    </entry>
</feed>